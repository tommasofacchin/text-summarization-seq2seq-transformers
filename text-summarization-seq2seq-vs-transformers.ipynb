{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tommasofacchin/text-summarization-seq2seq-vs-transformers?scriptVersionId=278380255\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"87510b79","metadata":{"papermill":{"duration":0.007933,"end_time":"2025-11-15T16:33:33.711989","exception":false,"start_time":"2025-11-15T16:33:33.704056","status":"completed"},"tags":[]},"source":["# Project Overview\n","\n","This project focuses on **text summarization** using two approaches: a traditional **Seq2Seq model** with LSTM and a **Transformer-based model**. The goal is to see how each model performs and understand the difference between step-by-step sequence processing and attention-based processing.\n","\n","### Steps in the Project\n","1. **Dataset Preparation**  \n","   - Load the XSum dataset with articles and summaries.  \n","   - Tokenize and pad sequences so they can be fed into the models.\n","\n","2. **Seq2Seq Model (LSTM)**  \n","   - Build an encoder-decoder model without attention.\n","   - Train it to generate summaries from the input articles.  \n","\n","3. **Transformer Model**  \n","   - Build a Transformer-based encoder-decoder model.  \n","   - Use self-attention to capture relationships between all tokens.  \n","   - Train on the same dataset to generate summaries.\n","\n","4. **Comparison**  \n","   - Compare the two models using metrics like ROUGE.  \n","   - Look at differences in summary quality, speed, and how well they handle long sequences.\n"]},{"cell_type":"markdown","id":"306d6f21","metadata":{"papermill":{"duration":0.006251,"end_time":"2025-11-15T16:33:33.725152","exception":false,"start_time":"2025-11-15T16:33:33.718901","status":"completed"},"tags":[]},"source":["# Seq2Seq and Encoder-Decoder\n","\n","## What is a Seq2Seq Model\n","A sequence-to-sequence (Seq2Seq) model is designed to take an input sequence and produce an output sequence. It’s widely used in tasks like machine translation, text summarization, and chatbots.\n","\n","**Example:**  \n","Input: \"Hello, how are you?\"  \n","Output: \"Ciao, come stai?\"\n","\n","---\n","\n","## Encoder-Decoder Architecture (Expanded)\n","\n","A typical Seq2Seq model has two main parts: the **encoder** and the **decoder**. The design allows the model to process sequences of variable length.  \n","\n","### Encoder\n","The encoder reads the input sequence and compresses it into a set of hidden states or a context vector. This vector captures the important information from the input and has a fixed size, though it does not need to match the decoder's size. The hidden states can either be passed as a whole to the decoder or connected at every decoding step.  \n","\n","At each step, the encoder updates its hidden state based on the previous hidden state and the current input. In mathematical terms, for a simple RNN:\n","\n","$$\n","H_t^{encoder} = \\phi(W_{HH} \\cdot H_{t-1}^{encoder} + W_{HX} \\cdot X_t)\n","$$\n","\n","Where:  \n","- $H_t^{encoder}$ = hidden state at time $t$ in the encoder  \n","- $X_t$ = input at time $t$  \n","- $W_{HH}$ = weight matrix connecting hidden states  \n","- $W_{HX}$ = weight matrix connecting input to hidden states  \n","- $\\phi$ = activation function (e.g., tanh or ReLU)\n","\n","---\n","\n","### Decoder\n","The decoder generates the output sequence one token at a time. Its initial hidden state is set to the final hidden state of the encoder. For a simple RNN decoder:\n","\n","$$\n","H_t^{decoder} = \\phi(W_{HH} \\cdot H_{t-1}^{decoder} + W_{HY} \\cdot Y_{t-1})\n","$$\n","\n","The output at each step is computed as:\n","\n","$$\n","Y_t = W_{HY} \\cdot H_t^{decoder}\n","$$\n","\n","Where:  \n","- $H_t^{decoder}$ = hidden state at time $t$ in the decoder  \n","- $Y_t$ = output at time $t$  \n","- $W_{HY}$ = weight matrix connecting decoder hidden state to output  \n","\n","### Implementation Notes\n","- Encoders and decoders are typically implemented with **RNNs, LSTMs, or GRUs**.  \n","- The input and output vectors are of fixed size, but the encoder and decoder can have different hidden dimensions.  \n","- During training, **teacher forcing** is often used, providing the correct previous token to the decoder instead of its own prediction.  \n","\n","---\n","\n","## Tokenization\n","\n","Before feeding text into a Seq2Seq or Transformer model, the raw text must be converted into numerical form.  \n","This is done through **tokenization**, which splits text into smaller units (tokens) such as words or subwords.  \n","\n","Each token is then mapped to a unique integer using a **vocabulary** built from the dataset.  \n","The model processes these integers rather than the raw text.\n","\n","**Example:**\n","\n","Input text: `\"Transformers improve summarization.\"`  \n","Tokens: `[\"transformers\", \"improve\", \"summarization\", \".\"]`  \n","Token IDs: `[201, 57, 1342, 4]`\n","\n","### Why Tokenization Matters\n","- Converts variable-length text into consistent, model-readable sequences.  \n","- Helps capture word frequency and context relationships.  \n","- Reduces vocabulary size when using subword tokenization (e.g., Byte Pair Encoding).  \n","\n","In this project, tokenization is part of preprocessing and includes:\n","- **Lowercasing** the text  \n","- **Removing special characters and URLs**  \n","- **Splitting into tokens by spaces**  \n","- Adding **start (`sostok`)** and **end (`eostok`)** tokens to mark summary boundaries  \n","\n","After tokenization, sequences will later be converted to integer IDs, padded or truncated to a fixed length\n","\n","---\n","\n","# Transformers\n","Transformers can be seen as an evolution of Seq2Seq models. Instead of processing sequences step by step like LSTMs or GRUs, they rely entirely on **attention mechanisms** to process all tokens in parallel and capture relationships between them.\n","\n","### Attention in Transformers\n","Attention is the core mechanism that allows Transformers to focus on relevant parts of the input sequence when producing a representation for each token. It works by comparing each token to all others and weighting them according to importance.\n","\n","#### How Attention Works\n","Each token in the sequence is represented by three vectors:\n","- **Query (Q):** what this token is looking for  \n","- **Key (K):** what information this token contains  \n","- **Value (V):** the actual information of the token  \n","\n","The attention score between two tokens is computed as the similarity between the Query of one token and the Key of another. This determines how much attention one token should pay to another. Mathematically, the attention weights are computed using a scaled dot-product:\n","\n","$$\n","\\text{Attention}(Q, K, V) = \\text{softmax}\\Big(\\frac{QK^T}{\\sqrt{d_k}}\\Big) V\n","$$\n","\n","Where $d_k$ is the dimensionality of the Key vectors.\n","\n","- The **softmax** ensures that the weights sum to 1.  \n","- Each token’s output is a weighted sum of all Value vectors, allowing it to incorporate context from the entire sequence.\n","\n","#### Multi-Head Attention\n","Instead of computing attention just once, Transformers use **multiple attention heads** in parallel. Each head can learn to focus on different types of relationships, such as:\n","- Syntactic relationships (e.g., subject-verb connections)  \n","- Semantic relationships (e.g., synonyms or related concepts)  \n","\n","The outputs of all heads are concatenated and projected to form the final representation for each token.\n","\n","#### Intuition\n","Imagine reading a sentence and highlighting all the words that are important for understanding each token. Each word “attends” to other words in the sentence that matter most for its meaning. Multi-head attention lets the model do this from multiple perspectives simultaneously.\n","\n","### Key Components of Transformers\n","- **Encoder-Decoder Structure:** Like Seq2Seq models, Transformers have an encoder that processes the input and a decoder that generates the output. Both use layers of self-attention and feed-forward networks.  \n","- **Positional Encoding:** Since Transformers don’t process tokens sequentially, they add positional information so the model knows the order of tokens.  \n","- **Feed-Forward Layers:** After attention, each token passes through fully connected layers for additional transformation.\n","\n","### Advantages over LSTM/GRU Seq2Seq\n","- Processes sequences **in parallel**, speeding up training.  \n","- Handles **long sequences** more effectively with attention.  \n","- Captures **complex relationships** between tokens regardless of distance.  \n","- Scales easily to **very deep models** and large datasets.\n","\n","### Use Cases\n","Transformers are the backbone of many state-of-the-art models for tasks such as:\n","- Machine translation (e.g., T5, MarianMT)  \n","- Text summarization (e.g., BART, Pegasus)  \n","- Question answering and chatbots (e.g., GPT, BERT-based models)"]},{"cell_type":"code","execution_count":1,"id":"7e44d269","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:33:33.738908Z","iopub.status.busy":"2025-11-15T16:33:33.738688Z","iopub.status.idle":"2025-11-15T16:33:40.739223Z","shell.execute_reply":"2025-11-15T16:33:40.738513Z"},"papermill":{"duration":7.009193,"end_time":"2025-11-15T16:33:40.740919","exception":false,"start_time":"2025-11-15T16:33:33.731726","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\r\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\r\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\r\n"]}],"source":["import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","!pip install openpyxl\n","!pip install sentencepiece"]},{"cell_type":"markdown","id":"ffa157a1","metadata":{"papermill":{"duration":0.006462,"end_time":"2025-11-15T16:33:40.755174","exception":false,"start_time":"2025-11-15T16:33:40.748712","status":"completed"},"tags":[]},"source":["# Data Preparation\n","\n","Prepare and clean the dataset for the summarization model:\n","\n","- **Load datasets:** Read two CSV files containing news articles and their summaries.\n","- **Combine datasets:** Merge datasets while selecting relevant `text` and `summary` columns.\n","- **Text cleaning:**  \n","  - Convert text to lowercase.  \n","  - Remove special characters.  \n","  - Replace URLs with domain names.  \n","  - Reduce multiple spaces.\n","- **Tokenization:** Split cleaned text into tokens (words) and add `_START_` and `_END_` tokens for summaries.\n","- **Handle missing values:** Drop rows with missing `text` values.\n","- **Analyze sequence lengths:** Calculate word counts for texts and summaries.\n","- **Limit sequence lengths:** Restrict `text` to 100 words and `summary` to 15 words.\n","- **Add model tokens:** Prepend `sostok` and append `eostok` to all summaries to mark start and end for the model.\n"]},{"cell_type":"code","execution_count":2,"id":"a14e5b31","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:33:40.769532Z","iopub.status.busy":"2025-11-15T16:33:40.769271Z","iopub.status.idle":"2025-11-15T16:33:48.320556Z","shell.execute_reply":"2025-11-15T16:33:48.319802Z"},"papermill":{"duration":7.560018,"end_time":"2025-11-15T16:33:48.321813","exception":false,"start_time":"2025-11-15T16:33:40.761795","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["(4514, 6)\n","(98401, 2)\n","(55104, 5)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","pd.set_option('display.max_colwidth', None)\n","\n","summary = pd.read_csv('/kaggle/input/news-summary/news_summary.csv', encoding='iso-8859-1')\n","summary_more = pd.read_csv('/kaggle/input/news-summary/news_summary_more.csv', encoding='iso-8859-1')\n","summary_ind = pd.read_excel(\"/kaggle/input/inshorts-news-data/Inshorts Cleaned Data.xlsx\")\n","\n","summary['text'] = '<news_summary> ' + summary['text']\n","summary_more['text'] = '<news_summary_more> ' + summary_more['text']\n","summary_ind['Short'] = '<news_ind> ' + summary_ind['Short']\n","\n","print(summary.shape)\n","print(summary_more.shape)\n","print(summary_ind.shape)"]},{"cell_type":"code","execution_count":3,"id":"c67cefbd","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:33:48.336799Z","iopub.status.busy":"2025-11-15T16:33:48.336444Z","iopub.status.idle":"2025-11-15T16:33:48.354333Z","shell.execute_reply":"2025-11-15T16:33:48.353664Z"},"papermill":{"duration":0.026644,"end_time":"2025-11-15T16:33:48.355535","exception":false,"start_time":"2025-11-15T16:33:48.328891","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>date</th>\n","      <th>headlines</th>\n","      <th>read_more</th>\n","      <th>text</th>\n","      <th>ctext</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Chhavi Tyagi</td>\n","      <td>03 Aug 2017,Thursday</td>\n","      <td>Daman &amp; Diu revokes mandatory Rakshabandhan in offices order</td>\n","      <td>http://www.hindustantimes.com/india-news/rakshabandhan-compulsory-in-daman-and-diu-women-employees-to-tie-rakhis-to-male-colleagues/story-E5h5U1ZDJii5zFpLXWRkhJ.html?utm_source=inshorts&amp;utm_medium=referral&amp;utm_campaign=fullarticle</td>\n","      <td>&lt;news_summary&gt; The Administration of Union Territory Daman and Diu has revoked its order that made it compulsory for women to tie rakhis to their male colleagues on the occasion of Rakshabandhan on August 7. The administration was forced to withdraw the decision within 24 hours of issuing the circular after it received flak from employees and was slammed on social media.</td>\n","      <td>The Daman and Diu administration on Wednesday withdrew a circular that asked women staff to tie rakhis on male colleagues after the order triggered a backlash from employees and was ripped apart on social media.The union territory?s administration was forced to retreat within 24 hours of issuing the circular that made it compulsory for its staff to celebrate Rakshabandhan at workplace.?It has been decided to celebrate the festival of Rakshabandhan on August 7. In this connection, all offices/ departments shall remain open and celebrate the festival collectively at a suitable time wherein all the lady staff shall tie rakhis to their colleagues,? the order, issued on August 1 by Gurpreet Singh, deputy secretary (personnel), had said.To ensure that no one skipped office, an attendance report was to be sent to the government the next evening.The two notifications ? one mandating the celebration of Rakshabandhan (left) and the other withdrawing the mandate (right) ? were issued by the Daman and Diu administration a day apart. The circular was withdrawn through a one-line order issued late in the evening by the UT?s department of personnel and administrative reforms.?The circular is ridiculous. There are sensitivities involved. How can the government dictate who I should tie rakhi to? We should maintain the professionalism of a workplace? an official told Hindustan Times earlier in the day. She refused to be identified.The notice was issued on Daman and Diu administrator and former Gujarat home minister Praful Kodabhai Patel?s direction, sources said.Rakshabandhan, a celebration of the bond between brothers and sisters, is one of several Hindu festivities and rituals that are no longer confined of private, family affairs but have become tools to push politic al ideologies.In 2014, the year BJP stormed to power at the Centre, Rashtriya Swayamsevak Sangh (RSS) chief Mohan Bhagwat said the festival had ?national significance? and should be celebrated widely ?to protect Hindu culture and live by the values enshrined in it?. The RSS is the ideological parent of the ruling BJP.Last year, women ministers in the Modi government went to the border areas to celebrate the festival with soldiers. A year before, all cabinet ministers were asked to go to their constituencies for the festival.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         author                  date  \\\n","0  Chhavi Tyagi  03 Aug 2017,Thursday   \n","\n","                                                      headlines  \\\n","0  Daman & Diu revokes mandatory Rakshabandhan in offices order   \n","\n","                                                                                                                                                                                                                                 read_more  \\\n","0  http://www.hindustantimes.com/india-news/rakshabandhan-compulsory-in-daman-and-diu-women-employees-to-tie-rakhis-to-male-colleagues/story-E5h5U1ZDJii5zFpLXWRkhJ.html?utm_source=inshorts&utm_medium=referral&utm_campaign=fullarticle    \n","\n","                                                                                                                                                                                                                                                                                                                                                                                    text  \\\n","0  <news_summary> The Administration of Union Territory Daman and Diu has revoked its order that made it compulsory for women to tie rakhis to their male colleagues on the occasion of Rakshabandhan on August 7. The administration was forced to withdraw the decision within 24 hours of issuing the circular after it received flak from employees and was slammed on social media.   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ctext  \n","0  The Daman and Diu administration on Wednesday withdrew a circular that asked women staff to tie rakhis on male colleagues after the order triggered a backlash from employees and was ripped apart on social media.The union territory?s administration was forced to retreat within 24 hours of issuing the circular that made it compulsory for its staff to celebrate Rakshabandhan at workplace.?It has been decided to celebrate the festival of Rakshabandhan on August 7. In this connection, all offices/ departments shall remain open and celebrate the festival collectively at a suitable time wherein all the lady staff shall tie rakhis to their colleagues,? the order, issued on August 1 by Gurpreet Singh, deputy secretary (personnel), had said.To ensure that no one skipped office, an attendance report was to be sent to the government the next evening.The two notifications ? one mandating the celebration of Rakshabandhan (left) and the other withdrawing the mandate (right) ? were issued by the Daman and Diu administration a day apart. The circular was withdrawn through a one-line order issued late in the evening by the UT?s department of personnel and administrative reforms.?The circular is ridiculous. There are sensitivities involved. How can the government dictate who I should tie rakhi to? We should maintain the professionalism of a workplace? an official told Hindustan Times earlier in the day. She refused to be identified.The notice was issued on Daman and Diu administrator and former Gujarat home minister Praful Kodabhai Patel?s direction, sources said.Rakshabandhan, a celebration of the bond between brothers and sisters, is one of several Hindu festivities and rituals that are no longer confined of private, family affairs but have become tools to push politic al ideologies.In 2014, the year BJP stormed to power at the Centre, Rashtriya Swayamsevak Sangh (RSS) chief Mohan Bhagwat said the festival had ?national significance? and should be celebrated widely ?to protect Hindu culture and live by the values enshrined in it?. The RSS is the ideological parent of the ruling BJP.Last year, women ministers in the Modi government went to the border areas to celebrate the festival with soldiers. A year before, all cabinet ministers were asked to go to their constituencies for the festival.  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["summary.head(1)"]},{"cell_type":"code","execution_count":4,"id":"3f143e54","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:33:48.370346Z","iopub.status.busy":"2025-11-15T16:33:48.370147Z","iopub.status.idle":"2025-11-15T16:33:48.376144Z","shell.execute_reply":"2025-11-15T16:33:48.375548Z"},"papermill":{"duration":0.014649,"end_time":"2025-11-15T16:33:48.377166","exception":false,"start_time":"2025-11-15T16:33:48.362517","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>headlines</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>upGrad learner switches to career in ML &amp; Al with 90% salary hike</td>\n","      <td>&lt;news_summary_more&gt; Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                           headlines  \\\n","0  upGrad learner switches to career in ML & Al with 90% salary hike   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                          text  \n","0  <news_summary_more> Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["summary_more.head(1)"]},{"cell_type":"code","execution_count":5,"id":"f16d2685","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:33:48.392364Z","iopub.status.busy":"2025-11-15T16:33:48.392175Z","iopub.status.idle":"2025-11-15T16:33:48.402768Z","shell.execute_reply":"2025-11-15T16:33:48.402219Z"},"papermill":{"duration":0.019352,"end_time":"2025-11-15T16:33:48.403815","exception":false,"start_time":"2025-11-15T16:33:48.384463","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Headline</th>\n","      <th>Short</th>\n","      <th>Source</th>\n","      <th>Time</th>\n","      <th>Publish Date</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4 ex-bank officials booked for cheating bank of ₹209 crore</td>\n","      <td>&lt;news_ind&gt; The CBI on Saturday booked four former officials of Syndicate Bank and six others for cheating, forgery, criminal conspiracy and causing ₹209 crore loss to the state-run bank. The accused had availed home loans and credit from Syndicate Bank on the basis of forged and fabricated documents. These funds were fraudulently transferred to the companies owned by the accused persons.</td>\n","      <td>The New Indian Express</td>\n","      <td>09:25:00</td>\n","      <td>2017-03-26</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                     Headline  \\\n","0  4 ex-bank officials booked for cheating bank of ₹209 crore   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                    Short  \\\n","0  <news_ind> The CBI on Saturday booked four former officials of Syndicate Bank and six others for cheating, forgery, criminal conspiracy and causing ₹209 crore loss to the state-run bank. The accused had availed home loans and credit from Syndicate Bank on the basis of forged and fabricated documents. These funds were fraudulently transferred to the companies owned by the accused persons.   \n","\n","                  Source      Time  Publish Date  \n","0  The New Indian Express  09:25:00   2017-03-26  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["summary_ind.head(1)"]},{"cell_type":"code","execution_count":6,"id":"632fe2ee","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:33:48.418628Z","iopub.status.busy":"2025-11-15T16:33:48.418431Z","iopub.status.idle":"2025-11-15T16:33:48.481411Z","shell.execute_reply":"2025-11-15T16:33:48.480711Z"},"papermill":{"duration":0.071571,"end_time":"2025-11-15T16:33:48.482537","exception":false,"start_time":"2025-11-15T16:33:48.410966","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["(158019, 2)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>&lt;news_summary_more&gt; Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.</td>\n","      <td>upGrad learner switches to career in ML &amp; Al with 90% salary hike</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>&lt;news_summary_more&gt; Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.</td>\n","      <td>Delhi techie wins free food from Swiggy for one year on CRED</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                                                                                                                                                                                                                                                                                                                                          text  \\\n","0  <news_summary_more> Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.   \n","1                              <news_summary_more> Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.   \n","\n","                                                             summary  \n","0  upGrad learner switches to career in ML & Al with 90% salary hike  \n","1       Delhi techie wins free food from Swiggy for one year on CRED  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["summary = summary.iloc[:, 0:6]\n","summary_more = summary_more.iloc[:, 0:2]\n","\n","# To increase the intake of possible text values to build a reliable model\n","summary['text'] = (\n","    summary['author'] + ' ' +\n","    summary['date'] + ' ' +\n","    summary['read_more'] + ' ' +\n","    summary['text'] + ' ' +\n","    summary['ctext']\n",")\n","\n","\n","df = pd.DataFrame()\n","\n","df['text'] = pd.concat([summary_more['text'], summary['text'], summary_ind['Short']], ignore_index=True)\n","df['summary'] = pd.concat([summary_more['headlines'], summary['headlines'], summary_ind['Headline']], ignore_index=True)\n","\n","df_trans = df\n","\n","print(df.shape)\n","df.head(2)"]},{"cell_type":"code","execution_count":7,"id":"6c49cf3f","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:33:48.498982Z","iopub.status.busy":"2025-11-15T16:33:48.498519Z","iopub.status.idle":"2025-11-15T16:33:53.686724Z","shell.execute_reply":"2025-11-15T16:33:53.685939Z"},"papermill":{"duration":5.197676,"end_time":"2025-11-15T16:33:53.688125","exception":false,"start_time":"2025-11-15T16:33:48.490449","status":"completed"},"tags":[]},"outputs":[],"source":["import re\n","\n","def clean_text(text):\n","    text = str(text)\n","    text = re.sub(r'https?://\\S+', '', text)  \n","    text = re.sub(r'\\s+', ' ', text).strip()  \n","    return text\n","\n","\n","# Tokenization: split text by spaces\n","def tokenize_texts(texts):\n","    return [' '.join(clean_text(t).split()) for t in texts]\n","\n","\n","processed_text = tokenize_texts(df['text'])\n","#processed_summary = ['_START_ ' + s + ' _END_' for s in tokenize_texts(df['summary'])]\n","processed_summary = ['sostok ' + s + ' eostok' for s in tokenize_texts(df['summary'])]\n","\n","\n","df['cleaned_text'] = pd.Series(processed_text)\n","df['cleaned_summary'] = pd.Series(processed_summary)"]},{"cell_type":"code","execution_count":8,"id":"4c1d3233","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:33:53.705487Z","iopub.status.busy":"2025-11-15T16:33:53.705242Z","iopub.status.idle":"2025-11-15T16:33:54.090771Z","shell.execute_reply":"2025-11-15T16:33:54.089935Z"},"papermill":{"duration":0.39486,"end_time":"2025-11-15T16:33:54.09211","exception":false,"start_time":"2025-11-15T16:33:53.69725","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["NaN dropped: 118\n","(157886, 4)\n"]}],"source":["print(f\"NaN dropped: {df.isna().sum().sum()}\")\n","\n","#df = df.dropna(subset=['text'])\n","df = df.dropna(subset=['cleaned_text', 'cleaned_summary'])\n","df = df.drop_duplicates(subset=['cleaned_text', 'cleaned_summary'])\n","\n","print(df.shape)"]},{"cell_type":"code","execution_count":9,"id":"516f63eb","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:33:54.108612Z","iopub.status.busy":"2025-11-15T16:33:54.108144Z","iopub.status.idle":"2025-11-15T16:33:55.312507Z","shell.execute_reply":"2025-11-15T16:33:55.311449Z"},"papermill":{"duration":1.214519,"end_time":"2025-11-15T16:33:55.314495","exception":false,"start_time":"2025-11-15T16:33:54.099976","status":"completed"},"tags":[]},"outputs":[{"data":{"image/png":"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\n","text/plain":["<Figure size 640x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","\n","text_count = []\n","summary_count = []\n","\n","for sent in df['cleaned_text']:\n","    text_count.append(len(sent.split()))\n","    \n","for sent in df['cleaned_summary']:\n","    summary_count.append(len(sent.split()))\n","\n","graph_df = pd.DataFrame() \n","\n","graph_df['text'] = text_count\n","graph_df['summary'] = summary_count\n","\n","graph_df.hist(bins = 30)\n","plt.show()"]},{"cell_type":"code","execution_count":10,"id":"c7a34462","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:33:55.338703Z","iopub.status.busy":"2025-11-15T16:33:55.338469Z","iopub.status.idle":"2025-11-15T16:33:55.951156Z","shell.execute_reply":"2025-11-15T16:33:55.950419Z"},"papermill":{"duration":0.624128,"end_time":"2025-11-15T16:33:55.952254","exception":false,"start_time":"2025-11-15T16:33:55.328126","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Percentage of texts with less than 100 words : 0.9724991449526874\n"]}],"source":["cnt = 0\n","for i in df['cleaned_text']:\n","    if len(i.split()) <= 100:\n","        cnt = cnt + 1\n","print(f\"Percentage of texts with less than 100 words : {cnt / len(df['cleaned_text'])}\")"]},{"cell_type":"code","execution_count":11,"id":"721e9948","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:33:55.968969Z","iopub.status.busy":"2025-11-15T16:33:55.968745Z","iopub.status.idle":"2025-11-15T16:33:58.567421Z","shell.execute_reply":"2025-11-15T16:33:58.566668Z"},"papermill":{"duration":2.608182,"end_time":"2025-11-15T16:33:58.568536","exception":false,"start_time":"2025-11-15T16:33:55.960354","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>summary</th>\n","      <th>cleaned_text</th>\n","      <th>cleaned_summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>&lt;news_summary_more&gt; Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.</td>\n","      <td>upGrad learner switches to career in ML &amp; Al with 90% salary hike</td>\n","      <td>&lt;news_summary_more&gt; Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.</td>\n","      <td>sostok sostok upGrad learner switches to career in ML &amp; Al with 90% salary hike eostok eostok</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>&lt;news_summary_more&gt; Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.</td>\n","      <td>Delhi techie wins free food from Swiggy for one year on CRED</td>\n","      <td>&lt;news_summary_more&gt; Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.</td>\n","      <td>sostok sostok Delhi techie wins free food from Swiggy for one year on CRED eostok eostok</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                                                                                                                                                                                                                                                                                                                                          text  \\\n","0  <news_summary_more> Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.   \n","1                              <news_summary_more> Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.   \n","\n","                                                             summary  \\\n","0  upGrad learner switches to career in ML & Al with 90% salary hike   \n","1       Delhi techie wins free food from Swiggy for one year on CRED   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                  cleaned_text  \\\n","0  <news_summary_more> Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.   \n","1                              <news_summary_more> Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.   \n","\n","                                                                                 cleaned_summary  \n","0  sostok sostok upGrad learner switches to career in ML & Al with 90% salary hike eostok eostok  \n","1       sostok sostok Delhi techie wins free food from Swiggy for one year on CRED eostok eostok  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["max_text_len = 100\n","max_summary_len = 20\n","\n","df['cleaned_text'] = df['cleaned_text'].astype(str)\n","df['cleaned_summary'] = df['cleaned_summary'].astype(str)\n","\n","mask = (df['cleaned_text'].str.split().str.len() <= max_text_len) & \\\n","       (df['cleaned_summary'].str.split().str.len() <= max_summary_len)\n","\n","df = df.loc[mask].reset_index(drop=True)\n","\n","# Add start and end tokens to each summary\n","df['cleaned_summary'] = df['cleaned_summary'].apply(lambda x: 'sostok ' + x + ' eostok')\n","\n","df.head(2)"]},{"cell_type":"markdown","id":"e6c8e6a6","metadata":{"papermill":{"duration":0.007836,"end_time":"2025-11-15T16:33:58.584795","exception":false,"start_time":"2025-11-15T16:33:58.576959","status":"completed"},"tags":[]},"source":["# **Tokenization**\n","\n","This block prepares the text data for a sequence-to-sequence model:\n","\n","- **Split dataset**: Separates `text` and `summary` into training and validation sets to evaluate model performance on unseen data.  \n","- **Initialize tokenizers**: Converts words into integer indices, which neural networks can process.  \n","- **Analyze rare words**: Computes the percentage of words appearing less than `thresh` times to identify infrequent words that might add noise.  \n","- **Limit vocabulary to frequent words**: Reduces vocabulary size by ignoring rare words, which improves training efficiency and prevents overfitting.  \n","- **Convert texts to sequences**: Maps each word in the texts to its corresponding integer index.  \n","- **Pad sequences**: Ensures all sequences have the same length, necessary for batch processing in neural networks.  \n","- **Compute final vocabulary size**: Includes the padding token to correctly define the input dimension for the model embedding layer."]},{"cell_type":"code","execution_count":12,"id":"efbea518","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:33:58.64574Z","iopub.status.busy":"2025-11-15T16:33:58.645454Z","iopub.status.idle":"2025-11-15T16:34:21.244973Z","shell.execute_reply":"2025-11-15T16:34:21.244346Z"},"papermill":{"duration":22.653424,"end_time":"2025-11-15T16:34:21.246346","exception":false,"start_time":"2025-11-15T16:33:58.592922","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-11-15 16:34:01.634304: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1763224441.860139      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1763224441.923450      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer \n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","x_train, x_val, y_train, y_val = train_test_split(\n","    np.array(df[\"cleaned_text\"]),\n","    np.array(df[\"cleaned_summary\"]),\n","    test_size=0.1,\n","    random_state=0,\n","    shuffle=True,\n",")\n","\n","x_tokenizer = Tokenizer(oov_token=\"<unk>\") \n","x_tokenizer.fit_on_texts(list(x_train))\n","\n","y_tokenizer = Tokenizer(oov_token=\"<unk>\")   \n","y_tokenizer.fit_on_texts(list(y_train))"]},{"cell_type":"code","execution_count":13,"id":"6cb4e836","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:34:21.264336Z","iopub.status.busy":"2025-11-15T16:34:21.26386Z","iopub.status.idle":"2025-11-15T16:34:21.314171Z","shell.execute_reply":"2025-11-15T16:34:21.313437Z"},"papermill":{"duration":0.06014,"end_time":"2025-11-15T16:34:21.315222","exception":false,"start_time":"2025-11-15T16:34:21.255082","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["% of rare words in X vocabulary: 57.17%\n","% of rare words in Y vocabulary: 55.67%\n"]}],"source":["thresh = 3\n","\n","def rare_word_stats(tokenizer, thresh):\n","    \"\"\"Return total and rare word counts for a given tokenizer.\"\"\"\n","    total_cnt = len(tokenizer.word_counts)\n","    rare_cnt = sum(1 for word, count in tokenizer.word_counts.items() if count < thresh)\n","    return total_cnt, rare_cnt\n","\n","\n","x_tot_cnt, x_cnt = rare_word_stats(x_tokenizer, thresh)\n","y_tot_cnt, y_cnt = rare_word_stats(y_tokenizer, thresh)\n","\n","print(f\"% of rare words in X vocabulary: {(x_cnt / x_tot_cnt) * 100:.2f}%\")\n","print(f\"% of rare words in Y vocabulary: {(y_cnt / y_tot_cnt) * 100:.2f}%\")"]},{"cell_type":"code","execution_count":14,"id":"1e1e6024","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:34:21.332316Z","iopub.status.busy":"2025-11-15T16:34:21.332103Z","iopub.status.idle":"2025-11-15T16:34:36.793629Z","shell.execute_reply":"2025-11-15T16:34:36.792806Z"},"papermill":{"duration":15.4712,"end_time":"2025-11-15T16:34:36.79478","exception":false,"start_time":"2025-11-15T16:34:21.32358","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Size of vocabulary in X = 53389\n","Size of vocabulary in Y = 22916\n"]}],"source":["# Create tokenizers considering only frequent words\n","x_tokenizer = Tokenizer(num_words = x_tot_cnt - x_cnt) \n","x_tokenizer.fit_on_texts(list(x_train))\n","\n","y_tokenizer = Tokenizer(num_words=y_tot_cnt-y_cnt) \n","y_tokenizer.fit_on_texts(list(y_train))\n","\n","# Convert text to sequences of integers\n","x_train_seq = x_tokenizer.texts_to_sequences(x_train) \n","x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n","\n","y_train_seq = y_tokenizer.texts_to_sequences(y_train) \n","y_val_seq = y_tokenizer.texts_to_sequences(y_val) \n","\n","# Pad sequences\n","x_train = pad_sequences(x_train_seq,  maxlen=max_text_len, padding='post')\n","x_val = pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n","\n","y_train = pad_sequences(y_train_seq, maxlen=max_summary_len, padding='post')\n","y_val = pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n","\n","# Vocab. size (+1 for padding token)\n","x_voc_size = x_tokenizer.num_words + 1\n","y_voc_size = y_tokenizer.num_words + 1\n","\n","print(f\"Size of vocabulary in X = {x_voc_size}\")\n","print(f\"Size of vocabulary in Y = {y_voc_size}\")"]},{"cell_type":"code","execution_count":15,"id":"4aaaa0e7","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:34:36.812501Z","iopub.status.busy":"2025-11-15T16:34:36.81225Z","iopub.status.idle":"2025-11-15T16:34:36.874896Z","shell.execute_reply":"2025-11-15T16:34:36.874093Z"},"papermill":{"duration":0.072962,"end_time":"2025-11-15T16:34:36.876332","exception":false,"start_time":"2025-11-15T16:34:36.80337","status":"completed"},"tags":[]},"outputs":[],"source":["# Finally remove from the dataset empty summaries that contain only the 'START' and 'END' tokens\n","\n","x_train = x_train[np.sum(y_train != 0, axis=1) > 2]\n","y_train = y_train[np.sum(y_train != 0, axis=1) > 2]\n","\n","x_val = x_val[np.sum(y_val != 0, axis=1) > 2]\n","y_val = y_val[np.sum(y_val != 0, axis=1) > 2]"]},{"cell_type":"markdown","id":"5a0d7d23","metadata":{"papermill":{"duration":0.007972,"end_time":"2025-11-15T16:34:36.892957","exception":false,"start_time":"2025-11-15T16:34:36.884985","status":"completed"},"tags":[]},"source":["# **Seq2seq model using LSTM**\n","\n","### Encoder-Decoder Architecture with LSTM\n","\n","During training, the model takes **two inputs**:  \n","1. The encoder input (`text`) – the original text sequence.  \n","2. The decoder input (`summary`) – the summary shifted by one token (so that the model learns to predict the next word).  \n","\n","The **target output** is the summary sequence shifted forward by one token. The model learns to predict the next word in the summary based on the previous words. During inference, the trained model generates summaries one word at a time, using the previously predicted words as input.\n","\n","---\n","\n","**Encoder**  \n","- The encoder accepts sequences of text with a fixed length (`max_text_len`).  \n","- The text is first passed through an **Embedding layer** that maps each word index to a dense vector of size `(embedding_dim)`.  \n","- The embedded sequence is then processed by **three stacked LSTM layers**:  \n","  - Each layer outputs the **full sequence of hidden states** (for possible attention or stacking) and the **last hidden and cell states**.  \n","  - The last hidden and cell states from the final LSTM are used to initialize the decoder.  \n","- Stacking multiple LSTMs allows the encoder to **capture both local patterns and long-range dependencies** in the text.\n","\n","---\n","\n","**Decoder**  \n","- The decoder input (shifted summary) is passed through an **Embedding layer** of size `(summary vocabulary size x embedding_dim)`.  \n","- A single **LSTM** processes the embedded sequence, using the **encoder's last hidden and cell states** as its initial state.  \n","- The LSTM output is passed through a **TimeDistributed Dense layer** with **softmax activation**, which predicts the probability of each word in the vocabulary at each time step.  \n","\n","This architecture ensures that the decoder can generate the summary step by step, **learning the sequence of words conditioned on the input text**.\n"]},{"cell_type":"code","execution_count":16,"id":"ba88530d","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:34:36.910186Z","iopub.status.busy":"2025-11-15T16:34:36.909943Z","iopub.status.idle":"2025-11-15T16:34:39.258312Z","shell.execute_reply":"2025-11-15T16:34:39.257615Z"},"papermill":{"duration":2.358471,"end_time":"2025-11-15T16:34:39.25942","exception":false,"start_time":"2025-11-15T16:34:36.900949","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["I0000 00:00:1763224477.708283      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10,677,800</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">601,200</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]             │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">721,200</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]             │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,583,200</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">721,200</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]             │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">601,200</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]             │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ time_distributed    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">6,897,716</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">22916</span>)            │            │                   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m200\u001b[0m)  │ \u001b[38;5;34m10,677,800\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n","│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m,      │    \u001b[38;5;34m601,200\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n","│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n","│                     │ \u001b[38;5;34m300\u001b[0m)]             │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m,      │    \u001b[38;5;34m721,200\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n","│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n","│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n","│                     │ \u001b[38;5;34m300\u001b[0m)]             │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m) │  \u001b[38;5;34m4,583,200\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n","│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m,      │    \u001b[38;5;34m721,200\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n","│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n","│                     │ \u001b[38;5;34m300\u001b[0m)]             │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m601,200\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n","│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n","│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n","│                     │ \u001b[38;5;34m300\u001b[0m)]             │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ time_distributed    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m6,897,716\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m22916\u001b[0m)            │            │                   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,803,516</span> (94.62 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,803,516\u001b[0m (94.62 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,803,516</span> (94.62 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m24,803,516\u001b[0m (94.62 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.callbacks import EarlyStopping\n","#from tensorflow.keras import mixed_precision\n","\n","#mixed_precision.set_global_policy('mixed_float16')\n","\n","latent_dim = 300\n","embedding_dim = 200\n","\n","# Encoder\n","encoder_inputs = Input(shape=(max_text_len, ))\n","\n","# Embedding layer\n","enc_emb = Embedding(x_voc_size, embedding_dim,\n","                    trainable=True)(encoder_inputs)\n","\n","# Encoder LSTM 1\n","encoder_lstm1 = LSTM(latent_dim, return_sequences=True,\n","                     return_state=True, dropout=0.4,\n","                     recurrent_dropout=0.4)\n","(encoder_output1, state_h1, state_c1) = encoder_lstm1(enc_emb)\n","\n","# Encoder LSTM 2\n","encoder_lstm2 = LSTM(latent_dim, return_sequences=True,\n","                     return_state=True, dropout=0.4,\n","                     recurrent_dropout=0.4)\n","(encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)\n","\n","# Encoder LSTM 3\n","encoder_lstm3 = LSTM(latent_dim, return_state=True,\n","                     return_sequences=True, dropout=0.4,\n","                     recurrent_dropout=0.4)\n","(encoder_outputs, state_h, state_c) = encoder_lstm3(encoder_output2)\n","\n","# Decoder\n","decoder_inputs = Input(shape=(None, ))\n","\n","# Embedding layer\n","dec_emb_layer = Embedding(y_voc_size, embedding_dim, trainable=True)\n","dec_emb = dec_emb_layer(decoder_inputs)\n","\n","# Decoder LSTM\n","decoder_lstm = LSTM(latent_dim, return_sequences=True,\n","                    return_state=True, dropout=0.4,\n","                    recurrent_dropout=0.2)\n","(decoder_outputs, decoder_fwd_state, decoder_back_state) = \\\n","    decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n","\n","# Dense layer\n","decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax'))\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Model\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","model.summary()"]},{"cell_type":"markdown","id":"14262916","metadata":{"papermill":{"duration":0.00846,"end_time":"2025-11-15T16:34:39.277293","exception":false,"start_time":"2025-11-15T16:34:39.268833","status":"completed"},"tags":[]},"source":["## **Training the model** "]},{"cell_type":"code","execution_count":17,"id":"f1eb0399","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:34:39.295086Z","iopub.status.busy":"2025-11-15T16:34:39.294859Z","iopub.status.idle":"2025-11-15T16:34:40.897334Z","shell.execute_reply":"2025-11-15T16:34:40.896753Z"},"papermill":{"duration":1.612906,"end_time":"2025-11-15T16:34:40.898694","exception":false,"start_time":"2025-11-15T16:34:39.285788","status":"completed"},"tags":[]},"outputs":[],"source":["x_train_lstm = x_train\n","x_val_lstm = x_val\n","y_train_lstm = y_train\n","y_val_lstm = y_val\n","\n","MODEL_INPUT_PATH = \"/kaggle/input/lstm-keras-summarization/keras/default/1/model_lstm.keras\" \n","MODEL_PATH = \"/kaggle/working/model_lstm.keras\" \n","\n","\n","if os.path.exists(MODEL_INPUT_PATH):\n","    model = load_model(MODEL_INPUT_PATH)\n","\n","else:\n","    model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n","    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n","\n","    history = model.fit(\n","        [x_train, y_train[:, :-1]],\n","        y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:, 1:],\n","        epochs=50,\n","        callbacks=[es],\n","        batch_size=128,\n","        validation_data=(\n","            [x_val, y_val[:, :-1]],\n","            y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:, 1:]\n","        )\n","    )\n","\n","    model.save(MODEL_PATH)\n","\n","    plt.plot(history.history['loss'], label='train')\n","    plt.plot(history.history['val_loss'], label='test')\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"markdown","id":"324bbc60","metadata":{"papermill":{"duration":0.008662,"end_time":"2025-11-15T16:34:40.916683","exception":false,"start_time":"2025-11-15T16:34:40.908021","status":"completed"},"tags":[]},"source":["## **Predict**\n","\n","Once the Seq2Seq model has been trained, we can use it to **generate summaries** for new input texts.  \n","This stage is known as **inference**, the model no longer learns, but uses its learned parameters to predict the most likely output sequence (summary).\n","\n","---\n","\n","### 1. Preparing the Mapping Dictionaries\n","Before generating predictions, we rebuild the word–token mappings from the tokenizers:\n","- `reverse_x_word_index`: converts article tokens → words  \n","- `reverse_y_word_index`: converts summary tokens → words  \n","- `y_word_index`: converts summary words → tokens  \n","\n","These dictionaries let us translate between the model’s numeric predictions and readable text.\n","\n","---\n","\n","### 2. Building Inference Models\n","During training, the encoder and decoder work together in a single model.  \n","At inference time, we separate them:\n","\n","- The **encoder model** processes the input text once and produces context vectors (`encoder_outputs`, `state_h`, `state_c`) — a compressed representation of the input.  \n","- The **decoder model** generates the summary **one word at a time**, taking as input the previous word and its previous internal states.\n","\n","This setup allows the decoder to iteratively predict each next token until the end-of-sequence marker (`eostok`) is reached.\n","\n","---\n","\n","### 3. The Decoding Process\n","The function `decode_sequence()` handles the actual text generation:\n","\n","1. **Encode the input sequence** using the encoder model to obtain its internal states.  \n","2. **Initialize** the decoder with the special start token (`sostok`).  \n","3. **Iteratively predict** the next word:\n","   - Feed the previous word and the latest decoder states into the model.\n","   - Pick the word with the highest probability (`argmax`).\n","   - Stop when the `eostok` token is predicted or the maximum summary length is reached.\n","4. **Concatenate** all predicted tokens into a readable summary.\n","\n","This process simulates how the model “writes” one word at a time, using its internal memory to maintain context.\n","\n","---\n","\n","### 4. Converting Sequences to Text\n","Two helper functions make the predictions human-readable:\n","- `seq2text()` converts numeric article sequences back into words.\n","- `seq2summary()` converts numeric summary sequences back into words, excluding special tokens (`sostok`, `eostok`).\n"]},{"cell_type":"code","execution_count":18,"id":"a32f357b","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:34:40.934885Z","iopub.status.busy":"2025-11-15T16:34:40.934687Z","iopub.status.idle":"2025-11-15T16:34:40.938295Z","shell.execute_reply":"2025-11-15T16:34:40.937646Z"},"papermill":{"duration":0.014044,"end_time":"2025-11-15T16:34:40.9394","exception":false,"start_time":"2025-11-15T16:34:40.925356","status":"completed"},"tags":[]},"outputs":[],"source":["# reverse_y_word_index: summary token → word\n","# reverse_x_word_index: article token → word\n","# y_word_index: summary word → token\n","\n","reverse_y_word_index = y_tokenizer.index_word\n","reverse_x_word_index = x_tokenizer.index_word\n","y_word_index = y_tokenizer.word_index"]},{"cell_type":"code","execution_count":19,"id":"163dfeb6","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:34:40.957321Z","iopub.status.busy":"2025-11-15T16:34:40.957113Z","iopub.status.idle":"2025-11-15T16:34:40.967417Z","shell.execute_reply":"2025-11-15T16:34:40.966743Z"},"papermill":{"duration":0.020311,"end_time":"2025-11-15T16:34:40.968373","exception":false,"start_time":"2025-11-15T16:34:40.948062","status":"completed"},"tags":[]},"outputs":[],"source":["# Inference Models\n","encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs,\n","                      state_h, state_c])\n","\n","# Below tensors will hold the states of the previous time step\n","decoder_state_input_h = Input(shape=(latent_dim, ))\n","decoder_state_input_c = Input(shape=(latent_dim, ))\n","decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim))\n","\n","# Get the embeddings of the decoder sequence\n","dec_emb2 = dec_emb_layer(decoder_inputs)\n","\n","# To predict the next word in the sequence, set the initial states to the states from the previous time step\n","(decoder_outputs2, state_h2, state_c2) = decoder_lstm(dec_emb2,\n","        initial_state=[decoder_state_input_h, decoder_state_input_c])\n","\n","# A dense softmax layer to generate prob dist. over the target vocabulary\n","decoder_outputs2 = decoder_dense(decoder_outputs2)\n","\n","# Final decoder model\n","#decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n","#                       decoder_state_input_h, decoder_state_input_c],\n","#                       [decoder_outputs2] + [state_h2, state_c2])\n","#TPU error\n","decoder_model = Model(\n","    [decoder_inputs, decoder_state_input_h, decoder_state_input_c],\n","    [decoder_outputs2, state_h2, state_c2]\n",")\n"]},{"cell_type":"code","execution_count":20,"id":"cd6b99c9","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:34:40.986458Z","iopub.status.busy":"2025-11-15T16:34:40.986227Z","iopub.status.idle":"2025-11-15T16:34:40.992512Z","shell.execute_reply":"2025-11-15T16:34:40.991854Z"},"papermill":{"duration":0.016515,"end_time":"2025-11-15T16:34:40.99355","exception":false,"start_time":"2025-11-15T16:34:40.977035","status":"completed"},"tags":[]},"outputs":[],"source":["# Convert sequence to summary\n","def seq2summary(input_seq):\n","    newString = ''\n","    for i in input_seq:\n","        if i != 0 and i != y_word_index['sostok'] and i \\\n","            != y_word_index['eostok']:\n","            newString = newString + reverse_y_word_index[i] + ' '\n","\n","    return newString\n","\n","# Convert sequence to text\n","def seq2text(input_seq):\n","    newString = ''\n","    for i in input_seq:\n","        if i != 0:\n","            newString = newString + reverse_x_word_index[i] + ' '\n","\n","    return newString\n","\n","def decode_sequence(input_seq):\n","\n","    # Encode the input as state vectors.\n","    (e_out, e_h, e_c) = encoder_model.predict(input_seq, verbose=0)\n","\n","    # Generate empty target sequence of length 1\n","    y_seq = np.zeros((1, 1))\n","\n","    # Populate the first word of target sequence with the start word.\n","    y_seq[0, 0] = y_word_index['sostok']\n","\n","    stop_condition = False\n","    decoded_sentence = ''\n","\n","    while not stop_condition:\n","        (output_tokens, h, c) = decoder_model.predict([y_seq]\n","                + [e_out, e_h, e_c], verbose=0)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_token = reverse_y_word_index[sampled_token_index]\n","\n","        if sampled_token != 'eostok':\n","            decoded_sentence += ' ' + sampled_token\n","\n","        # Exit condition: either hit max length or find the stop word.\n","        if sampled_token == 'eostok' or len(decoded_sentence.split()) \\\n","            >= max_summary_len - 1:\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1)\n","        y_seq = np.zeros((1, 1))\n","        y_seq[0, 0] = sampled_token_index\n","\n","        # Update internal states\n","        (e_h, e_c) = (h, c)\n","\n","    return decoded_sentence"]},{"cell_type":"code","execution_count":21,"id":"9fec3fa8","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:34:41.011813Z","iopub.status.busy":"2025-11-15T16:34:41.011305Z","iopub.status.idle":"2025-11-15T16:34:41.014392Z","shell.execute_reply":"2025-11-15T16:34:41.013864Z"},"papermill":{"duration":0.013193,"end_time":"2025-11-15T16:34:41.015358","exception":false,"start_time":"2025-11-15T16:34:41.002165","status":"completed"},"tags":[]},"outputs":[],"source":["# for i in range(0, 5):\n","#     print ('Review:', seq2text(x_train[i]))\n","#     print ('Original summary:', seq2summary(y_train[i]))\n","#     print ('Predicted summary:', decode_sequence(x_train[i].reshape(1, max_text_len)))\n","#     print('\\n')"]},{"cell_type":"markdown","id":"f7feea40","metadata":{"papermill":{"duration":0.008527,"end_time":"2025-11-15T16:34:41.032444","exception":false,"start_time":"2025-11-15T16:34:41.023917","status":"completed"},"tags":[]},"source":["# Transformer Model with Self-Attention\n","\n","Similar to the Seq2Seq architecture, the Transformer follows an **encoder–decoder structure**, but instead of recurrent layers it relies entirely on **Multi-Head Self-Attention**.  \n","This allows the model to process all tokens **in parallel** and learn relationships between words regardless of their distance in the sequence.\n","\n","During training, the model takes **two inputs**:  \n","1. The encoder input (`text`) – the tokenized article.  \n","2. The decoder input (`summary`) – the summary shifted by one token.  \n","\n","The **target output** is the summary shifted forward by one position. The decoder learns to predict each word based on the previously generated ones and the encoded representation of the full text.\n","\n","---\n","\n","**Encoder**  \n","- The input article sequence (`max_text_len`) is first transformed using an **Embedding layer**.  \n","- A **Positional Encoding** is added to preserve the order of words (since attention has no notion of sequence order by itself).  \n","- The embedded input is processed by one or more **Multi-Head Self-Attention** blocks:\n","  - Each word attends to **all other words** in the input\n","  - Relationships between distant tokens are captured more effectively than in RNNs  \n","- A **Feed-Forward Network** (FFN) refines the contextual representations.\n","- **Residual connections** and **Layer Normalization** improve gradient flow and training stability.\n","\n","---\n","\n","**Decoder**  \n","- Similar positional embeddings are applied to the shifted summary tokens.  \n","- The decoder uses two attention mechanisms:\n","  1. **Masked Self-Attention**: ensures the model cannot “peek” at future words when predicting the next token.\n","  2. **Encoder-Decoder Attention**: allows the decoder to focus on relevant parts of the input article.\n","- A **Feed-Forward Network** further processes the attended features.\n","- A final **Dense layer with Softmax** outputs a probability distribution over all words in the vocabulary at each time step.\n","\n","---\n","\n","**sostok and eostok**  \n","In sequence-to-sequence tasks such as abstractive text summarization, **special tokens** are essential for controlling how a model generates text:\n","\n","- `<sostok>` → marks the **start** of the output sequence  \n","- `<eostok>` → marks the **end** of the sequence  \n","\n","However, these tokens **do not** play the same role during training across different architectures.\n","\n","Transformers use **masked self-attention** in the decoder, meaning that at time *t* the model can only attend to **previous tokens**.\n","Therefore:\n","\n","- `<sostok>` must be present **only in the decoder input**  \n","- `<sostok>` must be removed from the decoder target  \n","\n","Predicting a start token would make no sense and causes failure modes such as:\n","\n","- the model repeatedly outputting `<sostok>`\n","- inability to begin sequences with meaningful content\n","\n","The EOS token **must remain in the targets**, because:\n","\n","- it teaches the model **when to stop writing**\n","- without it, generation may become too long or infinite\n","\n","LSTM encoder-decoder models:\n","\n","- receive the final hidden state as initial context\n","- do **not** use masked attention\n","- often ignore the first timestep in loss computation\n","\n","So `<sostok>` in targets is less harmful there.\n","\n","---\n","\n","Thanks to the Self-Attention mechanism, Transformers **capture global context efficiently** and typically produce **more coherent and fluent summaries**, especially for longer texts."]},{"cell_type":"markdown","id":"0feb5284","metadata":{"papermill":{"duration":0.008453,"end_time":"2025-11-15T16:34:41.049466","exception":false,"start_time":"2025-11-15T16:34:41.041013","status":"completed"},"tags":[]},"source":["### Preparing Transformer Inputs\n","\n","To train the Transformer in an encoder–decoder setup, we need to properly structure the input data:\n","\n","- The **encoder input** is the full tokenized article (`x_train`)\n","- The **decoder input** is the summary sequence **shifted right**, starting with `<sostok>`\n","- The **decoder target** is the same summary **shifted left**, ending with `<eostok>`\n","\n","This shifting ensures that at each timestep the decoder learns to predict the **next** word using:\n","1. The previously processed summary tokens  \n","2. Attention over the encoder output  "]},{"cell_type":"markdown","id":"0831faf5","metadata":{"papermill":{"duration":0.008477,"end_time":"2025-11-15T16:34:41.067096","exception":false,"start_time":"2025-11-15T16:34:41.058619","status":"completed"},"tags":[]},"source":["In text summarization, token-level accuracy can be misleading because it only measures whether each predicted token matches the ground truth at the same position. It does not capture semantic meaning, fluency, word order, or relevance, and it can be inflated by common tokens like padding or start/end markers. A model can have high accuracy while producing poor summaries. Better evaluation uses metrics like ROUGE-1, ROUGE-2, and ROUGE-L, which measure overlap of unigrams, bigrams, and longest common subsequences between generated and reference summaries. During training, it is better to monitor validation loss and evaluate summaries qualitatively or with ROUGE rather than relying on token accuracy."]},{"cell_type":"markdown","id":"0bf9ca4e","metadata":{"papermill":{"duration":0.008469,"end_time":"2025-11-15T16:34:41.084162","exception":false,"start_time":"2025-11-15T16:34:41.075693","status":"completed"},"tags":[]},"source":["## Predict"]},{"cell_type":"markdown","id":"fa1e526e","metadata":{"papermill":{"duration":0.008416,"end_time":"2025-11-15T16:34:41.101184","exception":false,"start_time":"2025-11-15T16:34:41.092768","status":"completed"},"tags":[]},"source":["Note importanti:\n","\n","Look-ahead mask a inference non serve se generi un token alla volta (greedy decoding step-by-step).\n","\n","Padding mask dell’encoder serve al decoder per ignorare i pad token dell’input.\n","\n","Quando fai l’inference dovrai generare token uno per uno, aggiornando dec_input_inf ad ogni step."]},{"cell_type":"markdown","id":"e68f594c","metadata":{"papermill":{"duration":0.008435,"end_time":"2025-11-15T16:34:41.118254","exception":false,"start_time":"2025-11-15T16:34:41.109819","status":"completed"},"tags":[]},"source":["Generated summary: ripete continuamente parole (cannot cannot cannot, power power power…) → questo è un loop di ripetizione, tipico dei modelli seq2seq che non hanno abbastanza regolarizzazione sulla generazione.\n","\n","Generated beam search summary: testo quasi completamente fuori tema → indica che il modello non ha appreso bene il contenuto semantico e il beam search amplifica le frasi che appaiono più “probabili” a livello di token, ma non corrette.\n"]},{"cell_type":"markdown","id":"89a969ff","metadata":{"papermill":{"duration":0.008447,"end_time":"2025-11-15T16:34:41.135319","exception":false,"start_time":"2025-11-15T16:34:41.126872","status":"completed"},"tags":[]},"source":["Limiting the vocabulary in a Transformer is important because it reduces the size of the embedding matrices and the final softmax layer, making the model faster and lighter. It also helps prevent overfitting by removing extremely rare words that add noise rather than useful information. A smaller vocabulary uses less memory and often leads to more stable training, which can be important when working with limited hardware. However, reducing the vocabulary also means losing information, because words outside the limit are replaced with an unknown token. This can harm tasks like summarization, where specific terms, names, or technical words matter. A limited vocabulary also restricts what the model can generate, since it can only output words it knows.\n","\n","I tried limiting the vocabulary, but it ended up harming the model’s performance."]},{"cell_type":"markdown","id":"6c578485","metadata":{"papermill":{"duration":0.008668,"end_time":"2025-11-15T16:34:41.152563","exception":false,"start_time":"2025-11-15T16:34:41.143895","status":"completed"},"tags":[]},"source":["We wanted to continue training a Transformer model after the first epoch without losing the optimizer state. The issue was that creating a new optimizer reset the step count, making the learning rate extremely small due to the warmup schedule. To fix this, we restored a full checkpoint including both the model and optimizer. This ensures the weights, optimizer moments, and step count are preserved, so the learning rate continues correctly. Training can now continue from where it left off, and checkpoints can be saved after each epoch to resume seamlessly in future sessions."]},{"cell_type":"markdown","id":"6f9893e3","metadata":{"papermill":{"duration":0.008487,"end_time":"2025-11-15T16:34:41.169707","exception":false,"start_time":"2025-11-15T16:34:41.16122","status":"completed"},"tags":[]},"source":["When resuming training from a checkpoint, the Transformer seems to start from zero only because the first batches always show very low accuracy. This is normal and not a sign of lost weights. The decoder struggles with the first tokens due to masking, so accuracy is naturally low at the start of each epoch. If the dataset uses a fixed shuffle and the same examples appear first every time, you will always see the same low accuracy at the beginning. The proof that the checkpoint is restored correctly is that the accuracy rises quickly after a few hundred batches, which would not happen if the model had really restarted from scratch."]},{"cell_type":"code","execution_count":22,"id":"0d2d2db4","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:34:41.187827Z","iopub.status.busy":"2025-11-15T16:34:41.187622Z","iopub.status.idle":"2025-11-15T16:34:41.190979Z","shell.execute_reply":"2025-11-15T16:34:41.190476Z"},"papermill":{"duration":0.0137,"end_time":"2025-11-15T16:34:41.191943","exception":false,"start_time":"2025-11-15T16:34:41.178243","status":"completed"},"tags":[]},"outputs":[],"source":["df_trans.head(1)\n","df_tmp = df_trans"]},{"cell_type":"code","execution_count":23,"id":"b10d857d","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:34:41.210089Z","iopub.status.busy":"2025-11-15T16:34:41.209892Z","iopub.status.idle":"2025-11-15T16:34:46.033045Z","shell.execute_reply":"2025-11-15T16:34:46.032426Z"},"papermill":{"duration":4.833712,"end_time":"2025-11-15T16:34:46.034308","exception":false,"start_time":"2025-11-15T16:34:41.200596","status":"completed"},"tags":[]},"outputs":[],"source":["import tensorflow as tf\n","\n","df_trans = df_tmp\n","\n","# Data processing\n","df_trans['cleaned_text'] = df_trans['text'].apply(clean_text)\n","df_trans['cleaned_summary'] = df_trans['summary'].apply(clean_text)\n","\n","df_trans = df_trans.dropna(subset=['cleaned_text', 'cleaned_summary'])\n","df_trans = df_trans.drop_duplicates(subset=['cleaned_text', 'cleaned_summary'])\n","\n","# df_trans['cleaned_summary'] = df_trans['cleaned_summary'].apply(lambda x: '<SOS> ' + x + ' <EOS>')\n","\n","# mask = (df_trans['cleaned_text'].str.split().str.len() <= max_text_len) & \\\n","#        (df_trans['cleaned_summary'].str.split().str.len() <= max_summary_len)\n","# df_trans = df_trans.loc[mask].reset_index(drop=True)\n","\n","# # Tokenizer\n","# oov_token = '<unk>'\n","# filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n","\n","\n","# x_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n","# x_tokenizer.fit_on_texts(df_trans['cleaned_text'])\n","\n","# y_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)\n","# y_tokenizer.fit_on_texts(df_trans['cleaned_summary'])\n","\n","# inputs = x_tokenizer.texts_to_sequences(df_trans['cleaned_text'])\n","# targets = y_tokenizer.texts_to_sequences(df_trans['cleaned_summary'])\n","\n","# # Padding\n","# inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=max_text_len, padding='post', truncating='post')\n","# targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=max_summary_len, padding='post', truncating='post')\n","\n","# inputs = tf.cast(inputs, dtype=tf.int64)\n","# targets = tf.cast(targets, dtype=tf.int64)\n","\n","# # Vocab. size (+1 for padding token)\n","# x_voc_size = len(x_tokenizer.word_index) + 1\n","# y_voc_size = len(y_tokenizer.word_index) + 1\n","\n","# print(f\"Size of vocabulary in X = {x_voc_size}\")\n","# print(f\"Size of vocabulary in Y = {y_voc_size}\")"]},{"cell_type":"code","execution_count":24,"id":"985c9789","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:34:46.053514Z","iopub.status.busy":"2025-11-15T16:34:46.053227Z","iopub.status.idle":"2025-11-15T16:35:29.496784Z","shell.execute_reply":"2025-11-15T16:35:29.496001Z"},"papermill":{"duration":43.462826,"end_time":"2025-11-15T16:35:29.506564","exception":false,"start_time":"2025-11-15T16:34:46.043738","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["text_sample.txt loaded\n","summary_sample.txt loaded\n","sp_text.model loaded\n","sp_summary.model loaded\n","Size of vocabulary in X = 20000\n","Size of vocabulary in Y = 20000\n"]}],"source":["import sentencepiece as spm\n","\n","text_file = \"text_sample.txt\"\n","summary_file = \"summary_sample.txt\"\n","text_model_file = \"sp_text.model\"\n","summary_model_file = \"sp_summary.model\"\n","\n","\n","# Text\n","if os.path.exists(f\"/kaggle/input/sentencepiece/{text_file}\"):\n","    print(f\"{text_file} loaded\")\n","    text_path = f\"/kaggle/input/sentencepiece/{text_file}\"\n","else:\n","    text_path = f\"/kaggle/working/{text_file}\"\n","    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n","        for t in df_trans['cleaned_text']:\n","            f.write(t + \"\\n\")\n","\n","# Summary\n","if os.path.exists(f\"/kaggle/input/sentencepiece/{summary_file}\"):\n","    print(f\"{summary_file} loaded\")\n","    summary_path = f\"/kaggle/input/sentencepiece/{summary_file}\"\n","else:\n","    summary_path = f\"/kaggle/working/{summary_file}\"\n","    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n","        for t in df_trans['cleaned_summary']:\n","            f.write(t + \"\\n\")\n","\n","# Text model\n","if os.path.exists(f\"/kaggle/input/sentencepiece/{text_model_file}\"):\n","    print(f\"{text_model_file} loaded\")\n","    text_model_path = f\"/kaggle/input/sentencepiece/{text_model_file}\"\n","else:\n","    text_model_path = f\"/kaggle/working/{text_model_file}\"\n","    spm.SentencePieceTrainer.Train(\n","        f'--input={text_path} '\n","        f'--model_prefix=/kaggle/working/sp_text '\n","        '--vocab_size=20000 --model_type=bpe '\n","        '--user_defined_symbols=<SOS>,<EOS> --num_threads=8'\n","    )\n","\n","# Summary model\n","if os.path.exists(f\"/kaggle/input/sentencepiece/{summary_model_file}\"):\n","    print(f\"{summary_model_file} loaded\")\n","    summary_model_path = f\"/kaggle/input/sentencepiece/{summary_model_file}\"\n","else:\n","    summary_model_path = f\"/kaggle/working/{summary_model_file}\"\n","    spm.SentencePieceTrainer.Train(\n","        f'--input={summary_path} '\n","        f'--model_prefix=/kaggle/working/sp_summary '\n","        '--vocab_size=20000 --model_type=bpe '\n","        '--user_defined_symbols=<SOS>,<EOS> --num_threads=8'\n","    )\n","\n","sp_text = spm.SentencePieceProcessor()\n","sp_text.load(text_model_path)\n","\n","sp_summary = spm.SentencePieceProcessor()\n","sp_summary.load(summary_model_path)\n","\n","inputs = [sp_text.encode_as_ids(t) for t in df_trans['cleaned_text']]\n","targets = [sp_summary.encode_as_ids(t) for t in df_trans['cleaned_summary']]\n","\n","inputs = pad_sequences(inputs, maxlen=max_text_len, padding='post', truncating='post')\n","targets = pad_sequences(targets, maxlen=max_summary_len, padding='post', truncating='post')\n","\n","inputs = tf.cast(inputs, dtype=tf.int64)\n","targets = tf.cast(targets, dtype=tf.int64)\n","\n","x_voc_size = sp_text.get_piece_size()\n","y_voc_size = sp_summary.get_piece_size()\n","\n","print(f\"Size of vocabulary in X = {x_voc_size}\")\n","print(f\"Size of vocabulary in Y = {y_voc_size}\")"]},{"cell_type":"code","execution_count":25,"id":"f14590bf","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:35:29.5253Z","iopub.status.busy":"2025-11-15T16:35:29.525069Z","iopub.status.idle":"2025-11-15T16:35:30.666548Z","shell.execute_reply":"2025-11-15T16:35:30.665721Z"},"papermill":{"duration":1.152563,"end_time":"2025-11-15T16:35:30.667962","exception":false,"start_time":"2025-11-15T16:35:29.515399","status":"completed"},"tags":[]},"outputs":[],"source":["#dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(10000).batch(32)\n","dataset = (tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(10000, seed=42, reshuffle_each_iteration=False).batch(32))\n","\n","\n","def get_angles(position, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n","    return position * angle_rates\n","\n","def positional_encoding(position, d_model):\n","    angle_rads = get_angles(\n","        np.arange(position)[:, np.newaxis],\n","        np.arange(d_model)[np.newaxis, :],\n","        d_model\n","    )\n","\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = angle_rads[np.newaxis, ...]\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","def create_padding_mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","    return seq[:, tf.newaxis, tf.newaxis, :]\n","\n","def create_look_ahead_mask(size):\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask\n","\n","def scaled_dot_product_attention(q, k, v, mask):\n","    matmul_qk = tf.matmul(q, k, transpose_b=True)\n","\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    #dk = tf.cast(tf.shape(k)[-1], q.dtype)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)  \n","\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n","\n","    output = tf.matmul(attention_weights, v)\n","    return output, attention_weights\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.wq = tf.keras.layers.Dense(d_model)\n","        self.wk = tf.keras.layers.Dense(d_model)\n","        self.wv = tf.keras.layers.Dense(d_model)\n","\n","        self.dense = tf.keras.layers.Dense(d_model)\n","        \n","    def split_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","    \n","    def call(self, v, k, q, mask):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q)\n","        k = self.wk(k)\n","        v = self.wv(v)\n","\n","        q = self.split_heads(q, batch_size)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","\n","        scaled_attention, attention_weights = scaled_dot_product_attention(\n","            q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","        \n","        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n","        output = self.dense(concat_attention)\n","            \n","        return output, attention_weights\n","    \n","def point_wise_feed_forward_network(d_model, dff):\n","    return tf.keras.Sequential([\n","        tf.keras.layers.Dense(dff, activation='relu'),\n","        tf.keras.layers.Dense(d_model)\n","    ])\n","\n","\n","class EncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(EncoderLayer, self).__init__()\n","\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","    \n","    def call(self, x, training=False, mask=None):\n","        attn_output, _ = self.mha(x, x, x, mask)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output)\n","    \n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        out2 = self.layernorm2(out1 + ffn_output)\n","    \n","        return out2\n","\n","\n","\n","class DecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(DecoderLayer, self).__init__()\n","\n","        self.mha1 = MultiHeadAttention(d_model, num_heads)\n","        self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","        self.dropout3 = tf.keras.layers.Dropout(rate)\n","    \n","    \n","    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n","        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n","        attn1 = self.dropout1(attn1, training=training)\n","        out1 = self.layernorm1(attn1 + x)\n","    \n","        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n","        attn2 = self.dropout2(attn2, training=training)\n","        out2 = self.layernorm2(attn2 + out1)\n","    \n","        ffn_output = self.ffn(out2)\n","        ffn_output = self.dropout3(ffn_output, training=training)\n","        out3 = self.layernorm3(ffn_output + out2)\n","    \n","        return out3, attn_weights_block1, attn_weights_block2\n","\n","\n","\n","class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n","        super(Encoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n","\n","        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","        \n","    def call(self, x, training=False, mask=None):\n","        seq_len = tf.shape(x)[1]\n","    \n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","        #x *= tf.math.sqrt(tf.cast(self.d_model, x.dtype)) \n","        #x += tf.cast(self.pos_encoding[:, :seq_len, :], x.dtype)\n","\n","        x = self.dropout(x, training=training)\n","    \n","        for i in range(self.num_layers):\n","            x = self.enc_layers[i](x, training=training, mask=mask)\n","    \n","        return x\n","    \n","class Decoder(tf.keras.layers.Layer):\n","        \n","    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n","        super(Decoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","\n","        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","    \n","    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n","        seq_len = tf.shape(x)[1]\n","        attention_weights = {}\n","    \n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","        #x *= tf.math.sqrt(tf.cast(self.d_model, x.dtype))              \n","        #x += tf.cast(self.pos_encoding[:, :seq_len, :], x.dtype)      \n","\n","    \n","        x = self.dropout(x, training=training)\n","    \n","        for i in range(self.num_layers):\n","            x, block1, block2 = self.dec_layers[i](\n","                x, \n","                enc_output, \n","                training=training, \n","                look_ahead_mask=look_ahead_mask, \n","                padding_mask=padding_mask\n","            )\n","            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n","            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n","    \n","        return x, attention_weights\n","\n","\n","\n","\n","class Transformer(tf.keras.Model):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n","        super(Transformer, self).__init__()\n","\n","        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n","        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n","        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","    \n","    def call(self, inp, tar, training=False, enc_padding_mask=None, look_ahead_mask=None, dec_padding_mask=None):\n","        enc_output = self.encoder(x=inp, training=training, mask=enc_padding_mask)\n","        dec_output, attention_weights = self.decoder(x=tar, enc_output=enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)\n","        final_output = self.final_layer(dec_output)\n","        return final_output, attention_weights"]},{"cell_type":"code","execution_count":26,"id":"f935d19a","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:35:30.687412Z","iopub.status.busy":"2025-11-15T16:35:30.686985Z","iopub.status.idle":"2025-11-15T16:35:30.694634Z","shell.execute_reply":"2025-11-15T16:35:30.693904Z"},"papermill":{"duration":0.018416,"end_time":"2025-11-15T16:35:30.695769","exception":false,"start_time":"2025-11-15T16:35:30.677353","status":"completed"},"tags":[]},"outputs":[],"source":["import time\n","\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n","\n","\n","def accuracy_function(real, pred):\n","    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n","    #accuracies = tf.cast(accuracies, dtype= tf.float32)\n","\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    accuracies = tf.math.logical_and(mask, accuracies)\n","\n","    accuracies = tf.cast(accuracies, dtype=tf.float32)\n","    mask = tf.cast(mask, dtype=tf.float32)\n","    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n","\n","def create_masks(inp, tar):\n","    enc_padding_mask = create_padding_mask(inp)\n","    dec_padding_mask = create_padding_mask(inp)\n","\n","    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","    dec_target_padding_mask = create_padding_mask(tar)\n","    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","  \n","    return enc_padding_mask, combined_mask, dec_padding_mask\n","\n","\n","class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=2000):\n","        super(CustomSchedule, self).__init__()\n","\n","        self.d_model = d_model\n","        self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","        self.warmup_steps = warmup_steps\n","    \n","    def __call__(self, step):\n","        step = tf.cast(step, tf.float32) \n","        step = tf.maximum(step, 1.0)\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps ** -1.5)\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"]},{"cell_type":"code","execution_count":27,"id":"c29f9591","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:35:30.713984Z","iopub.status.busy":"2025-11-15T16:35:30.713763Z","iopub.status.idle":"2025-11-15T16:35:36.444836Z","shell.execute_reply":"2025-11-15T16:35:36.444138Z"},"papermill":{"duration":5.741408,"end_time":"2025-11-15T16:35:36.445895","exception":false,"start_time":"2025-11-15T16:35:30.704487","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"transformer\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)               │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,279,040</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)               │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">9,333,760</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,140,000</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ encoder (\u001b[38;5;33mEncoder\u001b[0m)               │ ?                      │     \u001b[38;5;34m8,279,040\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ decoder (\u001b[38;5;33mDecoder\u001b[0m)               │ ?                      │     \u001b[38;5;34m9,333,760\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_65 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20000\u001b[0m)         │     \u001b[38;5;34m5,140,000\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,752,800</span> (86.80 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m22,752,800\u001b[0m (86.80 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,752,800</span> (86.80 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m22,752,800\u001b[0m (86.80 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["num_layers = 4\n","d_model = 256\n","dff = 1024\n","num_heads = 8\n","dropout_rate = 0.2\n","\n","transformer = Transformer(\n","    num_layers=num_layers,\n","    d_model=d_model,\n","    num_heads=num_heads,\n","    dff=dff,\n","    input_vocab_size=x_voc_size,\n","    target_vocab_size=y_voc_size,\n","    pe_input=1000,\n","    pe_target=1000,\n","    rate=dropout_rate\n",")\n","\n","dummy_input = tf.constant([[1]*max_text_len], dtype=tf.int64)\n","dummy_target = tf.constant([[1]*max_summary_len], dtype=tf.int64)\n","_ = transformer(dummy_input, dummy_target, training=False)\n","transformer.summary()"]},{"cell_type":"code","execution_count":28,"id":"a2bad08c","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:35:36.465641Z","iopub.status.busy":"2025-11-15T16:35:36.465441Z","iopub.status.idle":"2025-11-15T16:35:38.272628Z","shell.execute_reply":"2025-11-15T16:35:38.2719Z"},"papermill":{"duration":1.818186,"end_time":"2025-11-15T16:35:38.273786","exception":false,"start_time":"2025-11-15T16:35:36.4556","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Checkpoint restored from /kaggle/input/checkpoint16/ckpt-16\n","Learning rate: 0.000222443879\n","Optimizer step: 78944\n"]}],"source":["first_epoch = 0\n","\n","\n","#optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","learning_rate = CustomSchedule(d_model)\n","optimizer = tf.keras.optimizers.Adam(\n","    learning_rate=learning_rate,\n","    beta_1=0.9,\n","    beta_2=0.98,\n","    epsilon=1e-9\n",")\n","\n","\n","\n","checkpoint_path = \"/kaggle/working/checkpoints\"\n","ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n","#ckpt = tf.train.Checkpoint(transformer=transformer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n","\n","\n","curr_ckpt = 16\n","checkpoint_path_input = f\"/kaggle/input/checkpoint{curr_ckpt}/ckpt-{curr_ckpt}\"\n","if os.path.exists(checkpoint_path_input + \".index\"):\n","    #ckpt.restore(checkpoint_path_input).assert_existing_objects_matched()\n","    status = ckpt.restore(checkpoint_path_input)\n","    #status.expect_partial()\n","    status.assert_existing_objects_matched()\n","    print(f\"Checkpoint restored from {checkpoint_path_input}\")\n","    first_epoch = curr_ckpt\n","\n","tf.print(\"Learning rate:\", optimizer.learning_rate)\n","print(\"Optimizer step:\", optimizer.iterations.numpy())\n","\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n","\n","\n","@tf.function\n","def train_step(inp, tar):\n","    tar_inp = tar[:, :-1]\n","    tar_real = tar[:, 1:]\n","\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","\n","    with tf.GradientTape() as tape:\n","        predictions, _ = transformer(\n","            inp, \n","            tar_inp, \n","            training=True, \n","            enc_padding_mask=enc_padding_mask, \n","            look_ahead_mask=combined_mask, \n","            dec_padding_mask=dec_padding_mask\n","        )\n","\n","\n","        loss = loss_function(tar_real, predictions)\n","\n","    gradients = tape.gradient(loss, transformer.trainable_variables)    \n","    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","\n","    train_loss.update_state(loss)\n","    train_accuracy.update_state(accuracy_function(tar_real, predictions))"]},{"cell_type":"code","execution_count":29,"id":"28982af8","metadata":{"execution":{"iopub.execute_input":"2025-11-15T16:35:38.293992Z","iopub.status.busy":"2025-11-15T16:35:38.293774Z","iopub.status.idle":"2025-11-15T19:11:38.717611Z","shell.execute_reply":"2025-11-15T19:11:38.716691Z"},"papermill":{"duration":9360.43531,"end_time":"2025-11-15T19:11:38.718883","exception":false,"start_time":"2025-11-15T16:35:38.283573","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 17 Batch 100/4934 Accuracy 0.0265 Loss 8.0327 Time elapsed 42.47s\n","Epoch 17 Batch 200/4934 Accuracy 0.0381 Loss 7.8316 Time elapsed 51.02s\n","Epoch 17 Batch 300/4934 Accuracy 0.0589 Loss 7.6179 Time elapsed 59.57s\n","Epoch 17 Batch 400/4934 Accuracy 0.0749 Loss 7.4395 Time elapsed 68.12s\n","Epoch 17 Batch 500/4934 Accuracy 0.0877 Loss 7.2886 Time elapsed 76.67s\n","Epoch 17 Batch 600/4934 Accuracy 0.0976 Loss 7.1648 Time elapsed 85.21s\n","Epoch 17 Batch 700/4934 Accuracy 0.1060 Loss 7.0528 Time elapsed 93.76s\n","Epoch 17 Batch 800/4934 Accuracy 0.1129 Loss 6.9611 Time elapsed 102.31s\n","Epoch 17 Batch 900/4934 Accuracy 0.1192 Loss 6.8734 Time elapsed 110.86s\n","Epoch 17 Batch 1000/4934 Accuracy 0.1247 Loss 6.7958 Time elapsed 119.41s\n","Epoch 17 Batch 1100/4934 Accuracy 0.1295 Loss 6.7306 Time elapsed 127.96s\n","Epoch 17 Batch 1200/4934 Accuracy 0.1337 Loss 6.6716 Time elapsed 136.51s\n","Epoch 17 Batch 1300/4934 Accuracy 0.1374 Loss 6.6170 Time elapsed 145.06s\n","Epoch 17 Batch 1400/4934 Accuracy 0.1413 Loss 6.5649 Time elapsed 153.61s\n","Epoch 17 Batch 1500/4934 Accuracy 0.1446 Loss 6.5192 Time elapsed 162.16s\n","Epoch 17 Batch 1600/4934 Accuracy 0.1474 Loss 6.4785 Time elapsed 170.71s\n","Epoch 17 Batch 1700/4934 Accuracy 0.1498 Loss 6.4428 Time elapsed 179.26s\n","Epoch 17 Batch 1800/4934 Accuracy 0.1522 Loss 6.4093 Time elapsed 187.81s\n","Epoch 17 Batch 1900/4934 Accuracy 0.1545 Loss 6.3783 Time elapsed 196.36s\n","Epoch 17 Batch 2000/4934 Accuracy 0.1566 Loss 6.3490 Time elapsed 204.91s\n","Epoch 17 Batch 2100/4934 Accuracy 0.1587 Loss 6.3205 Time elapsed 213.46s\n","Epoch 17 Batch 2200/4934 Accuracy 0.1608 Loss 6.2922 Time elapsed 222.01s\n","Epoch 17 Batch 2300/4934 Accuracy 0.1627 Loss 6.2679 Time elapsed 230.56s\n","Epoch 17 Batch 2400/4934 Accuracy 0.1644 Loss 6.2432 Time elapsed 239.11s\n","Epoch 17 Batch 2500/4934 Accuracy 0.1662 Loss 6.2203 Time elapsed 247.66s\n","Epoch 17 Batch 2600/4934 Accuracy 0.1676 Loss 6.1997 Time elapsed 256.20s\n","Epoch 17 Batch 2700/4934 Accuracy 0.1692 Loss 6.1792 Time elapsed 264.75s\n","Epoch 17 Batch 2800/4934 Accuracy 0.1707 Loss 6.1590 Time elapsed 273.30s\n","Epoch 17 Batch 2900/4934 Accuracy 0.1720 Loss 6.1395 Time elapsed 281.84s\n","Epoch 17 Batch 3000/4934 Accuracy 0.1730 Loss 6.1214 Time elapsed 290.38s\n","Epoch 17 Batch 3100/4934 Accuracy 0.1737 Loss 6.1053 Time elapsed 298.92s\n","Epoch 17 Batch 3200/4934 Accuracy 0.1741 Loss 6.0925 Time elapsed 307.47s\n","Epoch 17 Batch 3300/4934 Accuracy 0.1747 Loss 6.0793 Time elapsed 316.01s\n","Epoch 17 Batch 3400/4934 Accuracy 0.1752 Loss 6.0674 Time elapsed 324.56s\n","Epoch 17 Batch 3500/4934 Accuracy 0.1758 Loss 6.0549 Time elapsed 333.11s\n","Epoch 17 Batch 3600/4934 Accuracy 0.1763 Loss 6.0448 Time elapsed 341.65s\n","Epoch 17 Batch 3700/4934 Accuracy 0.1768 Loss 6.0351 Time elapsed 350.20s\n","Epoch 17 Batch 3800/4934 Accuracy 0.1772 Loss 6.0253 Time elapsed 358.75s\n","Epoch 17 Batch 3900/4934 Accuracy 0.1775 Loss 6.0172 Time elapsed 367.30s\n","Epoch 17 Batch 4000/4934 Accuracy 0.1780 Loss 6.0088 Time elapsed 375.85s\n","Epoch 17 Batch 4100/4934 Accuracy 0.1784 Loss 6.0010 Time elapsed 384.39s\n","Epoch 17 Batch 4200/4934 Accuracy 0.1788 Loss 5.9932 Time elapsed 392.94s\n","Epoch 17 Batch 4300/4934 Accuracy 0.1793 Loss 5.9843 Time elapsed 401.49s\n","Epoch 17 Batch 4400/4934 Accuracy 0.1799 Loss 5.9751 Time elapsed 410.04s\n","Epoch 17 Batch 4500/4934 Accuracy 0.1804 Loss 5.9667 Time elapsed 418.59s\n","Epoch 17 Batch 4600/4934 Accuracy 0.1809 Loss 5.9583 Time elapsed 427.14s\n","Epoch 17 Batch 4700/4934 Accuracy 0.1814 Loss 5.9496 Time elapsed 435.69s\n","Epoch 17 Batch 4800/4934 Accuracy 0.1821 Loss 5.9402 Time elapsed 444.24s\n","Epoch 17 Batch 4900/4934 Accuracy 0.1827 Loss 5.9318 Time elapsed 452.79s\n","Epoch 17 Batch 4934/4934 Accuracy 0.1830 Loss 5.9290 Time elapsed 468.93s\n","Epoch 17 Accuracy 0.1830 Loss 5.9290 Time 470.05 seconds\n","\n","Epoch 18 Batch 100/4934 Accuracy 0.2173 Loss 5.4277 Time elapsed 8.55s\n","Epoch 18 Batch 200/4934 Accuracy 0.2197 Loss 5.3919 Time elapsed 17.09s\n","Epoch 18 Batch 300/4934 Accuracy 0.2241 Loss 5.3461 Time elapsed 25.64s\n","Epoch 18 Batch 400/4934 Accuracy 0.2267 Loss 5.3100 Time elapsed 34.18s\n","Epoch 18 Batch 500/4934 Accuracy 0.2285 Loss 5.2795 Time elapsed 42.74s\n","Epoch 18 Batch 600/4934 Accuracy 0.2294 Loss 5.2599 Time elapsed 51.28s\n","Epoch 18 Batch 700/4934 Accuracy 0.2313 Loss 5.2348 Time elapsed 59.83s\n","Epoch 18 Batch 800/4934 Accuracy 0.2323 Loss 5.2194 Time elapsed 68.37s\n","Epoch 18 Batch 900/4934 Accuracy 0.2341 Loss 5.1976 Time elapsed 76.91s\n","Epoch 18 Batch 1000/4934 Accuracy 0.2353 Loss 5.1781 Time elapsed 85.46s\n","Epoch 18 Batch 1100/4934 Accuracy 0.2363 Loss 5.1644 Time elapsed 94.01s\n","Epoch 18 Batch 1200/4934 Accuracy 0.2372 Loss 5.1514 Time elapsed 102.55s\n","Epoch 18 Batch 1300/4934 Accuracy 0.2381 Loss 5.1391 Time elapsed 111.10s\n","Epoch 18 Batch 1400/4934 Accuracy 0.2393 Loss 5.1256 Time elapsed 119.64s\n","Epoch 18 Batch 1500/4934 Accuracy 0.2401 Loss 5.1154 Time elapsed 128.19s\n","Epoch 18 Batch 1600/4934 Accuracy 0.2408 Loss 5.1068 Time elapsed 136.73s\n","Epoch 18 Batch 1700/4934 Accuracy 0.2413 Loss 5.1004 Time elapsed 145.28s\n","Epoch 18 Batch 1800/4934 Accuracy 0.2419 Loss 5.0942 Time elapsed 153.82s\n","Epoch 18 Batch 1900/4934 Accuracy 0.2426 Loss 5.0889 Time elapsed 162.37s\n","Epoch 18 Batch 2000/4934 Accuracy 0.2430 Loss 5.0839 Time elapsed 170.91s\n","Epoch 18 Batch 2100/4934 Accuracy 0.2435 Loss 5.0783 Time elapsed 179.46s\n","Epoch 18 Batch 2200/4934 Accuracy 0.2443 Loss 5.0718 Time elapsed 188.01s\n","Epoch 18 Batch 2300/4934 Accuracy 0.2447 Loss 5.0680 Time elapsed 196.55s\n","Epoch 18 Batch 2400/4934 Accuracy 0.2451 Loss 5.0630 Time elapsed 205.10s\n","Epoch 18 Batch 2500/4934 Accuracy 0.2454 Loss 5.0588 Time elapsed 213.65s\n","Epoch 18 Batch 2600/4934 Accuracy 0.2457 Loss 5.0561 Time elapsed 222.19s\n","Epoch 18 Batch 2700/4934 Accuracy 0.2460 Loss 5.0527 Time elapsed 230.74s\n","Epoch 18 Batch 2800/4934 Accuracy 0.2463 Loss 5.0484 Time elapsed 239.28s\n","Epoch 18 Batch 2900/4934 Accuracy 0.2467 Loss 5.0429 Time elapsed 247.82s\n","Epoch 18 Batch 3000/4934 Accuracy 0.2469 Loss 5.0371 Time elapsed 256.36s\n","Epoch 18 Batch 3100/4934 Accuracy 0.2468 Loss 5.0330 Time elapsed 264.91s\n","Epoch 18 Batch 3200/4934 Accuracy 0.2466 Loss 5.0321 Time elapsed 273.45s\n","Epoch 18 Batch 3300/4934 Accuracy 0.2465 Loss 5.0305 Time elapsed 282.00s\n","Epoch 18 Batch 3400/4934 Accuracy 0.2461 Loss 5.0303 Time elapsed 290.54s\n","Epoch 18 Batch 3500/4934 Accuracy 0.2460 Loss 5.0295 Time elapsed 299.09s\n","Epoch 18 Batch 3600/4934 Accuracy 0.2457 Loss 5.0306 Time elapsed 307.64s\n","Epoch 18 Batch 3700/4934 Accuracy 0.2454 Loss 5.0320 Time elapsed 316.18s\n","Epoch 18 Batch 3800/4934 Accuracy 0.2451 Loss 5.0330 Time elapsed 324.73s\n","Epoch 18 Batch 3900/4934 Accuracy 0.2448 Loss 5.0350 Time elapsed 333.27s\n","Epoch 18 Batch 4000/4934 Accuracy 0.2445 Loss 5.0368 Time elapsed 341.82s\n","Epoch 18 Batch 4100/4934 Accuracy 0.2443 Loss 5.0386 Time elapsed 350.37s\n","Epoch 18 Batch 4200/4934 Accuracy 0.2441 Loss 5.0401 Time elapsed 358.91s\n","Epoch 18 Batch 4300/4934 Accuracy 0.2440 Loss 5.0403 Time elapsed 367.46s\n","Epoch 18 Batch 4400/4934 Accuracy 0.2440 Loss 5.0402 Time elapsed 376.01s\n","Epoch 18 Batch 4500/4934 Accuracy 0.2441 Loss 5.0401 Time elapsed 384.55s\n","Epoch 18 Batch 4600/4934 Accuracy 0.2441 Loss 5.0401 Time elapsed 393.10s\n","Epoch 18 Batch 4700/4934 Accuracy 0.2441 Loss 5.0396 Time elapsed 401.65s\n","Epoch 18 Batch 4800/4934 Accuracy 0.2443 Loss 5.0385 Time elapsed 410.20s\n","Epoch 18 Batch 4900/4934 Accuracy 0.2444 Loss 5.0382 Time elapsed 418.75s\n","Epoch 18 Batch 4934/4934 Accuracy 0.2444 Loss 5.0383 Time elapsed 421.66s\n","Epoch 18 Accuracy 0.2444 Loss 5.0383 Time 422.68 seconds\n","\n","Epoch 19 Batch 100/4934 Accuracy 0.2479 Loss 4.9802 Time elapsed 8.55s\n","Epoch 19 Batch 200/4934 Accuracy 0.2501 Loss 4.9511 Time elapsed 17.10s\n","Epoch 19 Batch 300/4934 Accuracy 0.2551 Loss 4.9098 Time elapsed 25.64s\n","Epoch 19 Batch 400/4934 Accuracy 0.2574 Loss 4.8788 Time elapsed 34.19s\n","Epoch 19 Batch 500/4934 Accuracy 0.2599 Loss 4.8535 Time elapsed 42.73s\n","Epoch 19 Batch 600/4934 Accuracy 0.2605 Loss 4.8389 Time elapsed 51.28s\n","Epoch 19 Batch 700/4934 Accuracy 0.2621 Loss 4.8172 Time elapsed 59.82s\n","Epoch 19 Batch 800/4934 Accuracy 0.2626 Loss 4.8053 Time elapsed 68.37s\n","Epoch 19 Batch 900/4934 Accuracy 0.2644 Loss 4.7863 Time elapsed 76.92s\n","Epoch 19 Batch 1000/4934 Accuracy 0.2656 Loss 4.7715 Time elapsed 85.46s\n","Epoch 19 Batch 1100/4934 Accuracy 0.2665 Loss 4.7610 Time elapsed 94.01s\n","Epoch 19 Batch 1200/4934 Accuracy 0.2674 Loss 4.7517 Time elapsed 102.55s\n","Epoch 19 Batch 1300/4934 Accuracy 0.2681 Loss 4.7431 Time elapsed 111.10s\n","Epoch 19 Batch 1400/4934 Accuracy 0.2691 Loss 4.7327 Time elapsed 119.65s\n","Epoch 19 Batch 1500/4934 Accuracy 0.2697 Loss 4.7253 Time elapsed 128.20s\n","Epoch 19 Batch 1600/4934 Accuracy 0.2702 Loss 4.7183 Time elapsed 136.74s\n","Epoch 19 Batch 1700/4934 Accuracy 0.2706 Loss 4.7141 Time elapsed 145.29s\n","Epoch 19 Batch 1800/4934 Accuracy 0.2712 Loss 4.7103 Time elapsed 153.84s\n","Epoch 19 Batch 1900/4934 Accuracy 0.2716 Loss 4.7072 Time elapsed 162.39s\n","Epoch 19 Batch 2000/4934 Accuracy 0.2720 Loss 4.7043 Time elapsed 170.93s\n","Epoch 19 Batch 2100/4934 Accuracy 0.2723 Loss 4.7016 Time elapsed 179.48s\n","Epoch 19 Batch 2200/4934 Accuracy 0.2728 Loss 4.6972 Time elapsed 188.03s\n","Epoch 19 Batch 2300/4934 Accuracy 0.2731 Loss 4.6954 Time elapsed 196.57s\n","Epoch 19 Batch 2400/4934 Accuracy 0.2734 Loss 4.6925 Time elapsed 205.12s\n","Epoch 19 Batch 2500/4934 Accuracy 0.2736 Loss 4.6903 Time elapsed 213.67s\n","Epoch 19 Batch 2600/4934 Accuracy 0.2736 Loss 4.6896 Time elapsed 222.22s\n","Epoch 19 Batch 2700/4934 Accuracy 0.2738 Loss 4.6885 Time elapsed 230.76s\n","Epoch 19 Batch 2800/4934 Accuracy 0.2740 Loss 4.6858 Time elapsed 239.31s\n","Epoch 19 Batch 2900/4934 Accuracy 0.2743 Loss 4.6811 Time elapsed 247.85s\n","Epoch 19 Batch 3000/4934 Accuracy 0.2745 Loss 4.6758 Time elapsed 256.39s\n","Epoch 19 Batch 3100/4934 Accuracy 0.2745 Loss 4.6720 Time elapsed 264.94s\n","Epoch 19 Batch 3200/4934 Accuracy 0.2743 Loss 4.6719 Time elapsed 273.48s\n","Epoch 19 Batch 3300/4934 Accuracy 0.2741 Loss 4.6714 Time elapsed 282.03s\n","Epoch 19 Batch 3400/4934 Accuracy 0.2737 Loss 4.6725 Time elapsed 290.58s\n","Epoch 19 Batch 3500/4934 Accuracy 0.2736 Loss 4.6727 Time elapsed 299.12s\n","Epoch 19 Batch 3600/4934 Accuracy 0.2732 Loss 4.6755 Time elapsed 307.67s\n","Epoch 19 Batch 3700/4934 Accuracy 0.2727 Loss 4.6788 Time elapsed 316.21s\n","Epoch 19 Batch 3800/4934 Accuracy 0.2724 Loss 4.6816 Time elapsed 324.75s\n","Epoch 19 Batch 3900/4934 Accuracy 0.2720 Loss 4.6850 Time elapsed 333.30s\n","Epoch 19 Batch 4000/4934 Accuracy 0.2717 Loss 4.6884 Time elapsed 341.85s\n","Epoch 19 Batch 4100/4934 Accuracy 0.2714 Loss 4.6918 Time elapsed 350.39s\n","Epoch 19 Batch 4200/4934 Accuracy 0.2711 Loss 4.6950 Time elapsed 358.94s\n","Epoch 19 Batch 4300/4934 Accuracy 0.2709 Loss 4.6965 Time elapsed 367.48s\n","Epoch 19 Batch 4400/4934 Accuracy 0.2708 Loss 4.6980 Time elapsed 376.03s\n","Epoch 19 Batch 4500/4934 Accuracy 0.2707 Loss 4.6994 Time elapsed 384.57s\n","Epoch 19 Batch 4600/4934 Accuracy 0.2705 Loss 4.7011 Time elapsed 393.12s\n","Epoch 19 Batch 4700/4934 Accuracy 0.2705 Loss 4.7021 Time elapsed 401.67s\n","Epoch 19 Batch 4800/4934 Accuracy 0.2705 Loss 4.7025 Time elapsed 410.21s\n","Epoch 19 Batch 4900/4934 Accuracy 0.2705 Loss 4.7038 Time elapsed 418.76s\n","Epoch 19 Batch 4934/4934 Accuracy 0.2705 Loss 4.7044 Time elapsed 421.67s\n","Epoch 19 Accuracy 0.2705 Loss 4.7044 Time 422.75 seconds\n","\n","Epoch 20 Batch 100/4934 Accuracy 0.2672 Loss 4.7576 Time elapsed 8.54s\n","Epoch 20 Batch 200/4934 Accuracy 0.2692 Loss 4.7262 Time elapsed 17.09s\n","Epoch 20 Batch 300/4934 Accuracy 0.2737 Loss 4.6837 Time elapsed 25.63s\n","Epoch 20 Batch 400/4934 Accuracy 0.2764 Loss 4.6512 Time elapsed 34.18s\n","Epoch 20 Batch 500/4934 Accuracy 0.2788 Loss 4.6244 Time elapsed 42.73s\n","Epoch 20 Batch 600/4934 Accuracy 0.2795 Loss 4.6095 Time elapsed 51.27s\n","Epoch 20 Batch 700/4934 Accuracy 0.2808 Loss 4.5892 Time elapsed 59.82s\n","Epoch 20 Batch 800/4934 Accuracy 0.2814 Loss 4.5785 Time elapsed 68.36s\n","Epoch 20 Batch 900/4934 Accuracy 0.2828 Loss 4.5611 Time elapsed 76.91s\n","Epoch 20 Batch 1000/4934 Accuracy 0.2838 Loss 4.5484 Time elapsed 85.46s\n","Epoch 20 Batch 1100/4934 Accuracy 0.2844 Loss 4.5397 Time elapsed 94.00s\n","Epoch 20 Batch 1200/4934 Accuracy 0.2852 Loss 4.5310 Time elapsed 102.55s\n","Epoch 20 Batch 1300/4934 Accuracy 0.2860 Loss 4.5238 Time elapsed 111.09s\n","Epoch 20 Batch 1400/4934 Accuracy 0.2870 Loss 4.5139 Time elapsed 119.64s\n","Epoch 20 Batch 1500/4934 Accuracy 0.2875 Loss 4.5068 Time elapsed 128.18s\n","Epoch 20 Batch 1600/4934 Accuracy 0.2881 Loss 4.5005 Time elapsed 136.73s\n","Epoch 20 Batch 1700/4934 Accuracy 0.2883 Loss 4.4973 Time elapsed 145.28s\n","Epoch 20 Batch 1800/4934 Accuracy 0.2888 Loss 4.4941 Time elapsed 153.82s\n","Epoch 20 Batch 1900/4934 Accuracy 0.2893 Loss 4.4916 Time elapsed 162.37s\n","Epoch 20 Batch 2000/4934 Accuracy 0.2895 Loss 4.4897 Time elapsed 170.92s\n","Epoch 20 Batch 2100/4934 Accuracy 0.2900 Loss 4.4873 Time elapsed 179.46s\n","Epoch 20 Batch 2200/4934 Accuracy 0.2904 Loss 4.4838 Time elapsed 188.01s\n","Epoch 20 Batch 2300/4934 Accuracy 0.2908 Loss 4.4824 Time elapsed 196.55s\n","Epoch 20 Batch 2400/4934 Accuracy 0.2910 Loss 4.4801 Time elapsed 205.10s\n","Epoch 20 Batch 2500/4934 Accuracy 0.2912 Loss 4.4786 Time elapsed 213.65s\n","Epoch 20 Batch 2600/4934 Accuracy 0.2913 Loss 4.4785 Time elapsed 222.19s\n","Epoch 20 Batch 2700/4934 Accuracy 0.2914 Loss 4.4778 Time elapsed 230.74s\n","Epoch 20 Batch 2800/4934 Accuracy 0.2917 Loss 4.4753 Time elapsed 239.28s\n","Epoch 20 Batch 2900/4934 Accuracy 0.2920 Loss 4.4704 Time elapsed 247.83s\n","Epoch 20 Batch 3000/4934 Accuracy 0.2923 Loss 4.4646 Time elapsed 256.37s\n","Epoch 20 Batch 3100/4934 Accuracy 0.2924 Loss 4.4606 Time elapsed 264.91s\n","Epoch 20 Batch 3200/4934 Accuracy 0.2922 Loss 4.4605 Time elapsed 273.46s\n","Epoch 20 Batch 3300/4934 Accuracy 0.2921 Loss 4.4599 Time elapsed 282.00s\n","Epoch 20 Batch 3400/4934 Accuracy 0.2917 Loss 4.4615 Time elapsed 290.55s\n","Epoch 20 Batch 3500/4934 Accuracy 0.2914 Loss 4.4624 Time elapsed 299.10s\n","Epoch 20 Batch 3600/4934 Accuracy 0.2911 Loss 4.4655 Time elapsed 307.64s\n","Epoch 20 Batch 3700/4934 Accuracy 0.2906 Loss 4.4694 Time elapsed 316.19s\n","Epoch 20 Batch 3800/4934 Accuracy 0.2902 Loss 4.4729 Time elapsed 324.73s\n","Epoch 20 Batch 3900/4934 Accuracy 0.2898 Loss 4.4769 Time elapsed 333.28s\n","Epoch 20 Batch 4000/4934 Accuracy 0.2893 Loss 4.4810 Time elapsed 341.82s\n","Epoch 20 Batch 4100/4934 Accuracy 0.2889 Loss 4.4849 Time elapsed 350.37s\n","Epoch 20 Batch 4200/4934 Accuracy 0.2886 Loss 4.4886 Time elapsed 358.92s\n","Epoch 20 Batch 4300/4934 Accuracy 0.2884 Loss 4.4909 Time elapsed 367.46s\n","Epoch 20 Batch 4400/4934 Accuracy 0.2883 Loss 4.4928 Time elapsed 376.01s\n","Epoch 20 Batch 4500/4934 Accuracy 0.2882 Loss 4.4947 Time elapsed 384.56s\n","Epoch 20 Batch 4600/4934 Accuracy 0.2880 Loss 4.4971 Time elapsed 393.10s\n","Epoch 20 Batch 4700/4934 Accuracy 0.2879 Loss 4.4987 Time elapsed 401.65s\n","Epoch 20 Batch 4800/4934 Accuracy 0.2879 Loss 4.5000 Time elapsed 410.20s\n","Epoch 20 Batch 4900/4934 Accuracy 0.2878 Loss 4.5016 Time elapsed 418.75s\n","Epoch 20 Batch 4934/4934 Accuracy 0.2878 Loss 4.5024 Time elapsed 421.66s\n","Epoch 20 Accuracy 0.2878 Loss 4.5024 Time 422.72 seconds\n","\n","Epoch 21 Batch 100/4934 Accuracy 0.2780 Loss 4.6061 Time elapsed 8.54s\n","Epoch 21 Batch 200/4934 Accuracy 0.2816 Loss 4.5691 Time elapsed 17.09s\n","Epoch 21 Batch 300/4934 Accuracy 0.2860 Loss 4.5282 Time elapsed 25.63s\n","Epoch 21 Batch 400/4934 Accuracy 0.2894 Loss 4.4956 Time elapsed 34.18s\n","Epoch 21 Batch 500/4934 Accuracy 0.2918 Loss 4.4698 Time elapsed 42.73s\n","Epoch 21 Batch 600/4934 Accuracy 0.2926 Loss 4.4564 Time elapsed 51.27s\n","Epoch 21 Batch 700/4934 Accuracy 0.2940 Loss 4.4356 Time elapsed 59.81s\n","Epoch 21 Batch 800/4934 Accuracy 0.2947 Loss 4.4253 Time elapsed 68.36s\n","Epoch 21 Batch 900/4934 Accuracy 0.2965 Loss 4.4090 Time elapsed 76.90s\n","Epoch 21 Batch 1000/4934 Accuracy 0.2977 Loss 4.3959 Time elapsed 85.45s\n","Epoch 21 Batch 1100/4934 Accuracy 0.2982 Loss 4.3868 Time elapsed 93.99s\n","Epoch 21 Batch 1200/4934 Accuracy 0.2991 Loss 4.3784 Time elapsed 102.54s\n","Epoch 21 Batch 1300/4934 Accuracy 0.3000 Loss 4.3712 Time elapsed 111.09s\n","Epoch 21 Batch 1400/4934 Accuracy 0.3010 Loss 4.3618 Time elapsed 119.63s\n","Epoch 21 Batch 1500/4934 Accuracy 0.3019 Loss 4.3553 Time elapsed 128.18s\n","Epoch 21 Batch 1600/4934 Accuracy 0.3024 Loss 4.3493 Time elapsed 136.72s\n","Epoch 21 Batch 1700/4934 Accuracy 0.3028 Loss 4.3465 Time elapsed 145.27s\n","Epoch 21 Batch 1800/4934 Accuracy 0.3032 Loss 4.3439 Time elapsed 153.82s\n","Epoch 21 Batch 1900/4934 Accuracy 0.3037 Loss 4.3418 Time elapsed 162.36s\n","Epoch 21 Batch 2000/4934 Accuracy 0.3039 Loss 4.3407 Time elapsed 170.91s\n","Epoch 21 Batch 2100/4934 Accuracy 0.3043 Loss 4.3388 Time elapsed 179.45s\n","Epoch 21 Batch 2200/4934 Accuracy 0.3047 Loss 4.3353 Time elapsed 188.01s\n","Epoch 21 Batch 2300/4934 Accuracy 0.3049 Loss 4.3342 Time elapsed 196.55s\n","Epoch 21 Batch 2400/4934 Accuracy 0.3053 Loss 4.3322 Time elapsed 205.10s\n","Epoch 21 Batch 2500/4934 Accuracy 0.3053 Loss 4.3308 Time elapsed 213.65s\n","Epoch 21 Batch 2600/4934 Accuracy 0.3054 Loss 4.3310 Time elapsed 222.19s\n","Epoch 21 Batch 2700/4934 Accuracy 0.3055 Loss 4.3307 Time elapsed 230.74s\n","Epoch 21 Batch 2800/4934 Accuracy 0.3058 Loss 4.3282 Time elapsed 239.28s\n","Epoch 21 Batch 2900/4934 Accuracy 0.3062 Loss 4.3228 Time elapsed 247.83s\n","Epoch 21 Batch 3000/4934 Accuracy 0.3067 Loss 4.3162 Time elapsed 256.37s\n","Epoch 21 Batch 3100/4934 Accuracy 0.3069 Loss 4.3113 Time elapsed 264.92s\n","Epoch 21 Batch 3200/4934 Accuracy 0.3068 Loss 4.3112 Time elapsed 273.47s\n","Epoch 21 Batch 3300/4934 Accuracy 0.3067 Loss 4.3105 Time elapsed 282.01s\n","Epoch 21 Batch 3400/4934 Accuracy 0.3063 Loss 4.3120 Time elapsed 290.56s\n","Epoch 21 Batch 3500/4934 Accuracy 0.3061 Loss 4.3129 Time elapsed 299.10s\n","Epoch 21 Batch 3600/4934 Accuracy 0.3057 Loss 4.3164 Time elapsed 307.65s\n","Epoch 21 Batch 3700/4934 Accuracy 0.3052 Loss 4.3206 Time elapsed 316.19s\n","Epoch 21 Batch 3800/4934 Accuracy 0.3047 Loss 4.3240 Time elapsed 324.74s\n","Epoch 21 Batch 3900/4934 Accuracy 0.3043 Loss 4.3285 Time elapsed 333.29s\n","Epoch 21 Batch 4000/4934 Accuracy 0.3038 Loss 4.3330 Time elapsed 341.84s\n","Epoch 21 Batch 4100/4934 Accuracy 0.3034 Loss 4.3372 Time elapsed 350.38s\n","Epoch 21 Batch 4200/4934 Accuracy 0.3031 Loss 4.3415 Time elapsed 358.93s\n","Epoch 21 Batch 4300/4934 Accuracy 0.3029 Loss 4.3439 Time elapsed 367.48s\n","Epoch 21 Batch 4400/4934 Accuracy 0.3027 Loss 4.3462 Time elapsed 376.02s\n","Epoch 21 Batch 4500/4934 Accuracy 0.3026 Loss 4.3484 Time elapsed 384.57s\n","Epoch 21 Batch 4600/4934 Accuracy 0.3023 Loss 4.3512 Time elapsed 393.12s\n","Epoch 21 Batch 4700/4934 Accuracy 0.3022 Loss 4.3529 Time elapsed 401.67s\n","Epoch 21 Batch 4800/4934 Accuracy 0.3021 Loss 4.3545 Time elapsed 410.22s\n","Epoch 21 Batch 4900/4934 Accuracy 0.3021 Loss 4.3565 Time elapsed 418.76s\n","Epoch 21 Batch 4934/4934 Accuracy 0.3021 Loss 4.3574 Time elapsed 421.68s\n","Epoch 21 Accuracy 0.3021 Loss 4.3574 Time 422.74 seconds\n","\n","Epoch 22 Batch 100/4934 Accuracy 0.2896 Loss 4.4877 Time elapsed 8.54s\n","Epoch 22 Batch 200/4934 Accuracy 0.2928 Loss 4.4537 Time elapsed 17.08s\n","Epoch 22 Batch 300/4934 Accuracy 0.2978 Loss 4.4116 Time elapsed 25.63s\n","Epoch 22 Batch 400/4934 Accuracy 0.3007 Loss 4.3781 Time elapsed 34.17s\n","Epoch 22 Batch 500/4934 Accuracy 0.3030 Loss 4.3548 Time elapsed 42.72s\n","Epoch 22 Batch 600/4934 Accuracy 0.3037 Loss 4.3398 Time elapsed 51.26s\n","Epoch 22 Batch 700/4934 Accuracy 0.3057 Loss 4.3196 Time elapsed 59.81s\n","Epoch 22 Batch 800/4934 Accuracy 0.3063 Loss 4.3094 Time elapsed 68.35s\n","Epoch 22 Batch 900/4934 Accuracy 0.3078 Loss 4.2933 Time elapsed 76.90s\n","Epoch 22 Batch 1000/4934 Accuracy 0.3090 Loss 4.2806 Time elapsed 85.44s\n","Epoch 22 Batch 1100/4934 Accuracy 0.3098 Loss 4.2717 Time elapsed 93.99s\n","Epoch 22 Batch 1200/4934 Accuracy 0.3107 Loss 4.2636 Time elapsed 102.53s\n","Epoch 22 Batch 1300/4934 Accuracy 0.3112 Loss 4.2563 Time elapsed 111.08s\n","Epoch 22 Batch 1400/4934 Accuracy 0.3122 Loss 4.2471 Time elapsed 119.63s\n","Epoch 22 Batch 1500/4934 Accuracy 0.3129 Loss 4.2412 Time elapsed 128.18s\n","Epoch 22 Batch 1600/4934 Accuracy 0.3136 Loss 4.2353 Time elapsed 136.72s\n","Epoch 22 Batch 1700/4934 Accuracy 0.3140 Loss 4.2323 Time elapsed 145.27s\n","Epoch 22 Batch 1800/4934 Accuracy 0.3144 Loss 4.2293 Time elapsed 153.81s\n","Epoch 22 Batch 1900/4934 Accuracy 0.3147 Loss 4.2274 Time elapsed 162.36s\n","Epoch 22 Batch 2000/4934 Accuracy 0.3149 Loss 4.2264 Time elapsed 170.91s\n","Epoch 22 Batch 2100/4934 Accuracy 0.3154 Loss 4.2247 Time elapsed 179.45s\n","Epoch 22 Batch 2200/4934 Accuracy 0.3158 Loss 4.2217 Time elapsed 188.00s\n","Epoch 22 Batch 2300/4934 Accuracy 0.3160 Loss 4.2204 Time elapsed 196.54s\n","Epoch 22 Batch 2400/4934 Accuracy 0.3163 Loss 4.2182 Time elapsed 205.08s\n","Epoch 22 Batch 2500/4934 Accuracy 0.3164 Loss 4.2167 Time elapsed 213.63s\n","Epoch 22 Batch 2600/4934 Accuracy 0.3165 Loss 4.2171 Time elapsed 222.17s\n","Epoch 22 Batch 2700/4934 Accuracy 0.3167 Loss 4.2169 Time elapsed 230.72s\n","Epoch 22 Batch 2800/4934 Accuracy 0.3169 Loss 4.2148 Time elapsed 239.26s\n","Epoch 22 Batch 2900/4934 Accuracy 0.3175 Loss 4.2092 Time elapsed 247.81s\n","Epoch 22 Batch 3000/4934 Accuracy 0.3180 Loss 4.2019 Time elapsed 256.35s\n","Epoch 22 Batch 3100/4934 Accuracy 0.3182 Loss 4.1967 Time elapsed 264.89s\n","Epoch 22 Batch 3200/4934 Accuracy 0.3181 Loss 4.1960 Time elapsed 273.43s\n","Epoch 22 Batch 3300/4934 Accuracy 0.3180 Loss 4.1950 Time elapsed 281.98s\n","Epoch 22 Batch 3400/4934 Accuracy 0.3177 Loss 4.1963 Time elapsed 290.53s\n","Epoch 22 Batch 3500/4934 Accuracy 0.3174 Loss 4.1971 Time elapsed 299.07s\n","Epoch 22 Batch 3600/4934 Accuracy 0.3170 Loss 4.2003 Time elapsed 307.62s\n","Epoch 22 Batch 3700/4934 Accuracy 0.3165 Loss 4.2045 Time elapsed 316.17s\n","Epoch 22 Batch 3800/4934 Accuracy 0.3160 Loss 4.2083 Time elapsed 324.72s\n","Epoch 22 Batch 3900/4934 Accuracy 0.3155 Loss 4.2127 Time elapsed 333.26s\n","Epoch 22 Batch 4000/4934 Accuracy 0.3151 Loss 4.2172 Time elapsed 341.81s\n","Epoch 22 Batch 4100/4934 Accuracy 0.3147 Loss 4.2216 Time elapsed 350.36s\n","Epoch 22 Batch 4200/4934 Accuracy 0.3144 Loss 4.2259 Time elapsed 358.90s\n","Epoch 22 Batch 4300/4934 Accuracy 0.3141 Loss 4.2285 Time elapsed 367.45s\n","Epoch 22 Batch 4400/4934 Accuracy 0.3140 Loss 4.2311 Time elapsed 376.00s\n","Epoch 22 Batch 4500/4934 Accuracy 0.3139 Loss 4.2333 Time elapsed 384.55s\n","Epoch 22 Batch 4600/4934 Accuracy 0.3137 Loss 4.2362 Time elapsed 393.09s\n","Epoch 22 Batch 4700/4934 Accuracy 0.3135 Loss 4.2384 Time elapsed 401.64s\n","Epoch 22 Batch 4800/4934 Accuracy 0.3135 Loss 4.2403 Time elapsed 410.19s\n","Epoch 22 Batch 4900/4934 Accuracy 0.3134 Loss 4.2424 Time elapsed 418.74s\n","Epoch 22 Batch 4934/4934 Accuracy 0.3134 Loss 4.2433 Time elapsed 421.65s\n","Epoch 22 Accuracy 0.3134 Loss 4.2433 Time 422.72 seconds\n","\n","Epoch 23 Batch 100/4934 Accuracy 0.2993 Loss 4.3899 Time elapsed 8.55s\n","Epoch 23 Batch 200/4934 Accuracy 0.3021 Loss 4.3608 Time elapsed 17.10s\n","Epoch 23 Batch 300/4934 Accuracy 0.3060 Loss 4.3217 Time elapsed 25.64s\n","Epoch 23 Batch 400/4934 Accuracy 0.3089 Loss 4.2893 Time elapsed 34.19s\n","Epoch 23 Batch 500/4934 Accuracy 0.3113 Loss 4.2641 Time elapsed 42.73s\n","Epoch 23 Batch 600/4934 Accuracy 0.3124 Loss 4.2497 Time elapsed 51.28s\n","Epoch 23 Batch 700/4934 Accuracy 0.3139 Loss 4.2311 Time elapsed 59.82s\n","Epoch 23 Batch 800/4934 Accuracy 0.3148 Loss 4.2219 Time elapsed 68.37s\n","Epoch 23 Batch 900/4934 Accuracy 0.3160 Loss 4.2053 Time elapsed 76.92s\n","Epoch 23 Batch 1000/4934 Accuracy 0.3173 Loss 4.1931 Time elapsed 85.46s\n","Epoch 23 Batch 1100/4934 Accuracy 0.3181 Loss 4.1841 Time elapsed 94.01s\n","Epoch 23 Batch 1200/4934 Accuracy 0.3191 Loss 4.1764 Time elapsed 102.56s\n","Epoch 23 Batch 1300/4934 Accuracy 0.3197 Loss 4.1690 Time elapsed 111.10s\n","Epoch 23 Batch 1400/4934 Accuracy 0.3208 Loss 4.1596 Time elapsed 119.65s\n","Epoch 23 Batch 1500/4934 Accuracy 0.3216 Loss 4.1527 Time elapsed 128.20s\n","Epoch 23 Batch 1600/4934 Accuracy 0.3222 Loss 4.1461 Time elapsed 136.75s\n","Epoch 23 Batch 1700/4934 Accuracy 0.3225 Loss 4.1428 Time elapsed 145.29s\n","Epoch 23 Batch 1800/4934 Accuracy 0.3230 Loss 4.1400 Time elapsed 153.95s\n","Epoch 23 Batch 1900/4934 Accuracy 0.3235 Loss 4.1381 Time elapsed 162.50s\n","Epoch 23 Batch 2000/4934 Accuracy 0.3238 Loss 4.1372 Time elapsed 171.05s\n","Epoch 23 Batch 2100/4934 Accuracy 0.3242 Loss 4.1356 Time elapsed 179.59s\n","Epoch 23 Batch 2200/4934 Accuracy 0.3247 Loss 4.1331 Time elapsed 188.14s\n","Epoch 23 Batch 2300/4934 Accuracy 0.3250 Loss 4.1315 Time elapsed 196.70s\n","Epoch 23 Batch 2400/4934 Accuracy 0.3254 Loss 4.1291 Time elapsed 205.26s\n","Epoch 23 Batch 2500/4934 Accuracy 0.3256 Loss 4.1279 Time elapsed 213.81s\n","Epoch 23 Batch 2600/4934 Accuracy 0.3257 Loss 4.1282 Time elapsed 222.36s\n","Epoch 23 Batch 2700/4934 Accuracy 0.3259 Loss 4.1278 Time elapsed 230.92s\n","Epoch 23 Batch 2800/4934 Accuracy 0.3262 Loss 4.1253 Time elapsed 239.48s\n","Epoch 23 Batch 2900/4934 Accuracy 0.3269 Loss 4.1190 Time elapsed 248.03s\n","Epoch 23 Batch 3000/4934 Accuracy 0.3275 Loss 4.1109 Time elapsed 256.59s\n","Epoch 23 Batch 3100/4934 Accuracy 0.3278 Loss 4.1053 Time elapsed 265.16s\n","Epoch 23 Batch 3200/4934 Accuracy 0.3278 Loss 4.1042 Time elapsed 273.72s\n","Epoch 23 Batch 3300/4934 Accuracy 0.3277 Loss 4.1030 Time elapsed 282.28s\n","Epoch 23 Batch 3400/4934 Accuracy 0.3274 Loss 4.1045 Time elapsed 290.84s\n","Epoch 23 Batch 3500/4934 Accuracy 0.3272 Loss 4.1052 Time elapsed 299.40s\n","Epoch 23 Batch 3600/4934 Accuracy 0.3268 Loss 4.1083 Time elapsed 307.95s\n","Epoch 23 Batch 3700/4934 Accuracy 0.3264 Loss 4.1125 Time elapsed 316.50s\n","Epoch 23 Batch 3800/4934 Accuracy 0.3259 Loss 4.1163 Time elapsed 325.07s\n","Epoch 23 Batch 3900/4934 Accuracy 0.3254 Loss 4.1208 Time elapsed 333.62s\n","Epoch 23 Batch 4000/4934 Accuracy 0.3249 Loss 4.1255 Time elapsed 342.18s\n","Epoch 23 Batch 4100/4934 Accuracy 0.3245 Loss 4.1300 Time elapsed 350.75s\n","Epoch 23 Batch 4200/4934 Accuracy 0.3242 Loss 4.1343 Time elapsed 359.32s\n","Epoch 23 Batch 4300/4934 Accuracy 0.3239 Loss 4.1370 Time elapsed 367.93s\n","Epoch 23 Batch 4400/4934 Accuracy 0.3237 Loss 4.1396 Time elapsed 376.49s\n","Epoch 23 Batch 4500/4934 Accuracy 0.3237 Loss 4.1421 Time elapsed 385.05s\n","Epoch 23 Batch 4600/4934 Accuracy 0.3234 Loss 4.1452 Time elapsed 393.61s\n","Epoch 23 Batch 4700/4934 Accuracy 0.3233 Loss 4.1474 Time elapsed 402.17s\n","Epoch 23 Batch 4800/4934 Accuracy 0.3232 Loss 4.1494 Time elapsed 410.74s\n","Epoch 23 Batch 4900/4934 Accuracy 0.3231 Loss 4.1517 Time elapsed 419.31s\n","Epoch 23 Batch 4934/4934 Accuracy 0.3231 Loss 4.1526 Time elapsed 422.22s\n","Epoch 23 Accuracy 0.3231 Loss 4.1526 Time 423.34 seconds\n","\n","Epoch 24 Batch 100/4934 Accuracy 0.3077 Loss 4.3198 Time elapsed 8.57s\n","Epoch 24 Batch 200/4934 Accuracy 0.3106 Loss 4.2851 Time elapsed 17.14s\n","Epoch 24 Batch 300/4934 Accuracy 0.3149 Loss 4.2429 Time elapsed 25.70s\n","Epoch 24 Batch 400/4934 Accuracy 0.3183 Loss 4.2112 Time elapsed 34.26s\n","Epoch 24 Batch 500/4934 Accuracy 0.3204 Loss 4.1878 Time elapsed 42.82s\n","Epoch 24 Batch 600/4934 Accuracy 0.3212 Loss 4.1727 Time elapsed 51.38s\n","Epoch 24 Batch 700/4934 Accuracy 0.3228 Loss 4.1526 Time elapsed 59.93s\n","Epoch 24 Batch 800/4934 Accuracy 0.3237 Loss 4.1432 Time elapsed 68.49s\n","Epoch 24 Batch 900/4934 Accuracy 0.3253 Loss 4.1278 Time elapsed 77.05s\n","Epoch 24 Batch 1000/4934 Accuracy 0.3265 Loss 4.1153 Time elapsed 85.60s\n","Epoch 24 Batch 1100/4934 Accuracy 0.3273 Loss 4.1062 Time elapsed 94.16s\n","Epoch 24 Batch 1200/4934 Accuracy 0.3283 Loss 4.0987 Time elapsed 102.73s\n","Epoch 24 Batch 1300/4934 Accuracy 0.3288 Loss 4.0913 Time elapsed 111.29s\n","Epoch 24 Batch 1400/4934 Accuracy 0.3298 Loss 4.0825 Time elapsed 119.85s\n","Epoch 24 Batch 1500/4934 Accuracy 0.3307 Loss 4.0759 Time elapsed 128.41s\n","Epoch 24 Batch 1600/4934 Accuracy 0.3313 Loss 4.0697 Time elapsed 136.98s\n","Epoch 24 Batch 1700/4934 Accuracy 0.3317 Loss 4.0667 Time elapsed 145.54s\n","Epoch 24 Batch 1800/4934 Accuracy 0.3322 Loss 4.0634 Time elapsed 154.10s\n","Epoch 24 Batch 1900/4934 Accuracy 0.3325 Loss 4.0621 Time elapsed 162.66s\n","Epoch 24 Batch 2000/4934 Accuracy 0.3328 Loss 4.0604 Time elapsed 171.22s\n","Epoch 24 Batch 2100/4934 Accuracy 0.3331 Loss 4.0588 Time elapsed 179.78s\n","Epoch 24 Batch 2200/4934 Accuracy 0.3335 Loss 4.0560 Time elapsed 188.33s\n","Epoch 24 Batch 2300/4934 Accuracy 0.3338 Loss 4.0545 Time elapsed 196.89s\n","Epoch 24 Batch 2400/4934 Accuracy 0.3341 Loss 4.0519 Time elapsed 205.45s\n","Epoch 24 Batch 2500/4934 Accuracy 0.3342 Loss 4.0504 Time elapsed 214.01s\n","Epoch 24 Batch 2600/4934 Accuracy 0.3342 Loss 4.0509 Time elapsed 222.56s\n","Epoch 24 Batch 2700/4934 Accuracy 0.3344 Loss 4.0506 Time elapsed 231.12s\n","Epoch 24 Batch 2800/4934 Accuracy 0.3349 Loss 4.0481 Time elapsed 239.67s\n","Epoch 24 Batch 2900/4934 Accuracy 0.3356 Loss 4.0411 Time elapsed 248.22s\n","Epoch 24 Batch 3000/4934 Accuracy 0.3363 Loss 4.0327 Time elapsed 256.77s\n","Epoch 24 Batch 3100/4934 Accuracy 0.3368 Loss 4.0263 Time elapsed 265.33s\n","Epoch 24 Batch 3200/4934 Accuracy 0.3368 Loss 4.0249 Time elapsed 273.89s\n","Epoch 24 Batch 3300/4934 Accuracy 0.3368 Loss 4.0233 Time elapsed 282.44s\n","Epoch 24 Batch 3400/4934 Accuracy 0.3366 Loss 4.0243 Time elapsed 291.00s\n","Epoch 24 Batch 3500/4934 Accuracy 0.3364 Loss 4.0251 Time elapsed 299.55s\n","Epoch 24 Batch 3600/4934 Accuracy 0.3360 Loss 4.0283 Time elapsed 308.11s\n","Epoch 24 Batch 3700/4934 Accuracy 0.3354 Loss 4.0327 Time elapsed 316.66s\n","Epoch 24 Batch 3800/4934 Accuracy 0.3350 Loss 4.0364 Time elapsed 325.22s\n","Epoch 24 Batch 3900/4934 Accuracy 0.3345 Loss 4.0411 Time elapsed 333.78s\n","Epoch 24 Batch 4000/4934 Accuracy 0.3340 Loss 4.0461 Time elapsed 342.33s\n","Epoch 24 Batch 4100/4934 Accuracy 0.3336 Loss 4.0508 Time elapsed 350.89s\n","Epoch 24 Batch 4200/4934 Accuracy 0.3332 Loss 4.0554 Time elapsed 359.44s\n","Epoch 24 Batch 4300/4934 Accuracy 0.3329 Loss 4.0582 Time elapsed 368.00s\n","Epoch 24 Batch 4400/4934 Accuracy 0.3327 Loss 4.0611 Time elapsed 376.56s\n","Epoch 24 Batch 4500/4934 Accuracy 0.3326 Loss 4.0636 Time elapsed 385.13s\n","Epoch 24 Batch 4600/4934 Accuracy 0.3323 Loss 4.0667 Time elapsed 393.69s\n","Epoch 24 Batch 4700/4934 Accuracy 0.3322 Loss 4.0688 Time elapsed 402.25s\n","Epoch 24 Batch 4800/4934 Accuracy 0.3321 Loss 4.0709 Time elapsed 410.81s\n","Epoch 24 Batch 4900/4934 Accuracy 0.3321 Loss 4.0733 Time elapsed 419.37s\n","Epoch 24 Batch 4934/4934 Accuracy 0.3320 Loss 4.0742 Time elapsed 422.28s\n","Epoch 24 Accuracy 0.3320 Loss 4.0742 Time 423.36 seconds\n","\n","Epoch 25 Batch 100/4934 Accuracy 0.3141 Loss 4.2530 Time elapsed 8.55s\n","Epoch 25 Batch 200/4934 Accuracy 0.3165 Loss 4.2173 Time elapsed 17.11s\n","Epoch 25 Batch 300/4934 Accuracy 0.3220 Loss 4.1735 Time elapsed 25.67s\n","Epoch 25 Batch 400/4934 Accuracy 0.3254 Loss 4.1420 Time elapsed 34.23s\n","Epoch 25 Batch 500/4934 Accuracy 0.3279 Loss 4.1181 Time elapsed 42.79s\n","Epoch 25 Batch 600/4934 Accuracy 0.3289 Loss 4.1025 Time elapsed 51.36s\n","Epoch 25 Batch 700/4934 Accuracy 0.3307 Loss 4.0828 Time elapsed 59.92s\n","Epoch 25 Batch 800/4934 Accuracy 0.3316 Loss 4.0738 Time elapsed 68.48s\n","Epoch 25 Batch 900/4934 Accuracy 0.3331 Loss 4.0590 Time elapsed 77.04s\n","Epoch 25 Batch 1000/4934 Accuracy 0.3341 Loss 4.0467 Time elapsed 85.61s\n","Epoch 25 Batch 1100/4934 Accuracy 0.3347 Loss 4.0380 Time elapsed 94.17s\n","Epoch 25 Batch 1200/4934 Accuracy 0.3359 Loss 4.0299 Time elapsed 102.74s\n","Epoch 25 Batch 1300/4934 Accuracy 0.3364 Loss 4.0220 Time elapsed 111.30s\n","Epoch 25 Batch 1400/4934 Accuracy 0.3375 Loss 4.0132 Time elapsed 119.86s\n","Epoch 25 Batch 1500/4934 Accuracy 0.3384 Loss 4.0070 Time elapsed 128.43s\n","Epoch 25 Batch 1600/4934 Accuracy 0.3392 Loss 4.0008 Time elapsed 136.99s\n","Epoch 25 Batch 1700/4934 Accuracy 0.3395 Loss 3.9974 Time elapsed 145.56s\n","Epoch 25 Batch 1800/4934 Accuracy 0.3400 Loss 3.9942 Time elapsed 154.12s\n","Epoch 25 Batch 1900/4934 Accuracy 0.3404 Loss 3.9926 Time elapsed 162.69s\n","Epoch 25 Batch 2000/4934 Accuracy 0.3407 Loss 3.9913 Time elapsed 171.25s\n","Epoch 25 Batch 2100/4934 Accuracy 0.3410 Loss 3.9898 Time elapsed 179.81s\n","Epoch 25 Batch 2200/4934 Accuracy 0.3416 Loss 3.9869 Time elapsed 188.37s\n","Epoch 25 Batch 2300/4934 Accuracy 0.3418 Loss 3.9852 Time elapsed 196.93s\n","Epoch 25 Batch 2400/4934 Accuracy 0.3421 Loss 3.9832 Time elapsed 205.48s\n","Epoch 25 Batch 2500/4934 Accuracy 0.3423 Loss 3.9818 Time elapsed 214.05s\n","Epoch 25 Batch 2600/4934 Accuracy 0.3423 Loss 3.9821 Time elapsed 222.62s\n","Epoch 25 Batch 2700/4934 Accuracy 0.3423 Loss 3.9819 Time elapsed 231.18s\n","Epoch 25 Batch 2800/4934 Accuracy 0.3427 Loss 3.9791 Time elapsed 239.74s\n","Epoch 25 Batch 2900/4934 Accuracy 0.3434 Loss 3.9721 Time elapsed 248.30s\n","Epoch 25 Batch 3000/4934 Accuracy 0.3442 Loss 3.9633 Time elapsed 256.85s\n","Epoch 25 Batch 3100/4934 Accuracy 0.3447 Loss 3.9566 Time elapsed 265.41s\n","Epoch 25 Batch 3200/4934 Accuracy 0.3448 Loss 3.9551 Time elapsed 273.97s\n","Epoch 25 Batch 3300/4934 Accuracy 0.3448 Loss 3.9534 Time elapsed 282.52s\n","Epoch 25 Batch 3400/4934 Accuracy 0.3445 Loss 3.9544 Time elapsed 291.08s\n","Epoch 25 Batch 3500/4934 Accuracy 0.3443 Loss 3.9553 Time elapsed 299.64s\n","Epoch 25 Batch 3600/4934 Accuracy 0.3439 Loss 3.9586 Time elapsed 308.20s\n","Epoch 25 Batch 3700/4934 Accuracy 0.3435 Loss 3.9630 Time elapsed 316.76s\n","Epoch 25 Batch 3800/4934 Accuracy 0.3431 Loss 3.9668 Time elapsed 325.33s\n","Epoch 25 Batch 3900/4934 Accuracy 0.3425 Loss 3.9716 Time elapsed 333.90s\n","Epoch 25 Batch 4000/4934 Accuracy 0.3420 Loss 3.9764 Time elapsed 342.45s\n","Epoch 25 Batch 4100/4934 Accuracy 0.3416 Loss 3.9810 Time elapsed 351.01s\n","Epoch 25 Batch 4200/4934 Accuracy 0.3412 Loss 3.9857 Time elapsed 359.56s\n","Epoch 25 Batch 4300/4934 Accuracy 0.3409 Loss 3.9883 Time elapsed 368.12s\n","Epoch 25 Batch 4400/4934 Accuracy 0.3407 Loss 3.9911 Time elapsed 376.70s\n","Epoch 25 Batch 4500/4934 Accuracy 0.3405 Loss 3.9938 Time elapsed 385.27s\n","Epoch 25 Batch 4600/4934 Accuracy 0.3403 Loss 3.9968 Time elapsed 393.83s\n","Epoch 25 Batch 4700/4934 Accuracy 0.3401 Loss 3.9990 Time elapsed 402.40s\n","Epoch 25 Batch 4800/4934 Accuracy 0.3400 Loss 4.0011 Time elapsed 410.97s\n","Epoch 25 Batch 4900/4934 Accuracy 0.3400 Loss 4.0032 Time elapsed 419.52s\n","Epoch 25 Batch 4934/4934 Accuracy 0.3400 Loss 4.0041 Time elapsed 422.44s\n","Epoch 25 Accuracy 0.3400 Loss 4.0041 Time 424.48 seconds\n","\n","Epoch 26 Batch 100/4934 Accuracy 0.3207 Loss 4.1927 Time elapsed 8.56s\n","Epoch 26 Batch 200/4934 Accuracy 0.3235 Loss 4.1614 Time elapsed 17.12s\n","Epoch 26 Batch 300/4934 Accuracy 0.3285 Loss 4.1216 Time elapsed 25.67s\n","Epoch 26 Batch 400/4934 Accuracy 0.3317 Loss 4.0900 Time elapsed 34.23s\n","Epoch 26 Batch 500/4934 Accuracy 0.3340 Loss 4.0670 Time elapsed 42.78s\n","Epoch 26 Batch 600/4934 Accuracy 0.3348 Loss 4.0514 Time elapsed 51.34s\n","Epoch 26 Batch 700/4934 Accuracy 0.3363 Loss 4.0321 Time elapsed 59.90s\n","Epoch 26 Batch 800/4934 Accuracy 0.3371 Loss 4.0221 Time elapsed 68.47s\n","Epoch 26 Batch 900/4934 Accuracy 0.3389 Loss 4.0050 Time elapsed 77.03s\n","Epoch 26 Batch 1000/4934 Accuracy 0.3400 Loss 3.9926 Time elapsed 85.60s\n","Epoch 26 Batch 1100/4934 Accuracy 0.3409 Loss 3.9833 Time elapsed 94.17s\n","Epoch 26 Batch 1200/4934 Accuracy 0.3420 Loss 3.9752 Time elapsed 102.73s\n","Epoch 26 Batch 1300/4934 Accuracy 0.3427 Loss 3.9678 Time elapsed 111.30s\n","Epoch 26 Batch 1400/4934 Accuracy 0.3437 Loss 3.9593 Time elapsed 119.91s\n","Epoch 26 Batch 1500/4934 Accuracy 0.3445 Loss 3.9524 Time elapsed 128.48s\n","Epoch 26 Batch 1600/4934 Accuracy 0.3454 Loss 3.9458 Time elapsed 137.05s\n","Epoch 26 Batch 1700/4934 Accuracy 0.3459 Loss 3.9429 Time elapsed 145.62s\n","Epoch 26 Batch 1800/4934 Accuracy 0.3463 Loss 3.9401 Time elapsed 154.20s\n","Epoch 26 Batch 1900/4934 Accuracy 0.3467 Loss 3.9381 Time elapsed 162.77s\n","Epoch 26 Batch 2000/4934 Accuracy 0.3470 Loss 3.9373 Time elapsed 171.34s\n","Epoch 26 Batch 2100/4934 Accuracy 0.3474 Loss 3.9354 Time elapsed 179.91s\n","Epoch 26 Batch 2200/4934 Accuracy 0.3478 Loss 3.9324 Time elapsed 188.49s\n","Epoch 26 Batch 2300/4934 Accuracy 0.3481 Loss 3.9307 Time elapsed 197.06s\n","Epoch 26 Batch 2400/4934 Accuracy 0.3485 Loss 3.9280 Time elapsed 205.63s\n","Epoch 26 Batch 2500/4934 Accuracy 0.3487 Loss 3.9267 Time elapsed 214.20s\n","Epoch 26 Batch 2600/4934 Accuracy 0.3487 Loss 3.9271 Time elapsed 222.77s\n","Epoch 26 Batch 2700/4934 Accuracy 0.3488 Loss 3.9265 Time elapsed 231.33s\n","Epoch 26 Batch 2800/4934 Accuracy 0.3492 Loss 3.9236 Time elapsed 239.90s\n","Epoch 26 Batch 2900/4934 Accuracy 0.3500 Loss 3.9161 Time elapsed 248.47s\n","Epoch 26 Batch 3000/4934 Accuracy 0.3509 Loss 3.9062 Time elapsed 257.04s\n","Epoch 26 Batch 3100/4934 Accuracy 0.3514 Loss 3.8992 Time elapsed 265.61s\n","Epoch 26 Batch 3200/4934 Accuracy 0.3515 Loss 3.8973 Time elapsed 274.19s\n","Epoch 26 Batch 3300/4934 Accuracy 0.3517 Loss 3.8951 Time elapsed 282.76s\n","Epoch 26 Batch 3400/4934 Accuracy 0.3514 Loss 3.8957 Time elapsed 291.34s\n","Epoch 26 Batch 3500/4934 Accuracy 0.3513 Loss 3.8962 Time elapsed 299.91s\n","Epoch 26 Batch 3600/4934 Accuracy 0.3509 Loss 3.8992 Time elapsed 308.48s\n","Epoch 26 Batch 3700/4934 Accuracy 0.3505 Loss 3.9033 Time elapsed 317.05s\n","Epoch 26 Batch 3800/4934 Accuracy 0.3501 Loss 3.9070 Time elapsed 325.62s\n","Epoch 26 Batch 3900/4934 Accuracy 0.3496 Loss 3.9119 Time elapsed 334.19s\n","Epoch 26 Batch 4000/4934 Accuracy 0.3491 Loss 3.9170 Time elapsed 342.76s\n","Epoch 26 Batch 4100/4934 Accuracy 0.3486 Loss 3.9217 Time elapsed 351.32s\n","Epoch 26 Batch 4200/4934 Accuracy 0.3482 Loss 3.9264 Time elapsed 359.89s\n","Epoch 26 Batch 4300/4934 Accuracy 0.3479 Loss 3.9293 Time elapsed 368.46s\n","Epoch 26 Batch 4400/4934 Accuracy 0.3477 Loss 3.9319 Time elapsed 377.02s\n","Epoch 26 Batch 4500/4934 Accuracy 0.3475 Loss 3.9346 Time elapsed 385.59s\n","Epoch 26 Batch 4600/4934 Accuracy 0.3473 Loss 3.9377 Time elapsed 394.16s\n","Epoch 26 Batch 4700/4934 Accuracy 0.3471 Loss 3.9398 Time elapsed 402.73s\n","Epoch 26 Batch 4800/4934 Accuracy 0.3472 Loss 3.9416 Time elapsed 411.30s\n","Epoch 26 Batch 4900/4934 Accuracy 0.3471 Loss 3.9439 Time elapsed 419.87s\n","Epoch 26 Batch 4934/4934 Accuracy 0.3471 Loss 3.9447 Time elapsed 422.79s\n","Epoch 26 Accuracy 0.3471 Loss 3.9447 Time 423.93 seconds\n","\n","Epoch 27 Batch 100/4934 Accuracy 0.3251 Loss 4.1443 Time elapsed 8.56s\n","Epoch 27 Batch 200/4934 Accuracy 0.3283 Loss 4.1088 Time elapsed 17.12s\n","Epoch 27 Batch 300/4934 Accuracy 0.3340 Loss 4.0667 Time elapsed 25.69s\n","Epoch 27 Batch 400/4934 Accuracy 0.3374 Loss 4.0345 Time elapsed 34.26s\n","Epoch 27 Batch 500/4934 Accuracy 0.3401 Loss 4.0118 Time elapsed 42.82s\n","Epoch 27 Batch 600/4934 Accuracy 0.3412 Loss 3.9968 Time elapsed 51.39s\n","Epoch 27 Batch 700/4934 Accuracy 0.3426 Loss 3.9797 Time elapsed 59.95s\n","Epoch 27 Batch 800/4934 Accuracy 0.3433 Loss 3.9696 Time elapsed 68.52s\n","Epoch 27 Batch 900/4934 Accuracy 0.3449 Loss 3.9534 Time elapsed 77.09s\n","Epoch 27 Batch 1000/4934 Accuracy 0.3458 Loss 3.9409 Time elapsed 85.65s\n","Epoch 27 Batch 1100/4934 Accuracy 0.3466 Loss 3.9313 Time elapsed 94.21s\n","Epoch 27 Batch 1200/4934 Accuracy 0.3477 Loss 3.9232 Time elapsed 102.78s\n","Epoch 27 Batch 1300/4934 Accuracy 0.3485 Loss 3.9152 Time elapsed 111.34s\n","Epoch 27 Batch 1400/4934 Accuracy 0.3495 Loss 3.9066 Time elapsed 119.91s\n","Epoch 27 Batch 1500/4934 Accuracy 0.3503 Loss 3.8996 Time elapsed 128.47s\n","Epoch 27 Batch 1600/4934 Accuracy 0.3512 Loss 3.8930 Time elapsed 137.04s\n","Epoch 27 Batch 1700/4934 Accuracy 0.3517 Loss 3.8896 Time elapsed 145.60s\n","Epoch 27 Batch 1800/4934 Accuracy 0.3522 Loss 3.8864 Time elapsed 154.17s\n","Epoch 27 Batch 1900/4934 Accuracy 0.3525 Loss 3.8847 Time elapsed 162.74s\n","Epoch 27 Batch 2000/4934 Accuracy 0.3528 Loss 3.8831 Time elapsed 171.30s\n","Epoch 27 Batch 2100/4934 Accuracy 0.3532 Loss 3.8813 Time elapsed 179.87s\n","Epoch 27 Batch 2200/4934 Accuracy 0.3536 Loss 3.8790 Time elapsed 188.44s\n","Epoch 27 Batch 2300/4934 Accuracy 0.3539 Loss 3.8771 Time elapsed 197.00s\n","Epoch 27 Batch 2400/4934 Accuracy 0.3543 Loss 3.8747 Time elapsed 205.57s\n","Epoch 27 Batch 2500/4934 Accuracy 0.3545 Loss 3.8733 Time elapsed 214.13s\n","Epoch 27 Batch 2600/4934 Accuracy 0.3546 Loss 3.8737 Time elapsed 222.69s\n","Epoch 27 Batch 2700/4934 Accuracy 0.3548 Loss 3.8735 Time elapsed 231.26s\n","Epoch 27 Batch 2800/4934 Accuracy 0.3551 Loss 3.8707 Time elapsed 239.82s\n","Epoch 27 Batch 2900/4934 Accuracy 0.3559 Loss 3.8628 Time elapsed 248.39s\n","Epoch 27 Batch 3000/4934 Accuracy 0.3569 Loss 3.8526 Time elapsed 256.95s\n","Epoch 27 Batch 3100/4934 Accuracy 0.3576 Loss 3.8453 Time elapsed 265.52s\n","Epoch 27 Batch 3200/4934 Accuracy 0.3578 Loss 3.8431 Time elapsed 274.08s\n","Epoch 27 Batch 3300/4934 Accuracy 0.3580 Loss 3.8408 Time elapsed 282.65s\n","Epoch 27 Batch 3400/4934 Accuracy 0.3578 Loss 3.8415 Time elapsed 291.21s\n","Epoch 27 Batch 3500/4934 Accuracy 0.3576 Loss 3.8421 Time elapsed 299.78s\n","Epoch 27 Batch 3600/4934 Accuracy 0.3573 Loss 3.8453 Time elapsed 308.35s\n","Epoch 27 Batch 3700/4934 Accuracy 0.3568 Loss 3.8495 Time elapsed 316.91s\n","Epoch 27 Batch 3800/4934 Accuracy 0.3563 Loss 3.8535 Time elapsed 325.48s\n","Epoch 27 Batch 3900/4934 Accuracy 0.3558 Loss 3.8581 Time elapsed 334.05s\n","Epoch 27 Batch 4000/4934 Accuracy 0.3553 Loss 3.8632 Time elapsed 342.61s\n","Epoch 27 Batch 4100/4934 Accuracy 0.3548 Loss 3.8678 Time elapsed 351.18s\n","Epoch 27 Batch 4200/4934 Accuracy 0.3544 Loss 3.8725 Time elapsed 359.75s\n","Epoch 27 Batch 4300/4934 Accuracy 0.3541 Loss 3.8752 Time elapsed 368.32s\n","Epoch 27 Batch 4400/4934 Accuracy 0.3539 Loss 3.8778 Time elapsed 376.89s\n","Epoch 27 Batch 4500/4934 Accuracy 0.3538 Loss 3.8805 Time elapsed 385.46s\n","Epoch 27 Batch 4600/4934 Accuracy 0.3535 Loss 3.8838 Time elapsed 394.02s\n","Epoch 27 Batch 4700/4934 Accuracy 0.3533 Loss 3.8861 Time elapsed 402.59s\n","Epoch 27 Batch 4800/4934 Accuracy 0.3532 Loss 3.8883 Time elapsed 411.16s\n","Epoch 27 Batch 4900/4934 Accuracy 0.3531 Loss 3.8902 Time elapsed 419.73s\n","Epoch 27 Batch 4934/4934 Accuracy 0.3531 Loss 3.8912 Time elapsed 422.65s\n","Epoch 27 Accuracy 0.3531 Loss 3.8912 Time 423.75 seconds\n","\n","Epoch 28 Batch 100/4934 Accuracy 0.3286 Loss 4.0992 Time elapsed 8.57s\n","Epoch 28 Batch 200/4934 Accuracy 0.3328 Loss 4.0674 Time elapsed 17.14s\n","Epoch 28 Batch 300/4934 Accuracy 0.3387 Loss 4.0249 Time elapsed 25.71s\n","Epoch 28 Batch 400/4934 Accuracy 0.3416 Loss 3.9945 Time elapsed 34.27s\n","Epoch 28 Batch 500/4934 Accuracy 0.3441 Loss 3.9692 Time elapsed 42.83s\n","Epoch 28 Batch 600/4934 Accuracy 0.3452 Loss 3.9527 Time elapsed 51.40s\n","Epoch 28 Batch 700/4934 Accuracy 0.3469 Loss 3.9334 Time elapsed 59.97s\n","Epoch 28 Batch 800/4934 Accuracy 0.3479 Loss 3.9235 Time elapsed 68.54s\n","Epoch 28 Batch 900/4934 Accuracy 0.3496 Loss 3.9076 Time elapsed 77.11s\n","Epoch 28 Batch 1000/4934 Accuracy 0.3506 Loss 3.8956 Time elapsed 85.67s\n","Epoch 28 Batch 1100/4934 Accuracy 0.3515 Loss 3.8861 Time elapsed 94.23s\n","Epoch 28 Batch 1200/4934 Accuracy 0.3527 Loss 3.8784 Time elapsed 102.80s\n","Epoch 28 Batch 1300/4934 Accuracy 0.3533 Loss 3.8711 Time elapsed 111.36s\n","Epoch 28 Batch 1400/4934 Accuracy 0.3544 Loss 3.8622 Time elapsed 119.93s\n","Epoch 28 Batch 1500/4934 Accuracy 0.3553 Loss 3.8556 Time elapsed 128.50s\n","Epoch 28 Batch 1600/4934 Accuracy 0.3559 Loss 3.8491 Time elapsed 137.06s\n","Epoch 28 Batch 1700/4934 Accuracy 0.3564 Loss 3.8458 Time elapsed 145.63s\n","Epoch 28 Batch 1800/4934 Accuracy 0.3569 Loss 3.8427 Time elapsed 154.20s\n","Epoch 28 Batch 1900/4934 Accuracy 0.3574 Loss 3.8410 Time elapsed 162.76s\n","Epoch 28 Batch 2000/4934 Accuracy 0.3577 Loss 3.8395 Time elapsed 171.32s\n","Epoch 28 Batch 2100/4934 Accuracy 0.3581 Loss 3.8379 Time elapsed 179.89s\n","Epoch 28 Batch 2200/4934 Accuracy 0.3584 Loss 3.8353 Time elapsed 188.46s\n","Epoch 28 Batch 2300/4934 Accuracy 0.3587 Loss 3.8332 Time elapsed 197.03s\n","Epoch 28 Batch 2400/4934 Accuracy 0.3591 Loss 3.8307 Time elapsed 205.59s\n","Epoch 28 Batch 2500/4934 Accuracy 0.3593 Loss 3.8295 Time elapsed 214.15s\n","Epoch 28 Batch 2600/4934 Accuracy 0.3593 Loss 3.8298 Time elapsed 222.72s\n","Epoch 28 Batch 2700/4934 Accuracy 0.3595 Loss 3.8298 Time elapsed 231.29s\n","Epoch 28 Batch 2800/4934 Accuracy 0.3599 Loss 3.8266 Time elapsed 239.86s\n","Epoch 28 Batch 2900/4934 Accuracy 0.3608 Loss 3.8184 Time elapsed 248.41s\n","Epoch 28 Batch 3000/4934 Accuracy 0.3620 Loss 3.8077 Time elapsed 256.96s\n","Epoch 28 Batch 3100/4934 Accuracy 0.3628 Loss 3.7997 Time elapsed 265.52s\n","Epoch 28 Batch 3200/4934 Accuracy 0.3631 Loss 3.7973 Time elapsed 274.07s\n","Epoch 28 Batch 3300/4934 Accuracy 0.3633 Loss 3.7948 Time elapsed 282.63s\n","Epoch 28 Batch 3400/4934 Accuracy 0.3632 Loss 3.7954 Time elapsed 291.20s\n","Epoch 28 Batch 3500/4934 Accuracy 0.3631 Loss 3.7958 Time elapsed 299.76s\n","Epoch 28 Batch 3600/4934 Accuracy 0.3627 Loss 3.7992 Time elapsed 308.32s\n","Epoch 28 Batch 3700/4934 Accuracy 0.3622 Loss 3.8036 Time elapsed 316.89s\n","Epoch 28 Batch 3800/4934 Accuracy 0.3617 Loss 3.8074 Time elapsed 325.46s\n","Epoch 28 Batch 3900/4934 Accuracy 0.3612 Loss 3.8122 Time elapsed 334.02s\n","Epoch 28 Batch 4000/4934 Accuracy 0.3607 Loss 3.8173 Time elapsed 342.59s\n","Epoch 28 Batch 4100/4934 Accuracy 0.3603 Loss 3.8218 Time elapsed 351.16s\n","Epoch 28 Batch 4200/4934 Accuracy 0.3599 Loss 3.8266 Time elapsed 359.73s\n","Epoch 28 Batch 4300/4934 Accuracy 0.3596 Loss 3.8292 Time elapsed 368.29s\n","Epoch 28 Batch 4400/4934 Accuracy 0.3595 Loss 3.8318 Time elapsed 376.86s\n","Epoch 28 Batch 4500/4934 Accuracy 0.3594 Loss 3.8345 Time elapsed 385.43s\n","Epoch 28 Batch 4600/4934 Accuracy 0.3591 Loss 3.8379 Time elapsed 394.00s\n","Epoch 28 Batch 4700/4934 Accuracy 0.3590 Loss 3.8399 Time elapsed 402.56s\n","Epoch 28 Batch 4800/4934 Accuracy 0.3589 Loss 3.8419 Time elapsed 411.13s\n","Epoch 28 Batch 4900/4934 Accuracy 0.3589 Loss 3.8441 Time elapsed 419.70s\n","Epoch 28 Batch 4934/4934 Accuracy 0.3588 Loss 3.8450 Time elapsed 422.62s\n","Epoch 28 Accuracy 0.3588 Loss 3.8450 Time 423.75 seconds\n","\n","Epoch 29 Batch 100/4934 Accuracy 0.3330 Loss 4.0705 Time elapsed 8.57s\n","Epoch 29 Batch 200/4934 Accuracy 0.3365 Loss 4.0306 Time elapsed 17.13s\n","Epoch 29 Batch 300/4934 Accuracy 0.3429 Loss 3.9875 Time elapsed 25.70s\n","Epoch 29 Batch 400/4934 Accuracy 0.3467 Loss 3.9527 Time elapsed 34.26s\n","Epoch 29 Batch 500/4934 Accuracy 0.3496 Loss 3.9268 Time elapsed 42.83s\n","Epoch 29 Batch 600/4934 Accuracy 0.3507 Loss 3.9122 Time elapsed 51.39s\n","Epoch 29 Batch 700/4934 Accuracy 0.3521 Loss 3.8941 Time elapsed 59.95s\n","Epoch 29 Batch 800/4934 Accuracy 0.3529 Loss 3.8840 Time elapsed 68.52s\n","Epoch 29 Batch 900/4934 Accuracy 0.3548 Loss 3.8675 Time elapsed 77.08s\n","Epoch 29 Batch 1000/4934 Accuracy 0.3559 Loss 3.8558 Time elapsed 85.65s\n","Epoch 29 Batch 1100/4934 Accuracy 0.3566 Loss 3.8466 Time elapsed 94.22s\n","Epoch 29 Batch 1200/4934 Accuracy 0.3576 Loss 3.8388 Time elapsed 102.78s\n","Epoch 29 Batch 1300/4934 Accuracy 0.3584 Loss 3.8310 Time elapsed 111.35s\n","Epoch 29 Batch 1400/4934 Accuracy 0.3595 Loss 3.8222 Time elapsed 119.91s\n","Epoch 29 Batch 1500/4934 Accuracy 0.3604 Loss 3.8158 Time elapsed 128.48s\n","Epoch 29 Batch 1600/4934 Accuracy 0.3612 Loss 3.8091 Time elapsed 137.04s\n","Epoch 29 Batch 1700/4934 Accuracy 0.3617 Loss 3.8060 Time elapsed 145.60s\n","Epoch 29 Batch 1800/4934 Accuracy 0.3623 Loss 3.8027 Time elapsed 154.17s\n","Epoch 29 Batch 1900/4934 Accuracy 0.3627 Loss 3.8005 Time elapsed 162.73s\n","Epoch 29 Batch 2000/4934 Accuracy 0.3630 Loss 3.7992 Time elapsed 171.30s\n","Epoch 29 Batch 2100/4934 Accuracy 0.3634 Loss 3.7977 Time elapsed 179.86s\n","Epoch 29 Batch 2200/4934 Accuracy 0.3639 Loss 3.7951 Time elapsed 188.43s\n","Epoch 29 Batch 2300/4934 Accuracy 0.3642 Loss 3.7932 Time elapsed 196.99s\n","Epoch 29 Batch 2400/4934 Accuracy 0.3646 Loss 3.7905 Time elapsed 205.56s\n","Epoch 29 Batch 2500/4934 Accuracy 0.3648 Loss 3.7891 Time elapsed 214.12s\n","Epoch 29 Batch 2600/4934 Accuracy 0.3649 Loss 3.7893 Time elapsed 222.68s\n","Epoch 29 Batch 2700/4934 Accuracy 0.3652 Loss 3.7885 Time elapsed 231.25s\n","Epoch 29 Batch 2800/4934 Accuracy 0.3656 Loss 3.7850 Time elapsed 239.81s\n","Epoch 29 Batch 2900/4934 Accuracy 0.3666 Loss 3.7766 Time elapsed 248.37s\n","Epoch 29 Batch 3000/4934 Accuracy 0.3678 Loss 3.7656 Time elapsed 256.94s\n","Epoch 29 Batch 3100/4934 Accuracy 0.3686 Loss 3.7574 Time elapsed 265.50s\n","Epoch 29 Batch 3200/4934 Accuracy 0.3689 Loss 3.7546 Time elapsed 274.07s\n","Epoch 29 Batch 3300/4934 Accuracy 0.3692 Loss 3.7518 Time elapsed 282.63s\n","Epoch 29 Batch 3400/4934 Accuracy 0.3689 Loss 3.7525 Time elapsed 291.20s\n","Epoch 29 Batch 3500/4934 Accuracy 0.3688 Loss 3.7532 Time elapsed 299.76s\n","Epoch 29 Batch 3600/4934 Accuracy 0.3685 Loss 3.7563 Time elapsed 308.33s\n","Epoch 29 Batch 3700/4934 Accuracy 0.3680 Loss 3.7608 Time elapsed 316.89s\n","Epoch 29 Batch 3800/4934 Accuracy 0.3675 Loss 3.7650 Time elapsed 325.46s\n","Epoch 29 Batch 3900/4934 Accuracy 0.3670 Loss 3.7700 Time elapsed 334.02s\n","Epoch 29 Batch 4000/4934 Accuracy 0.3665 Loss 3.7750 Time elapsed 342.59s\n","Epoch 29 Batch 4100/4934 Accuracy 0.3660 Loss 3.7801 Time elapsed 351.15s\n","Epoch 29 Batch 4200/4934 Accuracy 0.3655 Loss 3.7850 Time elapsed 359.72s\n","Epoch 29 Batch 4300/4934 Accuracy 0.3652 Loss 3.7878 Time elapsed 368.29s\n","Epoch 29 Batch 4400/4934 Accuracy 0.3650 Loss 3.7907 Time elapsed 376.86s\n","Epoch 29 Batch 4500/4934 Accuracy 0.3648 Loss 3.7933 Time elapsed 385.42s\n","Epoch 29 Batch 4600/4934 Accuracy 0.3646 Loss 3.7964 Time elapsed 393.99s\n","Epoch 29 Batch 4700/4934 Accuracy 0.3644 Loss 3.7987 Time elapsed 402.56s\n","Epoch 29 Batch 4800/4934 Accuracy 0.3644 Loss 3.8007 Time elapsed 411.12s\n","Epoch 29 Batch 4900/4934 Accuracy 0.3643 Loss 3.8028 Time elapsed 419.69s\n","Epoch 29 Batch 4934/4934 Accuracy 0.3643 Loss 3.8037 Time elapsed 422.61s\n","Epoch 29 Accuracy 0.3643 Loss 3.8037 Time 423.72 seconds\n","\n","Epoch 30 Batch 100/4934 Accuracy 0.3385 Loss 4.0379 Time elapsed 8.56s\n","Epoch 30 Batch 200/4934 Accuracy 0.3432 Loss 3.9979 Time elapsed 17.12s\n","Epoch 30 Batch 300/4934 Accuracy 0.3491 Loss 3.9540 Time elapsed 25.69s\n","Epoch 30 Batch 400/4934 Accuracy 0.3522 Loss 3.9186 Time elapsed 34.26s\n","Epoch 30 Batch 500/4934 Accuracy 0.3548 Loss 3.8931 Time elapsed 42.82s\n","Epoch 30 Batch 600/4934 Accuracy 0.3558 Loss 3.8774 Time elapsed 51.38s\n","Epoch 30 Batch 700/4934 Accuracy 0.3573 Loss 3.8589 Time elapsed 59.95s\n","Epoch 30 Batch 800/4934 Accuracy 0.3579 Loss 3.8502 Time elapsed 68.51s\n","Epoch 30 Batch 900/4934 Accuracy 0.3594 Loss 3.8345 Time elapsed 77.08s\n","Epoch 30 Batch 1000/4934 Accuracy 0.3602 Loss 3.8226 Time elapsed 85.65s\n","Epoch 30 Batch 1100/4934 Accuracy 0.3613 Loss 3.8134 Time elapsed 94.22s\n","Epoch 30 Batch 1200/4934 Accuracy 0.3625 Loss 3.8048 Time elapsed 102.79s\n","Epoch 30 Batch 1300/4934 Accuracy 0.3632 Loss 3.7974 Time elapsed 111.35s\n","Epoch 30 Batch 1400/4934 Accuracy 0.3643 Loss 3.7886 Time elapsed 119.92s\n","Epoch 30 Batch 1500/4934 Accuracy 0.3652 Loss 3.7816 Time elapsed 128.49s\n","Epoch 30 Batch 1600/4934 Accuracy 0.3659 Loss 3.7758 Time elapsed 137.06s\n","Epoch 30 Batch 1700/4934 Accuracy 0.3665 Loss 3.7719 Time elapsed 145.63s\n","Epoch 30 Batch 1800/4934 Accuracy 0.3671 Loss 3.7686 Time elapsed 154.19s\n","Epoch 30 Batch 1900/4934 Accuracy 0.3676 Loss 3.7668 Time elapsed 162.76s\n","Epoch 30 Batch 2000/4934 Accuracy 0.3681 Loss 3.7650 Time elapsed 171.33s\n","Epoch 30 Batch 2100/4934 Accuracy 0.3684 Loss 3.7635 Time elapsed 179.90s\n","Epoch 30 Batch 2200/4934 Accuracy 0.3688 Loss 3.7607 Time elapsed 188.46s\n","Epoch 30 Batch 2300/4934 Accuracy 0.3692 Loss 3.7584 Time elapsed 197.03s\n","Epoch 30 Batch 2400/4934 Accuracy 0.3695 Loss 3.7562 Time elapsed 205.61s\n","Epoch 30 Batch 2500/4934 Accuracy 0.3698 Loss 3.7546 Time elapsed 214.18s\n","Epoch 30 Batch 2600/4934 Accuracy 0.3698 Loss 3.7549 Time elapsed 222.75s\n","Epoch 30 Batch 2700/4934 Accuracy 0.3699 Loss 3.7547 Time elapsed 231.32s\n","Epoch 30 Batch 2800/4934 Accuracy 0.3705 Loss 3.7509 Time elapsed 239.89s\n","Epoch 30 Batch 2900/4934 Accuracy 0.3713 Loss 3.7421 Time elapsed 248.46s\n","Epoch 30 Batch 3000/4934 Accuracy 0.3726 Loss 3.7305 Time elapsed 257.03s\n","Epoch 30 Batch 3100/4934 Accuracy 0.3735 Loss 3.7221 Time elapsed 265.60s\n","Epoch 30 Batch 3200/4934 Accuracy 0.3739 Loss 3.7190 Time elapsed 274.17s\n","Epoch 30 Batch 3300/4934 Accuracy 0.3741 Loss 3.7162 Time elapsed 282.74s\n","Epoch 30 Batch 3400/4934 Accuracy 0.3739 Loss 3.7165 Time elapsed 291.31s\n","Epoch 30 Batch 3500/4934 Accuracy 0.3739 Loss 3.7170 Time elapsed 299.88s\n","Epoch 30 Batch 3600/4934 Accuracy 0.3735 Loss 3.7201 Time elapsed 308.45s\n","Epoch 30 Batch 3700/4934 Accuracy 0.3729 Loss 3.7246 Time elapsed 317.02s\n","Epoch 30 Batch 3800/4934 Accuracy 0.3724 Loss 3.7287 Time elapsed 325.59s\n","Epoch 30 Batch 3900/4934 Accuracy 0.3719 Loss 3.7337 Time elapsed 334.17s\n","Epoch 30 Batch 4000/4934 Accuracy 0.3714 Loss 3.7390 Time elapsed 342.73s\n","Epoch 30 Batch 4100/4934 Accuracy 0.3709 Loss 3.7439 Time elapsed 351.30s\n","Epoch 30 Batch 4200/4934 Accuracy 0.3705 Loss 3.7490 Time elapsed 359.87s\n","Epoch 30 Batch 4300/4934 Accuracy 0.3702 Loss 3.7518 Time elapsed 368.44s\n","Epoch 30 Batch 4400/4934 Accuracy 0.3700 Loss 3.7547 Time elapsed 377.01s\n","Epoch 30 Batch 4500/4934 Accuracy 0.3698 Loss 3.7575 Time elapsed 385.58s\n","Epoch 30 Batch 4600/4934 Accuracy 0.3696 Loss 3.7607 Time elapsed 394.15s\n","Epoch 30 Batch 4700/4934 Accuracy 0.3693 Loss 3.7628 Time elapsed 402.72s\n","Epoch 30 Batch 4800/4934 Accuracy 0.3692 Loss 3.7650 Time elapsed 411.29s\n","Epoch 30 Batch 4900/4934 Accuracy 0.3692 Loss 3.7670 Time elapsed 419.86s\n","Epoch 30 Batch 4934/4934 Accuracy 0.3692 Loss 3.7677 Time elapsed 422.78s\n","Epoch 30 Accuracy 0.3692 Loss 3.7677 Time 423.91 seconds\n","\n","Epoch 31 Batch 100/4934 Accuracy 0.3399 Loss 4.0094 Time elapsed 8.56s\n","Epoch 31 Batch 200/4934 Accuracy 0.3459 Loss 3.9673 Time elapsed 17.13s\n","Epoch 31 Batch 300/4934 Accuracy 0.3519 Loss 3.9267 Time elapsed 25.70s\n","Epoch 31 Batch 400/4934 Accuracy 0.3560 Loss 3.8906 Time elapsed 34.27s\n","Epoch 31 Batch 500/4934 Accuracy 0.3586 Loss 3.8666 Time elapsed 42.84s\n","Epoch 31 Batch 600/4934 Accuracy 0.3598 Loss 3.8496 Time elapsed 51.41s\n","Epoch 31 Batch 700/4934 Accuracy 0.3613 Loss 3.8320 Time elapsed 59.98s\n","Epoch 31 Batch 800/4934 Accuracy 0.3622 Loss 3.8213 Time elapsed 68.55s\n","Epoch 31 Batch 900/4934 Accuracy 0.3638 Loss 3.8052 Time elapsed 77.11s\n","Epoch 31 Batch 1000/4934 Accuracy 0.3651 Loss 3.7919 Time elapsed 85.68s\n","Epoch 31 Batch 1100/4934 Accuracy 0.3660 Loss 3.7829 Time elapsed 94.25s\n","Epoch 31 Batch 1200/4934 Accuracy 0.3670 Loss 3.7751 Time elapsed 102.82s\n","Epoch 31 Batch 1300/4934 Accuracy 0.3677 Loss 3.7674 Time elapsed 111.38s\n","Epoch 31 Batch 1400/4934 Accuracy 0.3688 Loss 3.7582 Time elapsed 119.95s\n","Epoch 31 Batch 1500/4934 Accuracy 0.3697 Loss 3.7516 Time elapsed 128.51s\n","Epoch 31 Batch 1600/4934 Accuracy 0.3704 Loss 3.7450 Time elapsed 137.09s\n","Epoch 31 Batch 1700/4934 Accuracy 0.3710 Loss 3.7417 Time elapsed 145.66s\n","Epoch 31 Batch 1800/4934 Accuracy 0.3715 Loss 3.7385 Time elapsed 154.22s\n","Epoch 31 Batch 1900/4934 Accuracy 0.3720 Loss 3.7367 Time elapsed 162.79s\n","Epoch 31 Batch 2000/4934 Accuracy 0.3723 Loss 3.7355 Time elapsed 171.36s\n","Epoch 31 Batch 2100/4934 Accuracy 0.3725 Loss 3.7342 Time elapsed 179.93s\n","Epoch 31 Batch 2200/4934 Accuracy 0.3730 Loss 3.7314 Time elapsed 188.49s\n","Epoch 31 Batch 2300/4934 Accuracy 0.3733 Loss 3.7295 Time elapsed 197.06s\n","Epoch 31 Batch 2400/4934 Accuracy 0.3738 Loss 3.7270 Time elapsed 205.62s\n","Epoch 31 Batch 2500/4934 Accuracy 0.3740 Loss 3.7254 Time elapsed 214.19s\n","Epoch 31 Batch 2600/4934 Accuracy 0.3739 Loss 3.7257 Time elapsed 222.76s\n","Epoch 31 Batch 2700/4934 Accuracy 0.3741 Loss 3.7255 Time elapsed 231.33s\n","Epoch 31 Batch 2800/4934 Accuracy 0.3746 Loss 3.7220 Time elapsed 239.90s\n","Epoch 31 Batch 2900/4934 Accuracy 0.3756 Loss 3.7131 Time elapsed 248.47s\n","Epoch 31 Batch 3000/4934 Accuracy 0.3770 Loss 3.7011 Time elapsed 257.02s\n","Epoch 31 Batch 3100/4934 Accuracy 0.3779 Loss 3.6923 Time elapsed 265.59s\n","Epoch 31 Batch 3200/4934 Accuracy 0.3782 Loss 3.6892 Time elapsed 274.15s\n","Epoch 31 Batch 3300/4934 Accuracy 0.3785 Loss 3.6863 Time elapsed 282.71s\n","Epoch 31 Batch 3400/4934 Accuracy 0.3784 Loss 3.6869 Time elapsed 291.27s\n","Epoch 31 Batch 3500/4934 Accuracy 0.3783 Loss 3.6871 Time elapsed 299.84s\n","Epoch 31 Batch 3600/4934 Accuracy 0.3780 Loss 3.6903 Time elapsed 308.40s\n","Epoch 31 Batch 3700/4934 Accuracy 0.3774 Loss 3.6949 Time elapsed 316.96s\n","Epoch 31 Batch 3800/4934 Accuracy 0.3769 Loss 3.6990 Time elapsed 325.53s\n","Epoch 31 Batch 3900/4934 Accuracy 0.3764 Loss 3.7039 Time elapsed 334.09s\n","Epoch 31 Batch 4000/4934 Accuracy 0.3759 Loss 3.7090 Time elapsed 342.66s\n","Epoch 31 Batch 4100/4934 Accuracy 0.3755 Loss 3.7137 Time elapsed 351.23s\n","Epoch 31 Batch 4200/4934 Accuracy 0.3751 Loss 3.7186 Time elapsed 359.80s\n","Epoch 31 Batch 4300/4934 Accuracy 0.3748 Loss 3.7217 Time elapsed 368.37s\n","Epoch 31 Batch 4400/4934 Accuracy 0.3745 Loss 3.7248 Time elapsed 376.99s\n","Epoch 31 Batch 4500/4934 Accuracy 0.3743 Loss 3.7276 Time elapsed 385.56s\n","Epoch 31 Batch 4600/4934 Accuracy 0.3740 Loss 3.7310 Time elapsed 394.13s\n","Epoch 31 Batch 4700/4934 Accuracy 0.3738 Loss 3.7335 Time elapsed 402.70s\n","Epoch 31 Batch 4800/4934 Accuracy 0.3737 Loss 3.7356 Time elapsed 411.27s\n","Epoch 31 Batch 4900/4934 Accuracy 0.3737 Loss 3.7375 Time elapsed 419.84s\n","Epoch 31 Batch 4934/4934 Accuracy 0.3736 Loss 3.7382 Time elapsed 422.76s\n","Epoch 31 Accuracy 0.3736 Loss 3.7382 Time 423.94 seconds\n","\n","Epoch 32 Batch 100/4934 Accuracy 0.3468 Loss 3.9780 Time elapsed 8.57s\n","Epoch 32 Batch 200/4934 Accuracy 0.3503 Loss 3.9350 Time elapsed 17.14s\n","Epoch 32 Batch 300/4934 Accuracy 0.3557 Loss 3.8964 Time elapsed 25.71s\n","Epoch 32 Batch 400/4934 Accuracy 0.3593 Loss 3.8632 Time elapsed 34.29s\n","Epoch 32 Batch 500/4934 Accuracy 0.3621 Loss 3.8373 Time elapsed 42.86s\n","Epoch 32 Batch 600/4934 Accuracy 0.3634 Loss 3.8205 Time elapsed 51.43s\n","Epoch 32 Batch 700/4934 Accuracy 0.3648 Loss 3.8019 Time elapsed 59.99s\n","Epoch 32 Batch 800/4934 Accuracy 0.3656 Loss 3.7932 Time elapsed 68.55s\n","Epoch 32 Batch 900/4934 Accuracy 0.3671 Loss 3.7767 Time elapsed 77.12s\n","Epoch 32 Batch 1000/4934 Accuracy 0.3683 Loss 3.7644 Time elapsed 85.68s\n","Epoch 32 Batch 1100/4934 Accuracy 0.3691 Loss 3.7550 Time elapsed 94.25s\n","Epoch 32 Batch 1200/4934 Accuracy 0.3703 Loss 3.7471 Time elapsed 102.82s\n","Epoch 32 Batch 1300/4934 Accuracy 0.3711 Loss 3.7400 Time elapsed 111.39s\n","Epoch 32 Batch 1400/4934 Accuracy 0.3723 Loss 3.7317 Time elapsed 119.97s\n","Epoch 32 Batch 1500/4934 Accuracy 0.3731 Loss 3.7255 Time elapsed 128.53s\n","Epoch 32 Batch 1600/4934 Accuracy 0.3740 Loss 3.7193 Time elapsed 137.10s\n","Epoch 32 Batch 1700/4934 Accuracy 0.3744 Loss 3.7161 Time elapsed 145.67s\n","Epoch 32 Batch 1800/4934 Accuracy 0.3750 Loss 3.7130 Time elapsed 154.24s\n","Epoch 32 Batch 1900/4934 Accuracy 0.3754 Loss 3.7109 Time elapsed 162.81s\n","Epoch 32 Batch 2000/4934 Accuracy 0.3759 Loss 3.7093 Time elapsed 171.37s\n","Epoch 32 Batch 2100/4934 Accuracy 0.3762 Loss 3.7080 Time elapsed 179.94s\n","Epoch 32 Batch 2200/4934 Accuracy 0.3767 Loss 3.7053 Time elapsed 188.50s\n","Epoch 32 Batch 2300/4934 Accuracy 0.3771 Loss 3.7033 Time elapsed 197.06s\n","Epoch 32 Batch 2400/4934 Accuracy 0.3775 Loss 3.7012 Time elapsed 205.63s\n","Epoch 32 Batch 2500/4934 Accuracy 0.3776 Loss 3.6994 Time elapsed 214.19s\n","Epoch 32 Batch 2600/4934 Accuracy 0.3775 Loss 3.7000 Time elapsed 222.76s\n","Epoch 32 Batch 2700/4934 Accuracy 0.3777 Loss 3.6994 Time elapsed 231.33s\n","Epoch 32 Batch 2800/4934 Accuracy 0.3783 Loss 3.6959 Time elapsed 239.90s\n","Epoch 32 Batch 2900/4934 Accuracy 0.3794 Loss 3.6866 Time elapsed 248.46s\n","Epoch 32 Batch 3000/4934 Accuracy 0.3808 Loss 3.6743 Time elapsed 257.02s\n","Epoch 32 Batch 3100/4934 Accuracy 0.3817 Loss 3.6653 Time elapsed 265.58s\n","Epoch 32 Batch 3200/4934 Accuracy 0.3820 Loss 3.6623 Time elapsed 274.15s\n","Epoch 32 Batch 3300/4934 Accuracy 0.3822 Loss 3.6597 Time elapsed 282.71s\n","Epoch 32 Batch 3400/4934 Accuracy 0.3821 Loss 3.6599 Time elapsed 291.26s\n","Epoch 32 Batch 3500/4934 Accuracy 0.3821 Loss 3.6600 Time elapsed 299.82s\n","Epoch 32 Batch 3600/4934 Accuracy 0.3817 Loss 3.6632 Time elapsed 308.37s\n","Epoch 32 Batch 3700/4934 Accuracy 0.3812 Loss 3.6677 Time elapsed 316.93s\n","Epoch 32 Batch 3800/4934 Accuracy 0.3807 Loss 3.6720 Time elapsed 325.50s\n","Epoch 32 Batch 3900/4934 Accuracy 0.3802 Loss 3.6771 Time elapsed 334.07s\n","Epoch 32 Batch 4000/4934 Accuracy 0.3796 Loss 3.6825 Time elapsed 342.63s\n","Epoch 32 Batch 4100/4934 Accuracy 0.3792 Loss 3.6878 Time elapsed 351.20s\n","Epoch 32 Batch 4200/4934 Accuracy 0.3787 Loss 3.6929 Time elapsed 359.77s\n","Epoch 32 Batch 4300/4934 Accuracy 0.3784 Loss 3.6963 Time elapsed 368.33s\n","Epoch 32 Batch 4400/4934 Accuracy 0.3782 Loss 3.6993 Time elapsed 376.90s\n","Epoch 32 Batch 4500/4934 Accuracy 0.3781 Loss 3.7022 Time elapsed 385.46s\n","Epoch 32 Batch 4600/4934 Accuracy 0.3778 Loss 3.7055 Time elapsed 394.02s\n","Epoch 32 Batch 4700/4934 Accuracy 0.3777 Loss 3.7078 Time elapsed 402.59s\n","Epoch 32 Batch 4800/4934 Accuracy 0.3776 Loss 3.7097 Time elapsed 411.16s\n","Epoch 32 Batch 4900/4934 Accuracy 0.3776 Loss 3.7119 Time elapsed 419.73s\n","Epoch 32 Batch 4934/4934 Accuracy 0.3775 Loss 3.7128 Time elapsed 422.64s\n","Epoch 32 Accuracy 0.3775 Loss 3.7128 Time 423.76 seconds\n","\n","Epoch 33 Batch 100/4934 Accuracy 0.3474 Loss 3.9642 Time elapsed 8.56s\n","Epoch 33 Batch 200/4934 Accuracy 0.3521 Loss 3.9265 Time elapsed 17.13s\n","Epoch 33 Batch 300/4934 Accuracy 0.3581 Loss 3.8845 Time elapsed 25.69s\n","Epoch 33 Batch 400/4934 Accuracy 0.3626 Loss 3.8499 Time elapsed 34.26s\n","Epoch 33 Batch 500/4934 Accuracy 0.3654 Loss 3.8235 Time elapsed 42.83s\n","Epoch 33 Batch 600/4934 Accuracy 0.3665 Loss 3.8068 Time elapsed 51.40s\n","Epoch 33 Batch 700/4934 Accuracy 0.3682 Loss 3.7870 Time elapsed 59.96s\n","Epoch 33 Batch 800/4934 Accuracy 0.3691 Loss 3.7774 Time elapsed 68.53s\n","Epoch 33 Batch 900/4934 Accuracy 0.3709 Loss 3.7614 Time elapsed 77.09s\n","Epoch 33 Batch 1000/4934 Accuracy 0.3718 Loss 3.7488 Time elapsed 85.65s\n","Epoch 33 Batch 1100/4934 Accuracy 0.3726 Loss 3.7393 Time elapsed 94.21s\n","Epoch 33 Batch 1200/4934 Accuracy 0.3736 Loss 3.7310 Time elapsed 102.78s\n","Epoch 33 Batch 1300/4934 Accuracy 0.3744 Loss 3.7230 Time elapsed 111.35s\n","Epoch 33 Batch 1400/4934 Accuracy 0.3756 Loss 3.7152 Time elapsed 119.91s\n","Epoch 33 Batch 1500/4934 Accuracy 0.3764 Loss 3.7081 Time elapsed 128.48s\n","Epoch 33 Batch 1600/4934 Accuracy 0.3771 Loss 3.7016 Time elapsed 137.05s\n","Epoch 33 Batch 1700/4934 Accuracy 0.3777 Loss 3.6981 Time elapsed 145.62s\n","Epoch 33 Batch 1800/4934 Accuracy 0.3784 Loss 3.6948 Time elapsed 154.18s\n","Epoch 33 Batch 1900/4934 Accuracy 0.3790 Loss 3.6926 Time elapsed 162.74s\n","Epoch 33 Batch 2000/4934 Accuracy 0.3793 Loss 3.6916 Time elapsed 171.31s\n","Epoch 33 Batch 2100/4934 Accuracy 0.3797 Loss 3.6903 Time elapsed 179.88s\n","Epoch 33 Batch 2200/4934 Accuracy 0.3801 Loss 3.6877 Time elapsed 188.44s\n","Epoch 33 Batch 2300/4934 Accuracy 0.3805 Loss 3.6861 Time elapsed 197.01s\n","Epoch 33 Batch 2400/4934 Accuracy 0.3809 Loss 3.6834 Time elapsed 205.57s\n","Epoch 33 Batch 2500/4934 Accuracy 0.3812 Loss 3.6818 Time elapsed 214.14s\n","Epoch 33 Batch 2600/4934 Accuracy 0.3813 Loss 3.6824 Time elapsed 222.70s\n","Epoch 33 Batch 2700/4934 Accuracy 0.3815 Loss 3.6822 Time elapsed 231.27s\n","Epoch 33 Batch 2800/4934 Accuracy 0.3820 Loss 3.6784 Time elapsed 239.83s\n","Epoch 33 Batch 2900/4934 Accuracy 0.3831 Loss 3.6692 Time elapsed 248.39s\n","Epoch 33 Batch 3000/4934 Accuracy 0.3844 Loss 3.6566 Time elapsed 256.95s\n","Epoch 33 Batch 3100/4934 Accuracy 0.3853 Loss 3.6474 Time elapsed 265.50s\n","Epoch 33 Batch 3200/4934 Accuracy 0.3858 Loss 3.6438 Time elapsed 274.06s\n","Epoch 33 Batch 3300/4934 Accuracy 0.3861 Loss 3.6407 Time elapsed 282.62s\n","Epoch 33 Batch 3400/4934 Accuracy 0.3860 Loss 3.6409 Time elapsed 291.18s\n","Epoch 33 Batch 3500/4934 Accuracy 0.3859 Loss 3.6411 Time elapsed 299.75s\n","Epoch 33 Batch 3600/4934 Accuracy 0.3857 Loss 3.6447 Time elapsed 308.32s\n","Epoch 33 Batch 3700/4934 Accuracy 0.3852 Loss 3.6491 Time elapsed 316.88s\n","Epoch 33 Batch 3800/4934 Accuracy 0.3848 Loss 3.6531 Time elapsed 325.45s\n","Epoch 33 Batch 3900/4934 Accuracy 0.3842 Loss 3.6584 Time elapsed 334.02s\n","Epoch 33 Batch 4000/4934 Accuracy 0.3837 Loss 3.6638 Time elapsed 342.59s\n","Epoch 33 Batch 4100/4934 Accuracy 0.3833 Loss 3.6691 Time elapsed 351.15s\n","Epoch 33 Batch 4200/4934 Accuracy 0.3828 Loss 3.6741 Time elapsed 359.72s\n","Epoch 33 Batch 4300/4934 Accuracy 0.3825 Loss 3.6773 Time elapsed 368.29s\n","Epoch 33 Batch 4400/4934 Accuracy 0.3822 Loss 3.6806 Time elapsed 376.86s\n","Epoch 33 Batch 4500/4934 Accuracy 0.3821 Loss 3.6836 Time elapsed 385.42s\n","Epoch 33 Batch 4600/4934 Accuracy 0.3818 Loss 3.6872 Time elapsed 393.99s\n","Epoch 33 Batch 4700/4934 Accuracy 0.3816 Loss 3.6897 Time elapsed 402.55s\n","Epoch 33 Batch 4800/4934 Accuracy 0.3815 Loss 3.6917 Time elapsed 411.12s\n","Epoch 33 Batch 4900/4934 Accuracy 0.3815 Loss 3.6937 Time elapsed 419.69s\n","Epoch 33 Batch 4934/4934 Accuracy 0.3815 Loss 3.6945 Time elapsed 422.61s\n","Epoch 33 Accuracy 0.3815 Loss 3.6945 Time 423.76 seconds\n","\n","Epoch 34 Batch 100/4934 Accuracy 0.3523 Loss 3.9529 Time elapsed 8.56s\n","Epoch 34 Batch 200/4934 Accuracy 0.3558 Loss 3.9127 Time elapsed 17.13s\n","Epoch 34 Batch 300/4934 Accuracy 0.3617 Loss 3.8729 Time elapsed 25.69s\n","Epoch 34 Batch 400/4934 Accuracy 0.3650 Loss 3.8393 Time elapsed 34.26s\n","Epoch 34 Batch 500/4934 Accuracy 0.3681 Loss 3.8139 Time elapsed 42.83s\n","Epoch 34 Batch 600/4934 Accuracy 0.3693 Loss 3.7975 Time elapsed 51.39s\n","Epoch 34 Batch 700/4934 Accuracy 0.3712 Loss 3.7783 Time elapsed 59.95s\n","Epoch 34 Batch 800/4934 Accuracy 0.3720 Loss 3.7679 Time elapsed 68.52s\n","Epoch 34 Batch 900/4934 Accuracy 0.3738 Loss 3.7529 Time elapsed 77.09s\n","Epoch 34 Batch 1000/4934 Accuracy 0.3751 Loss 3.7393 Time elapsed 85.65s\n","Epoch 34 Batch 1100/4934 Accuracy 0.3758 Loss 3.7291 Time elapsed 94.22s\n","Epoch 34 Batch 1200/4934 Accuracy 0.3770 Loss 3.7204 Time elapsed 102.78s\n","Epoch 34 Batch 1300/4934 Accuracy 0.3776 Loss 3.7129 Time elapsed 111.35s\n","Epoch 34 Batch 1400/4934 Accuracy 0.3787 Loss 3.7045 Time elapsed 119.91s\n","Epoch 34 Batch 1500/4934 Accuracy 0.3795 Loss 3.6973 Time elapsed 128.48s\n","Epoch 34 Batch 1600/4934 Accuracy 0.3802 Loss 3.6910 Time elapsed 137.05s\n","Epoch 34 Batch 1700/4934 Accuracy 0.3808 Loss 3.6881 Time elapsed 145.61s\n","Epoch 34 Batch 1800/4934 Accuracy 0.3812 Loss 3.6852 Time elapsed 154.17s\n","Epoch 34 Batch 1900/4934 Accuracy 0.3818 Loss 3.6834 Time elapsed 162.74s\n","Epoch 34 Batch 2000/4934 Accuracy 0.3822 Loss 3.6822 Time elapsed 171.30s\n","Epoch 34 Batch 2100/4934 Accuracy 0.3826 Loss 3.6806 Time elapsed 179.87s\n","Epoch 34 Batch 2200/4934 Accuracy 0.3831 Loss 3.6780 Time elapsed 188.44s\n","Epoch 34 Batch 2300/4934 Accuracy 0.3834 Loss 3.6761 Time elapsed 197.00s\n","Epoch 34 Batch 2400/4934 Accuracy 0.3839 Loss 3.6737 Time elapsed 205.56s\n","Epoch 34 Batch 2500/4934 Accuracy 0.3842 Loss 3.6721 Time elapsed 214.13s\n","Epoch 34 Batch 2600/4934 Accuracy 0.3844 Loss 3.6723 Time elapsed 222.69s\n","Epoch 34 Batch 2700/4934 Accuracy 0.3845 Loss 3.6719 Time elapsed 231.26s\n","Epoch 34 Batch 2800/4934 Accuracy 0.3851 Loss 3.6679 Time elapsed 239.82s\n","Epoch 34 Batch 2900/4934 Accuracy 0.3862 Loss 3.6584 Time elapsed 248.38s\n","Epoch 34 Batch 3000/4934 Accuracy 0.3876 Loss 3.6455 Time elapsed 256.95s\n","Epoch 34 Batch 3100/4934 Accuracy 0.3886 Loss 3.6362 Time elapsed 265.51s\n","Epoch 34 Batch 3200/4934 Accuracy 0.3890 Loss 3.6327 Time elapsed 274.07s\n","Epoch 34 Batch 3300/4934 Accuracy 0.3893 Loss 3.6297 Time elapsed 282.64s\n","Epoch 34 Batch 3400/4934 Accuracy 0.3892 Loss 3.6299 Time elapsed 291.20s\n","Epoch 34 Batch 3500/4934 Accuracy 0.3891 Loss 3.6301 Time elapsed 299.77s\n","Epoch 34 Batch 3600/4934 Accuracy 0.3888 Loss 3.6337 Time elapsed 308.34s\n","Epoch 34 Batch 3700/4934 Accuracy 0.3883 Loss 3.6384 Time elapsed 316.91s\n","Epoch 34 Batch 3800/4934 Accuracy 0.3879 Loss 3.6424 Time elapsed 325.48s\n","Epoch 34 Batch 3900/4934 Accuracy 0.3873 Loss 3.6475 Time elapsed 334.05s\n","Epoch 34 Batch 4000/4934 Accuracy 0.3868 Loss 3.6530 Time elapsed 342.62s\n","Epoch 34 Batch 4100/4934 Accuracy 0.3864 Loss 3.6582 Time elapsed 351.18s\n","Epoch 34 Batch 4200/4934 Accuracy 0.3860 Loss 3.6635 Time elapsed 359.75s\n","Epoch 34 Batch 4300/4934 Accuracy 0.3857 Loss 3.6669 Time elapsed 368.32s\n","Epoch 34 Batch 4400/4934 Accuracy 0.3854 Loss 3.6699 Time elapsed 376.89s\n","Epoch 34 Batch 4500/4934 Accuracy 0.3852 Loss 3.6729 Time elapsed 385.46s\n","Epoch 34 Batch 4600/4934 Accuracy 0.3850 Loss 3.6764 Time elapsed 394.03s\n","Epoch 34 Batch 4700/4934 Accuracy 0.3848 Loss 3.6790 Time elapsed 402.59s\n","Epoch 34 Batch 4800/4934 Accuracy 0.3847 Loss 3.6810 Time elapsed 411.16s\n","Epoch 34 Batch 4900/4934 Accuracy 0.3846 Loss 3.6832 Time elapsed 419.73s\n","Epoch 34 Batch 4934/4934 Accuracy 0.3846 Loss 3.6838 Time elapsed 422.65s\n","Epoch 34 Accuracy 0.3846 Loss 3.6838 Time 423.77 seconds\n","\n","Epoch 35 Batch 100/4934 Accuracy 0.3520 Loss 3.9434 Time elapsed 8.56s\n","Epoch 35 Batch 200/4934 Accuracy 0.3564 Loss 3.9121 Time elapsed 17.13s\n","Epoch 35 Batch 300/4934 Accuracy 0.3630 Loss 3.8670 Time elapsed 25.70s\n","Epoch 35 Batch 400/4934 Accuracy 0.3676 Loss 3.8308 Time elapsed 34.26s\n","Epoch 35 Batch 500/4934 Accuracy 0.3704 Loss 3.8053 Time elapsed 42.82s\n","Epoch 35 Batch 600/4934 Accuracy 0.3714 Loss 3.7879 Time elapsed 51.38s\n","Epoch 35 Batch 700/4934 Accuracy 0.3732 Loss 3.7691 Time elapsed 59.95s\n","Epoch 35 Batch 800/4934 Accuracy 0.3742 Loss 3.7602 Time elapsed 68.52s\n","Epoch 35 Batch 900/4934 Accuracy 0.3759 Loss 3.7440 Time elapsed 77.09s\n","Epoch 35 Batch 1000/4934 Accuracy 0.3771 Loss 3.7317 Time elapsed 85.65s\n","Epoch 35 Batch 1100/4934 Accuracy 0.3779 Loss 3.7232 Time elapsed 94.22s\n","Epoch 35 Batch 1200/4934 Accuracy 0.3789 Loss 3.7158 Time elapsed 102.78s\n","Epoch 35 Batch 1300/4934 Accuracy 0.3795 Loss 3.7078 Time elapsed 111.35s\n","Epoch 35 Batch 1400/4934 Accuracy 0.3806 Loss 3.6996 Time elapsed 119.91s\n","Epoch 35 Batch 1500/4934 Accuracy 0.3814 Loss 3.6934 Time elapsed 128.47s\n","Epoch 35 Batch 1600/4934 Accuracy 0.3822 Loss 3.6875 Time elapsed 137.04s\n","Epoch 35 Batch 1700/4934 Accuracy 0.3827 Loss 3.6847 Time elapsed 145.60s\n","Epoch 35 Batch 1800/4934 Accuracy 0.3833 Loss 3.6816 Time elapsed 154.15s\n","Epoch 35 Batch 1900/4934 Accuracy 0.3838 Loss 3.6800 Time elapsed 162.71s\n","Epoch 35 Batch 2000/4934 Accuracy 0.3842 Loss 3.6790 Time elapsed 171.26s\n","Epoch 35 Batch 2100/4934 Accuracy 0.3846 Loss 3.6777 Time elapsed 179.81s\n","Epoch 35 Batch 2200/4934 Accuracy 0.3851 Loss 3.6753 Time elapsed 188.36s\n","Epoch 35 Batch 2300/4934 Accuracy 0.3855 Loss 3.6731 Time elapsed 196.91s\n","Epoch 35 Batch 2400/4934 Accuracy 0.3861 Loss 3.6701 Time elapsed 205.46s\n","Epoch 35 Batch 2500/4934 Accuracy 0.3862 Loss 3.6685 Time elapsed 214.02s\n","Epoch 35 Batch 2600/4934 Accuracy 0.3863 Loss 3.6689 Time elapsed 222.57s\n","Epoch 35 Batch 2700/4934 Accuracy 0.3864 Loss 3.6686 Time elapsed 231.12s\n","Epoch 35 Batch 2800/4934 Accuracy 0.3871 Loss 3.6648 Time elapsed 239.68s\n","Epoch 35 Batch 2900/4934 Accuracy 0.3884 Loss 3.6550 Time elapsed 248.23s\n","Epoch 35 Batch 3000/4934 Accuracy 0.3899 Loss 3.6424 Time elapsed 256.78s\n","Epoch 35 Batch 3100/4934 Accuracy 0.3909 Loss 3.6328 Time elapsed 265.33s\n","Epoch 35 Batch 3200/4934 Accuracy 0.3914 Loss 3.6291 Time elapsed 273.88s\n","Epoch 35 Batch 3300/4934 Accuracy 0.3917 Loss 3.6257 Time elapsed 282.44s\n","Epoch 35 Batch 3400/4934 Accuracy 0.3916 Loss 3.6263 Time elapsed 291.00s\n","Epoch 35 Batch 3500/4934 Accuracy 0.3916 Loss 3.6267 Time elapsed 299.57s\n","Epoch 35 Batch 3600/4934 Accuracy 0.3913 Loss 3.6301 Time elapsed 308.12s\n","Epoch 35 Batch 3700/4934 Accuracy 0.3908 Loss 3.6347 Time elapsed 316.68s\n","Epoch 35 Batch 3800/4934 Accuracy 0.3903 Loss 3.6388 Time elapsed 325.24s\n","Epoch 35 Batch 3900/4934 Accuracy 0.3898 Loss 3.6441 Time elapsed 333.80s\n","Epoch 35 Batch 4000/4934 Accuracy 0.3892 Loss 3.6499 Time elapsed 342.36s\n","Epoch 35 Batch 4100/4934 Accuracy 0.3888 Loss 3.6551 Time elapsed 350.91s\n","Epoch 35 Batch 4200/4934 Accuracy 0.3882 Loss 3.6608 Time elapsed 359.47s\n","Epoch 35 Batch 4300/4934 Accuracy 0.3879 Loss 3.6642 Time elapsed 368.04s\n","Epoch 35 Batch 4400/4934 Accuracy 0.3877 Loss 3.6671 Time elapsed 376.59s\n","Epoch 35 Batch 4500/4934 Accuracy 0.3875 Loss 3.6701 Time elapsed 385.14s\n","Epoch 35 Batch 4600/4934 Accuracy 0.3872 Loss 3.6735 Time elapsed 393.70s\n","Epoch 35 Batch 4700/4934 Accuracy 0.3870 Loss 3.6762 Time elapsed 402.26s\n","Epoch 35 Batch 4800/4934 Accuracy 0.3870 Loss 3.6786 Time elapsed 410.82s\n","Epoch 35 Batch 4900/4934 Accuracy 0.3869 Loss 3.6807 Time elapsed 419.38s\n","Epoch 35 Batch 4934/4934 Accuracy 0.3869 Loss 3.6816 Time elapsed 422.30s\n","Epoch 35 Accuracy 0.3869 Loss 3.6816 Time 423.38 seconds\n","\n","Epoch 36 Batch 100/4934 Accuracy 0.3565 Loss 3.9418 Time elapsed 8.56s\n","Epoch 36 Batch 200/4934 Accuracy 0.3619 Loss 3.9028 Time elapsed 17.11s\n","Epoch 36 Batch 300/4934 Accuracy 0.3670 Loss 3.8624 Time elapsed 25.66s\n","Epoch 36 Batch 400/4934 Accuracy 0.3711 Loss 3.8270 Time elapsed 34.22s\n","Epoch 36 Batch 500/4934 Accuracy 0.3737 Loss 3.8022 Time elapsed 42.77s\n","Epoch 36 Batch 600/4934 Accuracy 0.3751 Loss 3.7850 Time elapsed 51.33s\n","Epoch 36 Batch 700/4934 Accuracy 0.3768 Loss 3.7666 Time elapsed 59.89s\n","Epoch 36 Batch 800/4934 Accuracy 0.3778 Loss 3.7571 Time elapsed 68.46s\n","Epoch 36 Batch 900/4934 Accuracy 0.3795 Loss 3.7409 Time elapsed 77.01s\n","Epoch 36 Batch 1000/4934 Accuracy 0.3809 Loss 3.7278 Time elapsed 85.57s\n","Epoch 36 Batch 1100/4934 Accuracy 0.3815 Loss 3.7196 Time elapsed 94.13s\n","Epoch 36 Batch 1200/4934 Accuracy 0.3823 Loss 3.7121 Time elapsed 102.70s\n","Epoch 36 Batch 1300/4934 Accuracy 0.3830 Loss 3.7042 Time elapsed 111.27s\n","Epoch 36 Batch 1400/4934 Accuracy 0.3843 Loss 3.6960 Time elapsed 119.84s\n","Epoch 36 Batch 1500/4934 Accuracy 0.3850 Loss 3.6903 Time elapsed 128.40s\n","Epoch 36 Batch 1600/4934 Accuracy 0.3855 Loss 3.6852 Time elapsed 136.96s\n","Epoch 36 Batch 1700/4934 Accuracy 0.3860 Loss 3.6826 Time elapsed 145.52s\n","Epoch 36 Batch 1800/4934 Accuracy 0.3866 Loss 3.6800 Time elapsed 154.08s\n","Epoch 36 Batch 1900/4934 Accuracy 0.3870 Loss 3.6790 Time elapsed 162.64s\n","Epoch 36 Batch 2000/4934 Accuracy 0.3873 Loss 3.6785 Time elapsed 171.19s\n","Epoch 36 Batch 2100/4934 Accuracy 0.3877 Loss 3.6779 Time elapsed 179.75s\n","Epoch 36 Batch 2200/4934 Accuracy 0.3883 Loss 3.6750 Time elapsed 188.31s\n","Epoch 36 Batch 2300/4934 Accuracy 0.3886 Loss 3.6737 Time elapsed 196.87s\n","Epoch 36 Batch 2400/4934 Accuracy 0.3891 Loss 3.6711 Time elapsed 205.43s\n","Epoch 36 Batch 2500/4934 Accuracy 0.3892 Loss 3.6697 Time elapsed 213.98s\n","Epoch 36 Batch 2600/4934 Accuracy 0.3892 Loss 3.6701 Time elapsed 222.54s\n","Epoch 36 Batch 2700/4934 Accuracy 0.3894 Loss 3.6701 Time elapsed 231.10s\n","Epoch 36 Batch 2800/4934 Accuracy 0.3900 Loss 3.6664 Time elapsed 239.67s\n","Epoch 36 Batch 2900/4934 Accuracy 0.3912 Loss 3.6566 Time elapsed 248.23s\n","Epoch 36 Batch 3000/4934 Accuracy 0.3927 Loss 3.6438 Time elapsed 256.79s\n","Epoch 36 Batch 3100/4934 Accuracy 0.3938 Loss 3.6341 Time elapsed 265.36s\n","Epoch 36 Batch 3200/4934 Accuracy 0.3942 Loss 3.6302 Time elapsed 273.92s\n","Epoch 36 Batch 3300/4934 Accuracy 0.3946 Loss 3.6269 Time elapsed 282.49s\n","Epoch 36 Batch 3400/4934 Accuracy 0.3945 Loss 3.6274 Time elapsed 291.05s\n","Epoch 36 Batch 3500/4934 Accuracy 0.3944 Loss 3.6281 Time elapsed 299.62s\n","Epoch 36 Batch 3600/4934 Accuracy 0.3941 Loss 3.6316 Time elapsed 308.19s\n","Epoch 36 Batch 3700/4934 Accuracy 0.3936 Loss 3.6365 Time elapsed 316.76s\n","Epoch 36 Batch 3800/4934 Accuracy 0.3930 Loss 3.6407 Time elapsed 325.33s\n","Epoch 36 Batch 3900/4934 Accuracy 0.3925 Loss 3.6461 Time elapsed 333.90s\n","Epoch 36 Batch 4000/4934 Accuracy 0.3920 Loss 3.6518 Time elapsed 342.46s\n","Epoch 36 Batch 4100/4934 Accuracy 0.3915 Loss 3.6571 Time elapsed 351.03s\n","Epoch 36 Batch 4200/4934 Accuracy 0.3910 Loss 3.6625 Time elapsed 359.60s\n","Epoch 36 Batch 4300/4934 Accuracy 0.3907 Loss 3.6662 Time elapsed 368.17s\n","Epoch 36 Batch 4400/4934 Accuracy 0.3904 Loss 3.6696 Time elapsed 376.73s\n","Epoch 36 Batch 4500/4934 Accuracy 0.3902 Loss 3.6726 Time elapsed 385.30s\n","Epoch 36 Batch 4600/4934 Accuracy 0.3899 Loss 3.6762 Time elapsed 393.87s\n","Epoch 36 Batch 4700/4934 Accuracy 0.3897 Loss 3.6788 Time elapsed 402.44s\n","Epoch 36 Batch 4800/4934 Accuracy 0.3897 Loss 3.6811 Time elapsed 411.01s\n","Epoch 36 Batch 4900/4934 Accuracy 0.3896 Loss 3.6832 Time elapsed 419.58s\n","Epoch 36 Batch 4934/4934 Accuracy 0.3895 Loss 3.6840 Time elapsed 422.50s\n","Epoch 36 Accuracy 0.3895 Loss 3.6840 Time 422.52 seconds\n","\n","Epoch 37 Batch 100/4934 Accuracy 0.3579 Loss 3.9452 Time elapsed 8.56s\n","Epoch 37 Batch 200/4934 Accuracy 0.3626 Loss 3.9095 Time elapsed 17.12s\n","Epoch 37 Batch 300/4934 Accuracy 0.3685 Loss 3.8675 Time elapsed 25.69s\n","Epoch 37 Batch 400/4934 Accuracy 0.3730 Loss 3.8317 Time elapsed 34.25s\n","Epoch 37 Batch 500/4934 Accuracy 0.3755 Loss 3.8063 Time elapsed 42.82s\n","Epoch 37 Batch 600/4934 Accuracy 0.3762 Loss 3.7893 Time elapsed 51.38s\n","Epoch 37 Batch 700/4934 Accuracy 0.3780 Loss 3.7702 Time elapsed 59.95s\n","Epoch 37 Batch 800/4934 Accuracy 0.3789 Loss 3.7600 Time elapsed 68.52s\n","Epoch 37 Batch 900/4934 Accuracy 0.3807 Loss 3.7443 Time elapsed 77.08s\n","Epoch 37 Batch 1000/4934 Accuracy 0.3820 Loss 3.7311 Time elapsed 85.65s\n","Epoch 37 Batch 1100/4934 Accuracy 0.3831 Loss 3.7224 Time elapsed 94.21s\n","Epoch 37 Batch 1200/4934 Accuracy 0.3841 Loss 3.7153 Time elapsed 102.78s\n","Epoch 37 Batch 1300/4934 Accuracy 0.3848 Loss 3.7074 Time elapsed 111.34s\n","Epoch 37 Batch 1400/4934 Accuracy 0.3861 Loss 3.6998 Time elapsed 119.91s\n","Epoch 37 Batch 1500/4934 Accuracy 0.3868 Loss 3.6944 Time elapsed 128.47s\n","Epoch 37 Batch 1600/4934 Accuracy 0.3874 Loss 3.6894 Time elapsed 137.04s\n","Epoch 37 Batch 1700/4934 Accuracy 0.3879 Loss 3.6872 Time elapsed 145.60s\n","Epoch 37 Batch 1800/4934 Accuracy 0.3884 Loss 3.6847 Time elapsed 154.17s\n","Epoch 37 Batch 1900/4934 Accuracy 0.3888 Loss 3.6837 Time elapsed 162.73s\n","Epoch 37 Batch 2000/4934 Accuracy 0.3892 Loss 3.6834 Time elapsed 171.30s\n","Epoch 37 Batch 2100/4934 Accuracy 0.3895 Loss 3.6828 Time elapsed 179.87s\n","Epoch 37 Batch 2200/4934 Accuracy 0.3899 Loss 3.6805 Time elapsed 188.43s\n","Epoch 37 Batch 2300/4934 Accuracy 0.3902 Loss 3.6790 Time elapsed 196.99s\n","Epoch 37 Batch 2400/4934 Accuracy 0.3907 Loss 3.6763 Time elapsed 205.55s\n","Epoch 37 Batch 2500/4934 Accuracy 0.3910 Loss 3.6750 Time elapsed 214.11s\n","Epoch 37 Batch 2600/4934 Accuracy 0.3911 Loss 3.6752 Time elapsed 222.68s\n","Epoch 37 Batch 2700/4934 Accuracy 0.3913 Loss 3.6750 Time elapsed 231.24s\n","Epoch 37 Batch 2800/4934 Accuracy 0.3919 Loss 3.6713 Time elapsed 239.80s\n","Epoch 37 Batch 2900/4934 Accuracy 0.3930 Loss 3.6617 Time elapsed 248.36s\n","Epoch 37 Batch 3000/4934 Accuracy 0.3946 Loss 3.6490 Time elapsed 256.92s\n","Epoch 37 Batch 3100/4934 Accuracy 0.3956 Loss 3.6394 Time elapsed 265.48s\n","Epoch 37 Batch 3200/4934 Accuracy 0.3961 Loss 3.6355 Time elapsed 274.04s\n","Epoch 37 Batch 3300/4934 Accuracy 0.3964 Loss 3.6322 Time elapsed 282.61s\n","Epoch 37 Batch 3400/4934 Accuracy 0.3964 Loss 3.6326 Time elapsed 291.17s\n","Epoch 37 Batch 3500/4934 Accuracy 0.3963 Loss 3.6330 Time elapsed 299.74s\n","Epoch 37 Batch 3600/4934 Accuracy 0.3960 Loss 3.6363 Time elapsed 308.30s\n","Epoch 37 Batch 3700/4934 Accuracy 0.3955 Loss 3.6414 Time elapsed 316.86s\n","Epoch 37 Batch 3800/4934 Accuracy 0.3950 Loss 3.6456 Time elapsed 325.43s\n","Epoch 37 Batch 3900/4934 Accuracy 0.3945 Loss 3.6514 Time elapsed 334.00s\n","Epoch 37 Batch 4000/4934 Accuracy 0.3938 Loss 3.6572 Time elapsed 342.56s\n","Epoch 37 Batch 4100/4934 Accuracy 0.3934 Loss 3.6625 Time elapsed 351.13s\n","Epoch 37 Batch 4200/4934 Accuracy 0.3929 Loss 3.6679 Time elapsed 359.70s\n","Epoch 37 Batch 4300/4934 Accuracy 0.3926 Loss 3.6715 Time elapsed 368.26s\n","Epoch 37 Batch 4400/4934 Accuracy 0.3923 Loss 3.6747 Time elapsed 376.83s\n","Epoch 37 Batch 4500/4934 Accuracy 0.3922 Loss 3.6778 Time elapsed 385.40s\n","Epoch 37 Batch 4600/4934 Accuracy 0.3919 Loss 3.6813 Time elapsed 393.97s\n","Epoch 37 Batch 4700/4934 Accuracy 0.3918 Loss 3.6837 Time elapsed 402.54s\n","Epoch 37 Batch 4800/4934 Accuracy 0.3918 Loss 3.6861 Time elapsed 411.10s\n","Epoch 37 Batch 4900/4934 Accuracy 0.3917 Loss 3.6879 Time elapsed 419.67s\n","Epoch 37 Batch 4934/4934 Accuracy 0.3917 Loss 3.6886 Time elapsed 422.59s\n","Epoch 37 Accuracy 0.3917 Loss 3.6886 Time 422.61 seconds\n","\n","Epoch 38 Batch 100/4934 Accuracy 0.3598 Loss 3.9471 Time elapsed 8.56s\n","Epoch 38 Batch 200/4934 Accuracy 0.3657 Loss 3.9040 Time elapsed 17.12s\n","Epoch 38 Batch 300/4934 Accuracy 0.3723 Loss 3.8662 Time elapsed 25.70s\n","Epoch 38 Batch 400/4934 Accuracy 0.3760 Loss 3.8357 Time elapsed 34.26s\n","Epoch 38 Batch 500/4934 Accuracy 0.3789 Loss 3.8128 Time elapsed 42.86s\n","Epoch 38 Batch 600/4934 Accuracy 0.3796 Loss 3.7957 Time elapsed 51.42s\n","Epoch 38 Batch 700/4934 Accuracy 0.3813 Loss 3.7774 Time elapsed 59.99s\n","Epoch 38 Batch 800/4934 Accuracy 0.3820 Loss 3.7678 Time elapsed 68.56s\n","Epoch 38 Batch 900/4934 Accuracy 0.3837 Loss 3.7514 Time elapsed 77.12s\n","Epoch 38 Batch 1000/4934 Accuracy 0.3848 Loss 3.7392 Time elapsed 85.69s\n","Epoch 38 Batch 1100/4934 Accuracy 0.3854 Loss 3.7312 Time elapsed 94.27s\n","Epoch 38 Batch 1200/4934 Accuracy 0.3866 Loss 3.7237 Time elapsed 102.83s\n","Epoch 38 Batch 1300/4934 Accuracy 0.3873 Loss 3.7156 Time elapsed 111.41s\n","Epoch 38 Batch 1400/4934 Accuracy 0.3884 Loss 3.7077 Time elapsed 119.98s\n","Epoch 38 Batch 1500/4934 Accuracy 0.3893 Loss 3.7021 Time elapsed 128.55s\n","Epoch 38 Batch 1600/4934 Accuracy 0.3900 Loss 3.6976 Time elapsed 137.12s\n","Epoch 38 Batch 1700/4934 Accuracy 0.3905 Loss 3.6953 Time elapsed 145.69s\n","Epoch 38 Batch 1800/4934 Accuracy 0.3909 Loss 3.6933 Time elapsed 154.26s\n","Epoch 38 Batch 1900/4934 Accuracy 0.3914 Loss 3.6922 Time elapsed 162.82s\n","Epoch 38 Batch 2000/4934 Accuracy 0.3917 Loss 3.6912 Time elapsed 171.39s\n","Epoch 38 Batch 2100/4934 Accuracy 0.3921 Loss 3.6902 Time elapsed 179.97s\n","Epoch 38 Batch 2200/4934 Accuracy 0.3926 Loss 3.6877 Time elapsed 188.54s\n","Epoch 38 Batch 2300/4934 Accuracy 0.3930 Loss 3.6861 Time elapsed 197.11s\n","Epoch 38 Batch 2400/4934 Accuracy 0.3935 Loss 3.6833 Time elapsed 205.68s\n","Epoch 38 Batch 2500/4934 Accuracy 0.3938 Loss 3.6820 Time elapsed 214.26s\n","Epoch 38 Batch 2600/4934 Accuracy 0.3939 Loss 3.6822 Time elapsed 222.82s\n","Epoch 38 Batch 2700/4934 Accuracy 0.3940 Loss 3.6821 Time elapsed 231.39s\n","Epoch 38 Batch 2800/4934 Accuracy 0.3946 Loss 3.6782 Time elapsed 239.95s\n","Epoch 38 Batch 2900/4934 Accuracy 0.3958 Loss 3.6687 Time elapsed 248.51s\n","Epoch 38 Batch 3000/4934 Accuracy 0.3973 Loss 3.6559 Time elapsed 257.08s\n","Epoch 38 Batch 3100/4934 Accuracy 0.3984 Loss 3.6466 Time elapsed 265.64s\n","Epoch 38 Batch 3200/4934 Accuracy 0.3988 Loss 3.6429 Time elapsed 274.21s\n","Epoch 38 Batch 3300/4934 Accuracy 0.3992 Loss 3.6397 Time elapsed 282.77s\n","Epoch 38 Batch 3400/4934 Accuracy 0.3991 Loss 3.6405 Time elapsed 291.34s\n","Epoch 38 Batch 3500/4934 Accuracy 0.3991 Loss 3.6409 Time elapsed 299.90s\n","Epoch 38 Batch 3600/4934 Accuracy 0.3988 Loss 3.6444 Time elapsed 308.47s\n","Epoch 38 Batch 3700/4934 Accuracy 0.3983 Loss 3.6494 Time elapsed 317.04s\n","Epoch 38 Batch 3800/4934 Accuracy 0.3978 Loss 3.6537 Time elapsed 325.61s\n","Epoch 38 Batch 3900/4934 Accuracy 0.3973 Loss 3.6593 Time elapsed 334.18s\n","Epoch 38 Batch 4000/4934 Accuracy 0.3968 Loss 3.6647 Time elapsed 342.75s\n","Epoch 38 Batch 4100/4934 Accuracy 0.3964 Loss 3.6701 Time elapsed 351.32s\n","Epoch 38 Batch 4200/4934 Accuracy 0.3959 Loss 3.6755 Time elapsed 359.88s\n","Epoch 38 Batch 4300/4934 Accuracy 0.3956 Loss 3.6789 Time elapsed 368.45s\n","Epoch 38 Batch 4400/4934 Accuracy 0.3954 Loss 3.6822 Time elapsed 377.02s\n","Epoch 38 Batch 4500/4934 Accuracy 0.3952 Loss 3.6856 Time elapsed 385.58s\n","Epoch 38 Batch 4600/4934 Accuracy 0.3949 Loss 3.6891 Time elapsed 394.15s\n","Epoch 38 Batch 4700/4934 Accuracy 0.3947 Loss 3.6916 Time elapsed 402.72s\n","Epoch 38 Batch 4800/4934 Accuracy 0.3947 Loss 3.6937 Time elapsed 411.29s\n","Epoch 38 Batch 4900/4934 Accuracy 0.3946 Loss 3.6955 Time elapsed 419.86s\n","Epoch 38 Batch 4934/4934 Accuracy 0.3946 Loss 3.6960 Time elapsed 422.78s\n","Early stopping triggered at epoch 38\n"]}],"source":["patience = 3  \n","best_loss = float('inf')\n","wait = 0\n","total_batches = tf.data.experimental.cardinality(dataset).numpy()\n","epochs = 25\n","\n","\n","for epoch in range(first_epoch, first_epoch + epochs):\n","    start = time.time()\n","    start_epoch = time.time()\n","    train_loss.reset_state()\n","    train_accuracy.reset_state()\n","\n","\n","    for (batch, (inp, tar)) in enumerate(dataset):\n","        start_batch = time.time()\n","        train_step(inp, tar)\n","        \n","        if (batch + 1) % 100 == 0 or (batch + 1) == total_batches:\n","            elapsed_batch = time.time() - start_epoch\n","            print(f\"Epoch {epoch+1} Batch {batch+1}/{total_batches} \"\n","                  f\"Accuracy {train_accuracy.result():.4f} \"\n","                  f\"Loss {train_loss.result():.4f} \"\n","                  f\"Time elapsed {elapsed_batch:.2f}s\")\n","\n","    current_loss = train_loss.result()\n","    \n","    if current_loss < best_loss:\n","        best_loss = current_loss\n","        wait = 0\n","        ckpt_save_path = ckpt_manager.save()\n","    else:\n","        wait += 1\n","        if wait >= patience:\n","            print(f\"Early stopping triggered at epoch {epoch+1}\")\n","            break\n","\n","    print(f'Epoch {epoch + 1} Accuracy {train_accuracy.result():.4f} Loss {current_loss:.4f} Time {time.time() - start:.2f} seconds\\n')"]},{"cell_type":"code","execution_count":30,"id":"e9c28651","metadata":{"execution":{"iopub.execute_input":"2025-11-15T19:11:38.829759Z","iopub.status.busy":"2025-11-15T19:11:38.829028Z","iopub.status.idle":"2025-11-15T19:11:38.837899Z","shell.execute_reply":"2025-11-15T19:11:38.837236Z"},"papermill":{"duration":0.065588,"end_time":"2025-11-15T19:11:38.839035","exception":false,"start_time":"2025-11-15T19:11:38.773447","status":"completed"},"tags":[]},"outputs":[],"source":["def summarize(input_article, beam_width=3):\n","    input_article = sp_text.encode_as_ids(input_article)\n","    input_article = tf.keras.preprocessing.sequence.pad_sequences([input_article], maxlen=max_text_len, padding='post', truncating='post')\n","    encoder_input = tf.expand_dims(input_article[0], 0)\n","\n","    sos_id = sp_summary.piece_to_id('<SOS>')\n","    eos_id = sp_summary.piece_to_id('<EOS>')\n","\n","    sequences = [([sos_id], 0.0)]\n","    completed_sequences = []\n","\n","    for _ in range(max_summary_len):\n","        all_candidates = []\n","        for seq, score in sequences:\n","            if seq[-1] == eos_id:\n","                completed_sequences.append((seq, score))\n","                continue\n","\n","            output = tf.expand_dims(seq, 0)\n","            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n","\n","            predictions, _ = transformer(\n","                encoder_input, \n","                output,\n","                training=False,\n","                enc_padding_mask=enc_padding_mask,\n","                look_ahead_mask=combined_mask,\n","                dec_padding_mask=dec_padding_mask\n","            )\n","\n","            logits = predictions[:, -1, :]\n","            #log_probs = tf.math.log(tf.nn.softmax(logits))\n","            log_probs = tf.nn.log_softmax(logits)\n","            top_k = tf.math.top_k(log_probs, k=beam_width)\n","\n","            for i in range(beam_width):\n","                token = int(top_k.indices[0, i])\n","                candidate_score = score + float(top_k.values[0, i])\n","                candidate_seq = seq + [token]\n","                \n","                length_penalty = ((5 + len(candidate_seq)) / 6) ** 0.6\n","                normalized_score = candidate_score / length_penalty\n","\n","                \n","                all_candidates.append((candidate_seq, normalized_score))\n","\n","        sequences = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)[:beam_width]\n","\n","        if not sequences:\n","            break\n","\n","    completed_sequences.extend(sequences)\n","    best_seq = max(completed_sequences, key=lambda tup: tup[1])[0]\n","\n","    return sp_summary.decode_ids(best_seq)\n"]},{"cell_type":"code","execution_count":31,"id":"34c89805","metadata":{"execution":{"iopub.execute_input":"2025-11-15T19:11:38.95152Z","iopub.status.busy":"2025-11-15T19:11:38.950905Z","iopub.status.idle":"2025-11-15T19:13:20.18284Z","shell.execute_reply":"2025-11-15T19:13:20.182169Z"},"papermill":{"duration":101.343685,"end_time":"2025-11-15T19:13:20.23856","exception":false,"start_time":"2025-11-15T19:11:38.894875","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Text : <news_summary_more> Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers. \n","Real Headline : upGrad learner switches to career in ML & Al with 90% salary hike \n","Summary : <SOS>-based firm to be made in India&#39;s 2nd largest tech company: Report CEO Pichai K\n","\n","Text : <news_summary_more> Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more. \n","Real Headline : Delhi techie wins free food from Swiggy for one year on CRED \n","Summary : <SOS>-based firm offers free access to users in India-based game: Report headline data centre CEO\n","\n","Text : <news_summary_more> New Zealand defeated India by 8 wickets in the fourth ODI at Hamilton on Thursday to win their first match of the five-match ODI series. India lost an international match under Rohit Sharma's captaincy after 12 consecutive victories dating back to March 2018. The match witnessed India getting all out for 92, their seventh lowest total in ODI cricket history. \n","Real Headline : New Zealand end Rohit Sharma-led India's 12-match winning streak \n","Summary : <SOS> gives India 2nd consecutive T20I series win on this day ever: Kohli to B&#39;desh\n","\n","Text : <news_summary_more> With Aegon Life iTerm Insurance plan, customers can enjoy tax benefits on your premiums paid and save up to Ã¢ÂÂ¹46,800^ on taxes. The plan provides life cover up to the age of 100 years. Also, customers have options to insure against Critical Illnesses, Disability and Accidental Death Benefit Rider with a life cover up to the age of 80 years. \n","Real Headline : Aegon life iTerm insurance plan helps customers save tax \n","Summary : <SOS>-based firm offers free food to people on flight tickets sale: Report CEO Pichai BWT Dept\n","\n","Text : <news_summary_more> Speaking about the sexual harassment allegations against Rajkumar Hirani, Sonam Kapoor said, \"I've known Hirani for many years...What if it's not true, the [#MeToo] movement will get derailed.\" \"In the #MeToo movement, I always believe a woman. But in this case, we need to reserve our judgment,\" she added. Hirani has been accused by an assistant who worked in 'Sanju'. \n","Real Headline : Have known Hirani for yrs, what if MeToo claims are not true: Sonam \n","Summary : <SOS> to be allowed to go out on Twitter today: Report on SC order row: Report row with media\n","\n","Text : <news_summary_more> Pakistani singer Rahat Fateh Ali Khan has denied receiving any notice from the Enforcement Directorate over allegedly smuggling foreign currency out of India. \"It would have been better if the authorities would have served the notice first if any and then publicised this,\" reads a press release issued on behalf of Rahat. The statement further called the allegation \"bizarre\". \n","Real Headline : Rahat Fateh Ali Khan denies getting notice for smuggling currency \n","Summary : <SOS> gets legal notice from Pakistan on sale: Report on HC order to I-Day row: Report row\n","\n","Text : <news_summary_more> India recorded their lowest ODI total in New Zealand after getting all out for 92 runs in 30.5 overs in the fourth ODI at Hamilton on Thursday. Seven of India's batsmen were dismissed for single-digit scores, while their number ten batsman Yuzvendra Chahal top-scored with 18*(37). India's previous lowest ODI total in New Zealand was 108. \n","Real Headline : India get all out for 92, their lowest ODI total in New Zealand \n","Summary : <SOS> falls on India&#39;s 2nd highest ever ever T20I total at home on this day:\n","\n","Text : <news_summary_more> Weeks after ex-CBI Director Alok Verma told the Department of Personnel and Training to consider him retired, the Home Ministry asked him to join work on the last day of his fixed tenure as Director on Thursday. The ministry directed him to immediately join as DG, Fire Services, the post he was transferred to after his removal as CBI chief. \n","Real Headline : Govt directs Alok Verma to join work 1 day before his retirement \n","Summary : <SOS> to be made from ex-PM Singh: Report on govt&#39;s death row-level chief&#39;\n","\n","Text : <news_summary_more> Andhra Pradesh CM N Chandrababu Naidu has said, \"When I met then US President Bill Clinton, I addressed him as Mr Clinton, not as 'sir'. (PM Narendra) Modi is my junior in politics...I addressed him as sir 10 times.\" \"I did this...to satisfy his ego in the hope that he will do justice to the state,\" he added. \n","Real Headline : Called PM Modi 'sir' 10 times to satisfy his ego: Andhra CM \n","Summary : <SOS> of India&#39;s 1st ever President: Report on PM Modi&#39;s b&#39;day visit to\n","\n","Text : <news_summary_more> Congress candidate Shafia Zubair won the Ramgarh Assembly seat in Rajasthan, by defeating BJP's Sukhwant Singh with a margin of 12,228 votes in the bypoll. With this victory, Congress has taken its total to 100 seats in the 200-member assembly. The election to the Ramgarh seat was delayed due to the death of sitting MLA and BSP candidate Laxman Singh. \n","Real Headline : Cong wins Ramgarh bypoll in Rajasthan, takes total to 100 seats \n","Summary : <SOS> to be held in Maharashtra in 1st phase of HC order: BJP MP CM&#39;s son-\n"]}],"source":["for i in range(10):\n","    text = df_trans['cleaned_text'][i]\n","    real_summary = df_trans['cleaned_summary'][i]\n","    print(f\"\\nText : {text} \\nReal Headline : {real_summary} \\nSummary : {summarize(text)}\")"]},{"cell_type":"markdown","id":"3363d61b","metadata":{"papermill":{"duration":0.050953,"end_time":"2025-11-15T19:13:20.341115","exception":false,"start_time":"2025-11-15T19:13:20.290162","status":"completed"},"tags":[]},"source":["# Credits\n","\n","- **\"Implementing Seq2Seq Models for Text Summarization With Keras\"**  \n","  *by Samhita Alla* "]},{"cell_type":"code","execution_count":32,"id":"8c76bd55","metadata":{"execution":{"iopub.execute_input":"2025-11-15T19:13:20.444256Z","iopub.status.busy":"2025-11-15T19:13:20.443995Z","iopub.status.idle":"2025-11-15T19:13:33.425124Z","shell.execute_reply":"2025-11-15T19:13:33.424483Z"},"papermill":{"duration":13.034086,"end_time":"2025-11-15T19:13:33.426301","exception":false,"start_time":"2025-11-15T19:13:20.392215","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working/checkpoints/ckpt-35.data-00000-of-00001\n","/kaggle/working/checkpoints/checkpoint\n","/kaggle/working/checkpoints/ckpt-35.index\n"]},{"data":{"text/plain":["'/kaggle/working/checkpoints_backup.zip'"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import shutil\n","\n","checkpoint_dir = '/kaggle/working/checkpoints'\n","for root, dirs, files in os.walk(checkpoint_dir):\n","    for file in files:\n","        print(os.path.join(root, file))\n","        \n","shutil.make_archive('/kaggle/working/checkpoints_backup', 'zip', '/kaggle/working/checkpoints')"]},{"cell_type":"code","execution_count":null,"id":"05451c12","metadata":{"papermill":{"duration":0.051527,"end_time":"2025-11-15T19:13:33.530402","exception":false,"start_time":"2025-11-15T19:13:33.478875","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":33526,"isSourceIdPinned":false,"sourceId":44284,"sourceType":"datasetVersion"},{"datasetId":1895,"sourceId":791838,"sourceType":"datasetVersion"},{"datasetId":8737012,"sourceId":13732139,"sourceType":"datasetVersion"},{"datasetId":8738117,"sourceId":13733688,"sourceType":"datasetVersion"},{"datasetId":8740607,"sourceId":13737156,"sourceType":"datasetVersion"},{"datasetId":8744566,"sourceId":13743088,"sourceType":"datasetVersion"},{"datasetId":8745635,"sourceId":13744561,"sourceType":"datasetVersion"},{"datasetId":8746205,"sourceId":13745316,"sourceType":"datasetVersion"},{"modelId":477297,"modelInstanceId":461540,"sourceId":614196,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":31089,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":9607.114491,"end_time":"2025-11-15T19:13:36.409139","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-11-15T16:33:29.294648","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}