{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db65795e",
   "metadata": {
    "papermill": {
     "duration": 0.009916,
     "end_time": "2025-11-13T15:02:19.272231",
     "exception": false,
     "start_time": "2025-11-13T15:02:19.262315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Project Overview\n",
    "\n",
    "This project focuses on **text summarization** using two approaches: a traditional **Seq2Seq model** with LSTM and a **Transformer-based model**. The goal is to see how each model performs and understand the difference between step-by-step sequence processing and attention-based processing.\n",
    "\n",
    "### Steps in the Project\n",
    "1. **Dataset Preparation**  \n",
    "   - Load the XSum dataset with articles and summaries.  \n",
    "   - Tokenize and pad sequences so they can be fed into the models.\n",
    "\n",
    "2. **Seq2Seq Model (LSTM)**  \n",
    "   - Build an encoder-decoder model without attention.\n",
    "   - Train it to generate summaries from the input articles.  \n",
    "\n",
    "3. **Transformer Model**  \n",
    "   - Build a Transformer-based encoder-decoder model.  \n",
    "   - Use self-attention to capture relationships between all tokens.  \n",
    "   - Train on the same dataset to generate summaries.\n",
    "\n",
    "4. **Comparison**  \n",
    "   - Compare the two models using metrics like ROUGE.  \n",
    "   - Look at differences in summary quality, speed, and how well they handle long sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980cc597",
   "metadata": {
    "papermill": {
     "duration": 0.009308,
     "end_time": "2025-11-13T15:02:19.290944",
     "exception": false,
     "start_time": "2025-11-13T15:02:19.281636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Seq2Seq and Encoder-Decoder\n",
    "\n",
    "## What is a Seq2Seq Model\n",
    "A sequence-to-sequence (Seq2Seq) model is designed to take an input sequence and produce an output sequence. It’s widely used in tasks like machine translation, text summarization, and chatbots.\n",
    "\n",
    "**Example:**  \n",
    "Input: \"Hello, how are you?\"  \n",
    "Output: \"Ciao, come stai?\"\n",
    "\n",
    "---\n",
    "\n",
    "## Encoder-Decoder Architecture (Expanded)\n",
    "\n",
    "A typical Seq2Seq model has two main parts: the **encoder** and the **decoder**. The design allows the model to process sequences of variable length.  \n",
    "\n",
    "### Encoder\n",
    "The encoder reads the input sequence and compresses it into a set of hidden states or a context vector. This vector captures the important information from the input and has a fixed size, though it does not need to match the decoder's size. The hidden states can either be passed as a whole to the decoder or connected at every decoding step.  \n",
    "\n",
    "At each step, the encoder updates its hidden state based on the previous hidden state and the current input. In mathematical terms, for a simple RNN:\n",
    "\n",
    "$$\n",
    "H_t^{encoder} = \\phi(W_{HH} \\cdot H_{t-1}^{encoder} + W_{HX} \\cdot X_t)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $H_t^{encoder}$ = hidden state at time $t$ in the encoder  \n",
    "- $X_t$ = input at time $t$  \n",
    "- $W_{HH}$ = weight matrix connecting hidden states  \n",
    "- $W_{HX}$ = weight matrix connecting input to hidden states  \n",
    "- $\\phi$ = activation function (e.g., tanh or ReLU)\n",
    "\n",
    "---\n",
    "\n",
    "### Decoder\n",
    "The decoder generates the output sequence one token at a time. Its initial hidden state is set to the final hidden state of the encoder. For a simple RNN decoder:\n",
    "\n",
    "$$\n",
    "H_t^{decoder} = \\phi(W_{HH} \\cdot H_{t-1}^{decoder} + W_{HY} \\cdot Y_{t-1})\n",
    "$$\n",
    "\n",
    "The output at each step is computed as:\n",
    "\n",
    "$$\n",
    "Y_t = W_{HY} \\cdot H_t^{decoder}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $H_t^{decoder}$ = hidden state at time $t$ in the decoder  \n",
    "- $Y_t$ = output at time $t$  \n",
    "- $W_{HY}$ = weight matrix connecting decoder hidden state to output  \n",
    "\n",
    "### Implementation Notes\n",
    "- Encoders and decoders are typically implemented with **RNNs, LSTMs, or GRUs**.  \n",
    "- The input and output vectors are of fixed size, but the encoder and decoder can have different hidden dimensions.  \n",
    "- During training, **teacher forcing** is often used, providing the correct previous token to the decoder instead of its own prediction.  \n",
    "\n",
    "---\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "Before feeding text into a Seq2Seq or Transformer model, the raw text must be converted into numerical form.  \n",
    "This is done through **tokenization**, which splits text into smaller units (tokens) such as words or subwords.  \n",
    "\n",
    "Each token is then mapped to a unique integer using a **vocabulary** built from the dataset.  \n",
    "The model processes these integers rather than the raw text.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Input text: `\"Transformers improve summarization.\"`  \n",
    "Tokens: `[\"transformers\", \"improve\", \"summarization\", \".\"]`  \n",
    "Token IDs: `[201, 57, 1342, 4]`\n",
    "\n",
    "### Why Tokenization Matters\n",
    "- Converts variable-length text into consistent, model-readable sequences.  \n",
    "- Helps capture word frequency and context relationships.  \n",
    "- Reduces vocabulary size when using subword tokenization (e.g., Byte Pair Encoding).  \n",
    "\n",
    "In this project, tokenization is part of preprocessing and includes:\n",
    "- **Lowercasing** the text  \n",
    "- **Removing special characters and URLs**  \n",
    "- **Splitting into tokens by spaces**  \n",
    "- Adding **start (`sostok`)** and **end (`eostok`)** tokens to mark summary boundaries  \n",
    "\n",
    "After tokenization, sequences will later be converted to integer IDs, padded or truncated to a fixed length\n",
    "\n",
    "---\n",
    "\n",
    "# Transformers\n",
    "Transformers can be seen as an evolution of Seq2Seq models. Instead of processing sequences step by step like LSTMs or GRUs, they rely entirely on **attention mechanisms** to process all tokens in parallel and capture relationships between them.\n",
    "\n",
    "### Attention in Transformers\n",
    "Attention is the core mechanism that allows Transformers to focus on relevant parts of the input sequence when producing a representation for each token. It works by comparing each token to all others and weighting them according to importance.\n",
    "\n",
    "#### How Attention Works\n",
    "Each token in the sequence is represented by three vectors:\n",
    "- **Query (Q):** what this token is looking for  \n",
    "- **Key (K):** what information this token contains  \n",
    "- **Value (V):** the actual information of the token  \n",
    "\n",
    "The attention score between two tokens is computed as the similarity between the Query of one token and the Key of another. This determines how much attention one token should pay to another. Mathematically, the attention weights are computed using a scaled dot-product:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\Big(\\frac{QK^T}{\\sqrt{d_k}}\\Big) V\n",
    "$$\n",
    "\n",
    "Where $d_k$ is the dimensionality of the Key vectors.\n",
    "\n",
    "- The **softmax** ensures that the weights sum to 1.  \n",
    "- Each token’s output is a weighted sum of all Value vectors, allowing it to incorporate context from the entire sequence.\n",
    "\n",
    "#### Multi-Head Attention\n",
    "Instead of computing attention just once, Transformers use **multiple attention heads** in parallel. Each head can learn to focus on different types of relationships, such as:\n",
    "- Syntactic relationships (e.g., subject-verb connections)  \n",
    "- Semantic relationships (e.g., synonyms or related concepts)  \n",
    "\n",
    "The outputs of all heads are concatenated and projected to form the final representation for each token.\n",
    "\n",
    "#### Intuition\n",
    "Imagine reading a sentence and highlighting all the words that are important for understanding each token. Each word “attends” to other words in the sentence that matter most for its meaning. Multi-head attention lets the model do this from multiple perspectives simultaneously.\n",
    "\n",
    "### Key Components of Transformers\n",
    "- **Encoder-Decoder Structure:** Like Seq2Seq models, Transformers have an encoder that processes the input and a decoder that generates the output. Both use layers of self-attention and feed-forward networks.  \n",
    "- **Positional Encoding:** Since Transformers don’t process tokens sequentially, they add positional information so the model knows the order of tokens.  \n",
    "- **Feed-Forward Layers:** After attention, each token passes through fully connected layers for additional transformation.\n",
    "\n",
    "### Advantages over LSTM/GRU Seq2Seq\n",
    "- Processes sequences **in parallel**, speeding up training.  \n",
    "- Handles **long sequences** more effectively with attention.  \n",
    "- Captures **complex relationships** between tokens regardless of distance.  \n",
    "- Scales easily to **very deep models** and large datasets.\n",
    "\n",
    "### Use Cases\n",
    "Transformers are the backbone of many state-of-the-art models for tasks such as:\n",
    "- Machine translation (e.g., T5, MarianMT)  \n",
    "- Text summarization (e.g., BART, Pegasus)  \n",
    "- Question answering and chatbots (e.g., GPT, BERT-based models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a33753d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:02:19.311267Z",
     "iopub.status.busy": "2025-11-13T15:02:19.310916Z",
     "iopub.status.idle": "2025-11-13T15:02:19.319180Z",
     "shell.execute_reply": "2025-11-13T15:02:19.318277Z"
    },
    "papermill": {
     "duration": 0.021323,
     "end_time": "2025-11-13T15:02:19.320961",
     "exception": false,
     "start_time": "2025-11-13T15:02:19.299638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb8a9cca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:02:19.342204Z",
     "iopub.status.busy": "2025-11-13T15:02:19.341827Z",
     "iopub.status.idle": "2025-11-13T15:02:24.675300Z",
     "shell.execute_reply": "2025-11-13T15:02:24.673359Z"
    },
    "papermill": {
     "duration": 5.346979,
     "end_time": "2025-11-13T15:02:24.677751",
     "exception": false,
     "start_time": "2025-11-13T15:02:19.330772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\r\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e7214c",
   "metadata": {
    "papermill": {
     "duration": 0.009264,
     "end_time": "2025-11-13T15:02:24.696398",
     "exception": false,
     "start_time": "2025-11-13T15:02:24.687134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preparation\n",
    "\n",
    "Prepare and clean the dataset for the summarization model:\n",
    "\n",
    "- **Load datasets:** Read two CSV files containing news articles and their summaries.\n",
    "- **Combine datasets:** Merge datasets while selecting relevant `text` and `summary` columns.\n",
    "- **Text cleaning:**  \n",
    "  - Convert text to lowercase.  \n",
    "  - Remove special characters.  \n",
    "  - Replace URLs with domain names.  \n",
    "  - Reduce multiple spaces.\n",
    "- **Tokenization:** Split cleaned text into tokens (words) and add `_START_` and `_END_` tokens for summaries.\n",
    "- **Handle missing values:** Drop rows with missing `text` values.\n",
    "- **Analyze sequence lengths:** Calculate word counts for texts and summaries.\n",
    "- **Limit sequence lengths:** Restrict `text` to 100 words and `summary` to 15 words.\n",
    "- **Add model tokens:** Prepend `sostok` and append `eostok` to all summaries to mark start and end for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dccd2ba1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:02:24.715784Z",
     "iopub.status.busy": "2025-11-13T15:02:24.715451Z",
     "iopub.status.idle": "2025-11-13T15:02:35.872176Z",
     "shell.execute_reply": "2025-11-13T15:02:35.870958Z"
    },
    "papermill": {
     "duration": 11.168354,
     "end_time": "2025-11-13T15:02:35.873745",
     "exception": false,
     "start_time": "2025-11-13T15:02:24.705391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4514, 6)\n",
      "(98401, 2)\n",
      "(55104, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "summary = pd.read_csv('/kaggle/input/news-summary/news_summary.csv', encoding='iso-8859-1')\n",
    "summary_more = pd.read_csv('/kaggle/input/news-summary/news_summary_more.csv', encoding='iso-8859-1')\n",
    "summary_ind = pd.read_excel(\"/kaggle/input/inshorts-news-data/Inshorts Cleaned Data.xlsx\")\n",
    "\n",
    "summary['text'] = '<news_summary> ' + summary['text']\n",
    "summary_more['text'] = '<news_summary_more> ' + summary_more['text']\n",
    "summary_ind['Short'] = '<news_ind> ' + summary_ind['Short']\n",
    "\n",
    "print(summary.shape)\n",
    "print(summary_more.shape)\n",
    "print(summary_ind.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b506d26a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:02:35.893642Z",
     "iopub.status.busy": "2025-11-13T15:02:35.893208Z",
     "iopub.status.idle": "2025-11-13T15:02:35.919403Z",
     "shell.execute_reply": "2025-11-13T15:02:35.918371Z"
    },
    "papermill": {
     "duration": 0.038501,
     "end_time": "2025-11-13T15:02:35.921175",
     "exception": false,
     "start_time": "2025-11-13T15:02:35.882674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>headlines</th>\n",
       "      <th>read_more</th>\n",
       "      <th>text</th>\n",
       "      <th>ctext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chhavi Tyagi</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Daman &amp; Diu revokes mandatory Rakshabandhan in offices order</td>\n",
       "      <td>http://www.hindustantimes.com/india-news/rakshabandhan-compulsory-in-daman-and-diu-women-employees-to-tie-rakhis-to-male-colleagues/story-E5h5U1ZDJii5zFpLXWRkhJ.html?utm_source=inshorts&amp;utm_medium=referral&amp;utm_campaign=fullarticle</td>\n",
       "      <td>&lt;news_summary&gt; The Administration of Union Territory Daman and Diu has revoked its order that made it compulsory for women to tie rakhis to their male colleagues on the occasion of Rakshabandhan on August 7. The administration was forced to withdraw the decision within 24 hours of issuing the circular after it received flak from employees and was slammed on social media.</td>\n",
       "      <td>The Daman and Diu administration on Wednesday withdrew a circular that asked women staff to tie rakhis on male colleagues after the order triggered a backlash from employees and was ripped apart on social media.The union territory?s administration was forced to retreat within 24 hours of issuing the circular that made it compulsory for its staff to celebrate Rakshabandhan at workplace.?It has been decided to celebrate the festival of Rakshabandhan on August 7. In this connection, all offices/ departments shall remain open and celebrate the festival collectively at a suitable time wherein all the lady staff shall tie rakhis to their colleagues,? the order, issued on August 1 by Gurpreet Singh, deputy secretary (personnel), had said.To ensure that no one skipped office, an attendance report was to be sent to the government the next evening.The two notifications ? one mandating the celebration of Rakshabandhan (left) and the other withdrawing the mandate (right) ? were issued by the Daman and Diu administration a day apart. The circular was withdrawn through a one-line order issued late in the evening by the UT?s department of personnel and administrative reforms.?The circular is ridiculous. There are sensitivities involved. How can the government dictate who I should tie rakhi to? We should maintain the professionalism of a workplace? an official told Hindustan Times earlier in the day. She refused to be identified.The notice was issued on Daman and Diu administrator and former Gujarat home minister Praful Kodabhai Patel?s direction, sources said.Rakshabandhan, a celebration of the bond between brothers and sisters, is one of several Hindu festivities and rituals that are no longer confined of private, family affairs but have become tools to push politic al ideologies.In 2014, the year BJP stormed to power at the Centre, Rashtriya Swayamsevak Sangh (RSS) chief Mohan Bhagwat said the festival had ?national significance? and should be celebrated widely ?to protect Hindu culture and live by the values enshrined in it?. The RSS is the ideological parent of the ruling BJP.Last year, women ministers in the Modi government went to the border areas to celebrate the festival with soldiers. A year before, all cabinet ministers were asked to go to their constituencies for the festival.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author                  date  \\\n",
       "0  Chhavi Tyagi  03 Aug 2017,Thursday   \n",
       "\n",
       "                                                      headlines  \\\n",
       "0  Daman & Diu revokes mandatory Rakshabandhan in offices order   \n",
       "\n",
       "                                                                                                                                                                                                                                 read_more  \\\n",
       "0  http://www.hindustantimes.com/india-news/rakshabandhan-compulsory-in-daman-and-diu-women-employees-to-tie-rakhis-to-male-colleagues/story-E5h5U1ZDJii5zFpLXWRkhJ.html?utm_source=inshorts&utm_medium=referral&utm_campaign=fullarticle    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                    text  \\\n",
       "0  <news_summary> The Administration of Union Territory Daman and Diu has revoked its order that made it compulsory for women to tie rakhis to their male colleagues on the occasion of Rakshabandhan on August 7. The administration was forced to withdraw the decision within 24 hours of issuing the circular after it received flak from employees and was slammed on social media.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ctext  \n",
       "0  The Daman and Diu administration on Wednesday withdrew a circular that asked women staff to tie rakhis on male colleagues after the order triggered a backlash from employees and was ripped apart on social media.The union territory?s administration was forced to retreat within 24 hours of issuing the circular that made it compulsory for its staff to celebrate Rakshabandhan at workplace.?It has been decided to celebrate the festival of Rakshabandhan on August 7. In this connection, all offices/ departments shall remain open and celebrate the festival collectively at a suitable time wherein all the lady staff shall tie rakhis to their colleagues,? the order, issued on August 1 by Gurpreet Singh, deputy secretary (personnel), had said.To ensure that no one skipped office, an attendance report was to be sent to the government the next evening.The two notifications ? one mandating the celebration of Rakshabandhan (left) and the other withdrawing the mandate (right) ? were issued by the Daman and Diu administration a day apart. The circular was withdrawn through a one-line order issued late in the evening by the UT?s department of personnel and administrative reforms.?The circular is ridiculous. There are sensitivities involved. How can the government dictate who I should tie rakhi to? We should maintain the professionalism of a workplace? an official told Hindustan Times earlier in the day. She refused to be identified.The notice was issued on Daman and Diu administrator and former Gujarat home minister Praful Kodabhai Patel?s direction, sources said.Rakshabandhan, a celebration of the bond between brothers and sisters, is one of several Hindu festivities and rituals that are no longer confined of private, family affairs but have become tools to push politic al ideologies.In 2014, the year BJP stormed to power at the Centre, Rashtriya Swayamsevak Sangh (RSS) chief Mohan Bhagwat said the festival had ?national significance? and should be celebrated widely ?to protect Hindu culture and live by the values enshrined in it?. The RSS is the ideological parent of the ruling BJP.Last year, women ministers in the Modi government went to the border areas to celebrate the festival with soldiers. A year before, all cabinet ministers were asked to go to their constituencies for the festival.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abfc7139",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:02:35.940861Z",
     "iopub.status.busy": "2025-11-13T15:02:35.940540Z",
     "iopub.status.idle": "2025-11-13T15:02:35.949680Z",
     "shell.execute_reply": "2025-11-13T15:02:35.948987Z"
    },
    "papermill": {
     "duration": 0.020936,
     "end_time": "2025-11-13T15:02:35.951432",
     "exception": false,
     "start_time": "2025-11-13T15:02:35.930496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upGrad learner switches to career in ML &amp; Al with 90% salary hike</td>\n",
       "      <td>&lt;news_summary_more&gt; Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           headlines  \\\n",
       "0  upGrad learner switches to career in ML & Al with 90% salary hike   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                          text  \n",
       "0  <news_summary_more> Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_more.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3b061fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:02:35.971359Z",
     "iopub.status.busy": "2025-11-13T15:02:35.971049Z",
     "iopub.status.idle": "2025-11-13T15:02:35.984198Z",
     "shell.execute_reply": "2025-11-13T15:02:35.983140Z"
    },
    "papermill": {
     "duration": 0.024727,
     "end_time": "2025-11-13T15:02:35.985683",
     "exception": false,
     "start_time": "2025-11-13T15:02:35.960956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Short</th>\n",
       "      <th>Source</th>\n",
       "      <th>Time</th>\n",
       "      <th>Publish Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4 ex-bank officials booked for cheating bank of ₹209 crore</td>\n",
       "      <td>&lt;news_ind&gt; The CBI on Saturday booked four former officials of Syndicate Bank and six others for cheating, forgery, criminal conspiracy and causing ₹209 crore loss to the state-run bank. The accused had availed home loans and credit from Syndicate Bank on the basis of forged and fabricated documents. These funds were fraudulently transferred to the companies owned by the accused persons.</td>\n",
       "      <td>The New Indian Express</td>\n",
       "      <td>09:25:00</td>\n",
       "      <td>2017-03-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Headline  \\\n",
       "0  4 ex-bank officials booked for cheating bank of ₹209 crore   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                    Short  \\\n",
       "0  <news_ind> The CBI on Saturday booked four former officials of Syndicate Bank and six others for cheating, forgery, criminal conspiracy and causing ₹209 crore loss to the state-run bank. The accused had availed home loans and credit from Syndicate Bank on the basis of forged and fabricated documents. These funds were fraudulently transferred to the companies owned by the accused persons.   \n",
       "\n",
       "                  Source      Time  Publish Date  \n",
       "0  The New Indian Express  09:25:00   2017-03-26  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_ind.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caa85f88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:02:36.006383Z",
     "iopub.status.busy": "2025-11-13T15:02:36.006073Z",
     "iopub.status.idle": "2025-11-13T15:02:36.080849Z",
     "shell.execute_reply": "2025-11-13T15:02:36.079809Z"
    },
    "papermill": {
     "duration": 0.086931,
     "end_time": "2025-11-13T15:02:36.082563",
     "exception": false,
     "start_time": "2025-11-13T15:02:35.995632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158019, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;news_summary_more&gt; Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.</td>\n",
       "      <td>upGrad learner switches to career in ML &amp; Al with 90% salary hike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;news_summary_more&gt; Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.</td>\n",
       "      <td>Delhi techie wins free food from Swiggy for one year on CRED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                          text  \\\n",
       "0  <news_summary_more> Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.   \n",
       "1                              <news_summary_more> Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.   \n",
       "\n",
       "                                                             summary  \n",
       "0  upGrad learner switches to career in ML & Al with 90% salary hike  \n",
       "1       Delhi techie wins free food from Swiggy for one year on CRED  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = summary.iloc[:, 0:6]\n",
    "summary_more = summary_more.iloc[:, 0:2]\n",
    "\n",
    "# To increase the intake of possible text values to build a reliable model\n",
    "summary['text'] = (\n",
    "    summary['author'] + ' ' +\n",
    "    summary['date'] + ' ' +\n",
    "    summary['read_more'] + ' ' +\n",
    "    summary['text'] + ' ' +\n",
    "    summary['ctext']\n",
    ")\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['text'] = pd.concat([summary_more['text'], summary['text'], summary_ind['Short']], ignore_index=True)\n",
    "df['summary'] = pd.concat([summary_more['headlines'], summary['headlines'], summary_ind['Headline']], ignore_index=True)\n",
    "\n",
    "df_trans = df\n",
    "\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb71db1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:02:36.103410Z",
     "iopub.status.busy": "2025-11-13T15:02:36.103102Z",
     "iopub.status.idle": "2025-11-13T15:02:42.643866Z",
     "shell.execute_reply": "2025-11-13T15:02:42.642839Z"
    },
    "papermill": {
     "duration": 6.553436,
     "end_time": "2025-11-13T15:02:42.645981",
     "exception": false,
     "start_time": "2025-11-13T15:02:36.092545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'https?://\\S+', '', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  \n",
    "    return text\n",
    "\n",
    "\n",
    "# Tokenization: split text by spaces\n",
    "def tokenize_texts(texts):\n",
    "    return [' '.join(clean_text(t).split()) for t in texts]\n",
    "\n",
    "\n",
    "processed_text = tokenize_texts(df['text'])\n",
    "#processed_summary = ['_START_ ' + s + ' _END_' for s in tokenize_texts(df['summary'])]\n",
    "processed_summary = ['sostok ' + s + ' eostok' for s in tokenize_texts(df['summary'])]\n",
    "\n",
    "\n",
    "df['cleaned_text'] = pd.Series(processed_text)\n",
    "df['cleaned_summary'] = pd.Series(processed_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8a6f416",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:02:42.667407Z",
     "iopub.status.busy": "2025-11-13T15:02:42.666506Z",
     "iopub.status.idle": "2025-11-13T15:02:43.218325Z",
     "shell.execute_reply": "2025-11-13T15:02:43.217224Z"
    },
    "papermill": {
     "duration": 0.564006,
     "end_time": "2025-11-13T15:02:43.219990",
     "exception": false,
     "start_time": "2025-11-13T15:02:42.655984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN dropped: 118\n",
      "(157886, 4)\n"
     ]
    }
   ],
   "source": [
    "print(f\"NaN dropped: {df.isna().sum().sum()}\")\n",
    "\n",
    "#df = df.dropna(subset=['text'])\n",
    "df = df.dropna(subset=['cleaned_text', 'cleaned_summary'])\n",
    "df = df.drop_duplicates(subset=['cleaned_text', 'cleaned_summary'])\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "139e8bdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:02:43.240966Z",
     "iopub.status.busy": "2025-11-13T15:02:43.240580Z",
     "iopub.status.idle": "2025-11-13T15:02:44.826631Z",
     "shell.execute_reply": "2025-11-13T15:02:44.825554Z"
    },
    "papermill": {
     "duration": 1.598332,
     "end_time": "2025-11-13T15:02:44.828351",
     "exception": false,
     "start_time": "2025-11-13T15:02:43.230019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_count = []\n",
    "summary_count = []\n",
    "\n",
    "for sent in df['cleaned_text']:\n",
    "    text_count.append(len(sent.split()))\n",
    "    \n",
    "for sent in df['cleaned_summary']:\n",
    "    summary_count.append(len(sent.split()))\n",
    "\n",
    "graph_df = pd.DataFrame() \n",
    "\n",
    "graph_df['text'] = text_count\n",
    "graph_df['summary'] = summary_count\n",
    "\n",
    "graph_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2255ca0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:02:44.851159Z",
     "iopub.status.busy": "2025-11-13T15:02:44.850790Z",
     "iopub.status.idle": "2025-11-13T15:02:45.640470Z",
     "shell.execute_reply": "2025-11-13T15:02:45.639382Z"
    },
    "papermill": {
     "duration": 0.802639,
     "end_time": "2025-11-13T15:02:45.642112",
     "exception": false,
     "start_time": "2025-11-13T15:02:44.839473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of texts with less than 100 words : 0.9724991449526874\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in df['cleaned_text']:\n",
    "    if len(i.split()) <= 100:\n",
    "        cnt = cnt + 1\n",
    "print(f\"Percentage of texts with less than 100 words : {cnt / len(df['cleaned_text'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8080c9df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:02:45.666116Z",
     "iopub.status.busy": "2025-11-13T15:02:45.664773Z",
     "iopub.status.idle": "2025-11-13T15:02:48.820481Z",
     "shell.execute_reply": "2025-11-13T15:02:48.819331Z"
    },
    "papermill": {
     "duration": 3.16946,
     "end_time": "2025-11-13T15:02:48.822183",
     "exception": false,
     "start_time": "2025-11-13T15:02:45.652723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;news_summary_more&gt; Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.</td>\n",
       "      <td>upGrad learner switches to career in ML &amp; Al with 90% salary hike</td>\n",
       "      <td>&lt;news_summary_more&gt; Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.</td>\n",
       "      <td>sostok sostok upGrad learner switches to career in ML &amp; Al with 90% salary hike eostok eostok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;news_summary_more&gt; Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.</td>\n",
       "      <td>Delhi techie wins free food from Swiggy for one year on CRED</td>\n",
       "      <td>&lt;news_summary_more&gt; Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.</td>\n",
       "      <td>sostok sostok Delhi techie wins free food from Swiggy for one year on CRED eostok eostok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                          text  \\\n",
       "0  <news_summary_more> Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.   \n",
       "1                              <news_summary_more> Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.   \n",
       "\n",
       "                                                             summary  \\\n",
       "0  upGrad learner switches to career in ML & Al with 90% salary hike   \n",
       "1       Delhi techie wins free food from Swiggy for one year on CRED   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                  cleaned_text  \\\n",
       "0  <news_summary_more> Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.   \n",
       "1                              <news_summary_more> Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.   \n",
       "\n",
       "                                                                                 cleaned_summary  \n",
       "0  sostok sostok upGrad learner switches to career in ML & Al with 90% salary hike eostok eostok  \n",
       "1       sostok sostok Delhi techie wins free food from Swiggy for one year on CRED eostok eostok  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_text_len = 300\n",
    "max_summary_len = 20\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].astype(str)\n",
    "df['cleaned_summary'] = df['cleaned_summary'].astype(str)\n",
    "\n",
    "mask = (df['cleaned_text'].str.split().str.len() <= max_text_len) & \\\n",
    "       (df['cleaned_summary'].str.split().str.len() <= max_summary_len)\n",
    "\n",
    "df = df.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# Add start and end tokens to each summary\n",
    "df['cleaned_summary'] = df['cleaned_summary'].apply(lambda x: 'sostok ' + x + ' eostok')\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fccb5b",
   "metadata": {
    "papermill": {
     "duration": 0.010691,
     "end_time": "2025-11-13T15:02:48.844956",
     "exception": false,
     "start_time": "2025-11-13T15:02:48.834265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Tokenization**\n",
    "\n",
    "This block prepares the text data for a sequence-to-sequence model:\n",
    "\n",
    "- **Split dataset**: Separates `text` and `summary` into training and validation sets to evaluate model performance on unseen data.  \n",
    "- **Initialize tokenizers**: Converts words into integer indices, which neural networks can process.  \n",
    "- **Analyze rare words**: Computes the percentage of words appearing less than `thresh` times to identify infrequent words that might add noise.  \n",
    "- **Limit vocabulary to frequent words**: Reduces vocabulary size by ignoring rare words, which improves training efficiency and prevents overfitting.  \n",
    "- **Convert texts to sequences**: Maps each word in the texts to its corresponding integer index.  \n",
    "- **Pad sequences**: Ensures all sequences have the same length, necessary for batch processing in neural networks.  \n",
    "- **Compute final vocabulary size**: Includes the padding token to correctly define the input dimension for the model embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1e8d520",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:02:48.867811Z",
     "iopub.status.busy": "2025-11-13T15:02:48.867444Z",
     "iopub.status.idle": "2025-11-13T15:03:19.234925Z",
     "shell.execute_reply": "2025-11-13T15:03:19.233782Z"
    },
    "papermill": {
     "duration": 30.380965,
     "end_time": "2025-11-13T15:03:19.236663",
     "exception": false,
     "start_time": "2025-11-13T15:02:48.855698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 15:02:52.256257: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763046172.478452      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763046172.550265      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    np.array(df[\"cleaned_text\"]),\n",
    "    np.array(df[\"cleaned_summary\"]),\n",
    "    test_size=0.1,\n",
    "    random_state=0,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "x_tokenizer = Tokenizer(oov_token=\"<unk>\") \n",
    "x_tokenizer.fit_on_texts(list(x_train))\n",
    "\n",
    "y_tokenizer = Tokenizer(oov_token=\"<unk>\")   \n",
    "y_tokenizer.fit_on_texts(list(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98feae1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:03:19.318681Z",
     "iopub.status.busy": "2025-11-13T15:03:19.318087Z",
     "iopub.status.idle": "2025-11-13T15:03:19.375930Z",
     "shell.execute_reply": "2025-11-13T15:03:19.374704Z"
    },
    "papermill": {
     "duration": 0.129155,
     "end_time": "2025-11-13T15:03:19.377364",
     "exception": false,
     "start_time": "2025-11-13T15:03:19.248209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in X vocabulary: 57.14%\n",
      "% of rare words in Y vocabulary: 55.57%\n"
     ]
    }
   ],
   "source": [
    "thresh = 3\n",
    "\n",
    "def rare_word_stats(tokenizer, thresh):\n",
    "    \"\"\"Return total and rare word counts for a given tokenizer.\"\"\"\n",
    "    total_cnt = len(tokenizer.word_counts)\n",
    "    rare_cnt = sum(1 for word, count in tokenizer.word_counts.items() if count < thresh)\n",
    "    return total_cnt, rare_cnt\n",
    "\n",
    "\n",
    "x_tot_cnt, x_cnt = rare_word_stats(x_tokenizer, thresh)\n",
    "y_tot_cnt, y_cnt = rare_word_stats(y_tokenizer, thresh)\n",
    "\n",
    "print(f\"% of rare words in X vocabulary: {(x_cnt / x_tot_cnt) * 100:.2f}%\")\n",
    "print(f\"% of rare words in Y vocabulary: {(y_cnt / y_tot_cnt) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "618b05e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:03:19.401298Z",
     "iopub.status.busy": "2025-11-13T15:03:19.400578Z",
     "iopub.status.idle": "2025-11-13T15:03:41.581106Z",
     "shell.execute_reply": "2025-11-13T15:03:41.579918Z"
    },
    "papermill": {
     "duration": 22.19418,
     "end_time": "2025-11-13T15:03:41.582769",
     "exception": false,
     "start_time": "2025-11-13T15:03:19.388589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary in X = 54632\n",
      "Size of vocabulary in Y = 22973\n"
     ]
    }
   ],
   "source": [
    "# Create tokenizers considering only frequent words\n",
    "x_tokenizer = Tokenizer(num_words = x_tot_cnt - x_cnt) \n",
    "x_tokenizer.fit_on_texts(list(x_train))\n",
    "\n",
    "y_tokenizer = Tokenizer(num_words=y_tot_cnt-y_cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_train))\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "x_train_seq = x_tokenizer.texts_to_sequences(x_train) \n",
    "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "y_train_seq = y_tokenizer.texts_to_sequences(y_train) \n",
    "y_val_seq = y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "# Pad sequences\n",
    "x_train = pad_sequences(x_train_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val = pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "y_train = pad_sequences(y_train_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val = pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "# Vocab. size (+1 for padding token)\n",
    "x_voc_size = x_tokenizer.num_words + 1\n",
    "y_voc_size = y_tokenizer.num_words + 1\n",
    "\n",
    "print(f\"Size of vocabulary in X = {x_voc_size}\")\n",
    "print(f\"Size of vocabulary in Y = {y_voc_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6382d873",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:03:41.606501Z",
     "iopub.status.busy": "2025-11-13T15:03:41.606165Z",
     "iopub.status.idle": "2025-11-13T15:03:41.731020Z",
     "shell.execute_reply": "2025-11-13T15:03:41.729776Z"
    },
    "papermill": {
     "duration": 0.138623,
     "end_time": "2025-11-13T15:03:41.732656",
     "exception": false,
     "start_time": "2025-11-13T15:03:41.594033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finally remove from the dataset empty summaries that contain only the 'START' and 'END' tokens\n",
    "\n",
    "x_train = x_train[np.sum(y_train != 0, axis=1) > 2]\n",
    "y_train = y_train[np.sum(y_train != 0, axis=1) > 2]\n",
    "\n",
    "x_val = x_val[np.sum(y_val != 0, axis=1) > 2]\n",
    "y_val = y_val[np.sum(y_val != 0, axis=1) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee9a561",
   "metadata": {
    "papermill": {
     "duration": 0.011044,
     "end_time": "2025-11-13T15:03:41.754797",
     "exception": false,
     "start_time": "2025-11-13T15:03:41.743753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Seq2seq model using LSTM**\n",
    "\n",
    "### Encoder-Decoder Architecture with LSTM\n",
    "\n",
    "During training, the model takes **two inputs**:  \n",
    "1. The encoder input (`text`) – the original text sequence.  \n",
    "2. The decoder input (`summary`) – the summary shifted by one token (so that the model learns to predict the next word).  \n",
    "\n",
    "The **target output** is the summary sequence shifted forward by one token. The model learns to predict the next word in the summary based on the previous words. During inference, the trained model generates summaries one word at a time, using the previously predicted words as input.\n",
    "\n",
    "---\n",
    "\n",
    "**Encoder**  \n",
    "- The encoder accepts sequences of text with a fixed length (`max_text_len`).  \n",
    "- The text is first passed through an **Embedding layer** that maps each word index to a dense vector of size `(embedding_dim)`.  \n",
    "- The embedded sequence is then processed by **three stacked LSTM layers**:  \n",
    "  - Each layer outputs the **full sequence of hidden states** (for possible attention or stacking) and the **last hidden and cell states**.  \n",
    "  - The last hidden and cell states from the final LSTM are used to initialize the decoder.  \n",
    "- Stacking multiple LSTMs allows the encoder to **capture both local patterns and long-range dependencies** in the text.\n",
    "\n",
    "---\n",
    "\n",
    "**Decoder**  \n",
    "- The decoder input (shifted summary) is passed through an **Embedding layer** of size `(summary vocabulary size x embedding_dim)`.  \n",
    "- A single **LSTM** processes the embedded sequence, using the **encoder's last hidden and cell states** as its initial state.  \n",
    "- The LSTM output is passed through a **TimeDistributed Dense layer** with **softmax activation**, which predicts the probability of each word in the vocabulary at each time step.  \n",
    "\n",
    "This architecture ensures that the decoder can generate the summary step by step, **learning the sequence of words conditioned on the input text**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "105c1d62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:03:41.777780Z",
     "iopub.status.busy": "2025-11-13T15:03:41.777459Z",
     "iopub.status.idle": "2025-11-13T15:03:42.354014Z",
     "shell.execute_reply": "2025-11-13T15:03:42.352945Z"
    },
    "papermill": {
     "duration": 0.589946,
     "end_time": "2025-11-13T15:03:42.355589",
     "exception": false,
     "start_time": "2025-11-13T15:03:41.765643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 15:03:41.797819: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10,926,400</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">601,200</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">721,200</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,594,600</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">721,200</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">601,200</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">6,914,873</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">22973</span>)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m200\u001b[0m)  │ \u001b[38;5;34m10,926,400\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m,      │    \u001b[38;5;34m601,200\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m300\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m,      │    \u001b[38;5;34m721,200\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m300\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m) │  \u001b[38;5;34m4,594,600\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m,      │    \u001b[38;5;34m721,200\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m300\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m601,200\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n",
       "│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n",
       "│                     │ \u001b[38;5;34m300\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m6,914,873\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m22973\u001b[0m)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,080,673</span> (95.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,080,673\u001b[0m (95.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,080,673</span> (95.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,080,673\u001b[0m (95.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim = 200\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len, ))\n",
    "\n",
    "# Embedding layer\n",
    "enc_emb = Embedding(x_voc_size, embedding_dim,\n",
    "                    trainable=True)(encoder_inputs)\n",
    "\n",
    "# Encoder LSTM 1\n",
    "encoder_lstm1 = LSTM(latent_dim, return_sequences=True,\n",
    "                     return_state=True, dropout=0.4,\n",
    "                     recurrent_dropout=0.4)\n",
    "(encoder_output1, state_h1, state_c1) = encoder_lstm1(enc_emb)\n",
    "\n",
    "# Encoder LSTM 2\n",
    "encoder_lstm2 = LSTM(latent_dim, return_sequences=True,\n",
    "                     return_state=True, dropout=0.4,\n",
    "                     recurrent_dropout=0.4)\n",
    "(encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# Encoder LSTM 3\n",
    "encoder_lstm3 = LSTM(latent_dim, return_state=True,\n",
    "                     return_sequences=True, dropout=0.4,\n",
    "                     recurrent_dropout=0.4)\n",
    "(encoder_outputs, state_h, state_c) = encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None, ))\n",
    "\n",
    "# Embedding layer\n",
    "dec_emb_layer = Embedding(y_voc_size, embedding_dim, trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# Decoder LSTM\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True,\n",
    "                    return_state=True, dropout=0.4,\n",
    "                    recurrent_dropout=0.2)\n",
    "(decoder_outputs, decoder_fwd_state, decoder_back_state) = \\\n",
    "    decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bc3fb1",
   "metadata": {
    "papermill": {
     "duration": 0.011509,
     "end_time": "2025-11-13T15:03:42.379111",
     "exception": false,
     "start_time": "2025-11-13T15:03:42.367602",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Training the model** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bdca3eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:03:42.404379Z",
     "iopub.status.busy": "2025-11-13T15:03:42.404049Z",
     "iopub.status.idle": "2025-11-13T15:03:44.429529Z",
     "shell.execute_reply": "2025-11-13T15:03:44.428576Z"
    },
    "papermill": {
     "duration": 2.040005,
     "end_time": "2025-11-13T15:03:44.431312",
     "exception": false,
     "start_time": "2025-11-13T15:03:42.391307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'loss_scale_optimizer', because it has 22 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 18 variables whereas the saved optimizer has 0 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "x_train_lstm = x_train\n",
    "x_val_lstm = x_val\n",
    "y_train_lstm = y_train\n",
    "y_val_lstm = y_val\n",
    "\n",
    "MODEL_INPUT_PATH = \"/kaggle/input/lstm-keras-summarization/keras/default/1/model_lstm.keras\" \n",
    "MODEL_PATH = \"/kaggle/working/model_lstm.keras\" \n",
    "\n",
    "# retrain the model\n",
    "if os.path.exists(MODEL_INPUT_PATH):\n",
    "    model = load_model(MODEL_INPUT_PATH)\n",
    "\n",
    "else:\n",
    "    model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "\n",
    "    history = model.fit(\n",
    "        [x_train, y_train[:, :-1]],\n",
    "        y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:, 1:],\n",
    "        epochs=50,\n",
    "        callbacks=[es],\n",
    "        batch_size=128,\n",
    "        validation_data=(\n",
    "            [x_val, y_val[:, :-1]],\n",
    "            y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:, 1:]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.save(MODEL_PATH)\n",
    "\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d1ef2c",
   "metadata": {
    "papermill": {
     "duration": 0.011409,
     "end_time": "2025-11-13T15:03:44.454629",
     "exception": false,
     "start_time": "2025-11-13T15:03:44.443220",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Predict**\n",
    "\n",
    "Once the Seq2Seq model has been trained, we can use it to **generate summaries** for new input texts.  \n",
    "This stage is known as **inference**, the model no longer learns, but uses its learned parameters to predict the most likely output sequence (summary).\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Preparing the Mapping Dictionaries\n",
    "Before generating predictions, we rebuild the word–token mappings from the tokenizers:\n",
    "- `reverse_x_word_index`: converts article tokens → words  \n",
    "- `reverse_y_word_index`: converts summary tokens → words  \n",
    "- `y_word_index`: converts summary words → tokens  \n",
    "\n",
    "These dictionaries let us translate between the model’s numeric predictions and readable text.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Building Inference Models\n",
    "During training, the encoder and decoder work together in a single model.  \n",
    "At inference time, we separate them:\n",
    "\n",
    "- The **encoder model** processes the input text once and produces context vectors (`encoder_outputs`, `state_h`, `state_c`) — a compressed representation of the input.  \n",
    "- The **decoder model** generates the summary **one word at a time**, taking as input the previous word and its previous internal states.\n",
    "\n",
    "This setup allows the decoder to iteratively predict each next token until the end-of-sequence marker (`eostok`) is reached.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. The Decoding Process\n",
    "The function `decode_sequence()` handles the actual text generation:\n",
    "\n",
    "1. **Encode the input sequence** using the encoder model to obtain its internal states.  \n",
    "2. **Initialize** the decoder with the special start token (`sostok`).  \n",
    "3. **Iteratively predict** the next word:\n",
    "   - Feed the previous word and the latest decoder states into the model.\n",
    "   - Pick the word with the highest probability (`argmax`).\n",
    "   - Stop when the `eostok` token is predicted or the maximum summary length is reached.\n",
    "4. **Concatenate** all predicted tokens into a readable summary.\n",
    "\n",
    "This process simulates how the model “writes” one word at a time, using its internal memory to maintain context.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Converting Sequences to Text\n",
    "Two helper functions make the predictions human-readable:\n",
    "- `seq2text()` converts numeric article sequences back into words.\n",
    "- `seq2summary()` converts numeric summary sequences back into words, excluding special tokens (`sostok`, `eostok`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab19da3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:03:44.479186Z",
     "iopub.status.busy": "2025-11-13T15:03:44.478819Z",
     "iopub.status.idle": "2025-11-13T15:03:44.483326Z",
     "shell.execute_reply": "2025-11-13T15:03:44.482474Z"
    },
    "papermill": {
     "duration": 0.018468,
     "end_time": "2025-11-13T15:03:44.484757",
     "exception": false,
     "start_time": "2025-11-13T15:03:44.466289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reverse_y_word_index: summary token → word\n",
    "# reverse_x_word_index: article token → word\n",
    "# y_word_index: summary word → token\n",
    "\n",
    "reverse_y_word_index = y_tokenizer.index_word\n",
    "reverse_x_word_index = x_tokenizer.index_word\n",
    "y_word_index = y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82450231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:03:44.510337Z",
     "iopub.status.busy": "2025-11-13T15:03:44.509985Z",
     "iopub.status.idle": "2025-11-13T15:03:44.526091Z",
     "shell.execute_reply": "2025-11-13T15:03:44.525169Z"
    },
    "papermill": {
     "duration": 0.031119,
     "end_time": "2025-11-13T15:03:44.527698",
     "exception": false,
     "start_time": "2025-11-13T15:03:44.496579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inference Models\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs,\n",
    "                      state_h, state_c])\n",
    "\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim, ))\n",
    "decoder_state_input_c = Input(shape=(latent_dim, ))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "(decoder_outputs2, state_h2, state_c2) = decoder_lstm(dec_emb2,\n",
    "        initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "# Final decoder model\n",
    "# decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n",
    "#                       decoder_state_input_h, decoder_state_input_c],\n",
    "#                       [decoder_outputs2] + [state_h2, state_c2])\n",
    "#TPU error\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2, state_h2, state_c2]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dc424c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:03:44.553194Z",
     "iopub.status.busy": "2025-11-13T15:03:44.552724Z",
     "iopub.status.idle": "2025-11-13T15:03:44.562518Z",
     "shell.execute_reply": "2025-11-13T15:03:44.561435Z"
    },
    "papermill": {
     "duration": 0.024364,
     "end_time": "2025-11-13T15:03:44.564069",
     "exception": false,
     "start_time": "2025-11-13T15:03:44.539705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert sequence to summary\n",
    "def seq2summary(input_seq):\n",
    "    newString = ''\n",
    "    for i in input_seq:\n",
    "        if i != 0 and i != y_word_index['sostok'] and i \\\n",
    "            != y_word_index['eostok']:\n",
    "            newString = newString + reverse_y_word_index[i] + ' '\n",
    "\n",
    "    return newString\n",
    "\n",
    "# Convert sequence to text\n",
    "def seq2text(input_seq):\n",
    "    newString = ''\n",
    "    for i in input_seq:\n",
    "        if i != 0:\n",
    "            newString = newString + reverse_x_word_index[i] + ' '\n",
    "\n",
    "    return newString\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "\n",
    "    # Encode the input as state vectors.\n",
    "    (e_out, e_h, e_c) = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    # Generate empty target sequence of length 1\n",
    "    y_seq = np.zeros((1, 1))\n",
    "\n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    y_seq[0, 0] = y_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        (output_tokens, h, c) = decoder_model.predict([y_seq]\n",
    "                + [e_out, e_h, e_c], verbose=0)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_y_word_index[sampled_token_index]\n",
    "\n",
    "        if sampled_token != 'eostok':\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find the stop word.\n",
    "        if sampled_token == 'eostok' or len(decoded_sentence.split()) \\\n",
    "            >= max_summary_len - 1:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1)\n",
    "        y_seq = np.zeros((1, 1))\n",
    "        y_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        (e_h, e_c) = (h, c)\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a64a397",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:03:44.589446Z",
     "iopub.status.busy": "2025-11-13T15:03:44.589117Z",
     "iopub.status.idle": "2025-11-13T15:03:44.593933Z",
     "shell.execute_reply": "2025-11-13T15:03:44.592848Z"
    },
    "papermill": {
     "duration": 0.019583,
     "end_time": "2025-11-13T15:03:44.595769",
     "exception": false,
     "start_time": "2025-11-13T15:03:44.576186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in range(0, 5):\n",
    "#     print ('Review:', seq2text(x_train[i]))\n",
    "#     print ('Original summary:', seq2summary(y_train[i]))\n",
    "#     print ('Predicted summary:', decode_sequence(x_train[i].reshape(1, max_text_len)))\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc61fd",
   "metadata": {
    "papermill": {
     "duration": 0.013109,
     "end_time": "2025-11-13T15:03:44.620970",
     "exception": false,
     "start_time": "2025-11-13T15:03:44.607861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transformer Model with Self-Attention\n",
    "\n",
    "Similar to the Seq2Seq architecture, the Transformer follows an **encoder–decoder structure**, but instead of recurrent layers it relies entirely on **Multi-Head Self-Attention**.  \n",
    "This allows the model to process all tokens **in parallel** and learn relationships between words regardless of their distance in the sequence.\n",
    "\n",
    "During training, the model takes **two inputs**:  \n",
    "1. The encoder input (`text`) – the tokenized article.  \n",
    "2. The decoder input (`summary`) – the summary shifted by one token.  \n",
    "\n",
    "The **target output** is the summary shifted forward by one position. The decoder learns to predict each word based on the previously generated ones and the encoded representation of the full text.\n",
    "\n",
    "---\n",
    "\n",
    "**Encoder**  \n",
    "- The input article sequence (`max_text_len`) is first transformed using an **Embedding layer**.  \n",
    "- A **Positional Encoding** is added to preserve the order of words (since attention has no notion of sequence order by itself).  \n",
    "- The embedded input is processed by one or more **Multi-Head Self-Attention** blocks:\n",
    "  - Each word attends to **all other words** in the input\n",
    "  - Relationships between distant tokens are captured more effectively than in RNNs  \n",
    "- A **Feed-Forward Network** (FFN) refines the contextual representations.\n",
    "- **Residual connections** and **Layer Normalization** improve gradient flow and training stability.\n",
    "\n",
    "---\n",
    "\n",
    "**Decoder**  \n",
    "- Similar positional embeddings are applied to the shifted summary tokens.  \n",
    "- The decoder uses two attention mechanisms:\n",
    "  1. **Masked Self-Attention**: ensures the model cannot “peek” at future words when predicting the next token.\n",
    "  2. **Encoder-Decoder Attention**: allows the decoder to focus on relevant parts of the input article.\n",
    "- A **Feed-Forward Network** further processes the attended features.\n",
    "- A final **Dense layer with Softmax** outputs a probability distribution over all words in the vocabulary at each time step.\n",
    "\n",
    "---\n",
    "\n",
    "**sostok and eostok**  \n",
    "In sequence-to-sequence tasks such as abstractive text summarization, **special tokens** are essential for controlling how a model generates text:\n",
    "\n",
    "- `<sostok>` → marks the **start** of the output sequence  \n",
    "- `<eostok>` → marks the **end** of the sequence  \n",
    "\n",
    "However, these tokens **do not** play the same role during training across different architectures.\n",
    "\n",
    "Transformers use **masked self-attention** in the decoder, meaning that at time *t* the model can only attend to **previous tokens**.\n",
    "Therefore:\n",
    "\n",
    "- `<sostok>` must be present **only in the decoder input**  \n",
    "- `<sostok>` must be removed from the decoder target  \n",
    "\n",
    "Predicting a start token would make no sense and causes failure modes such as:\n",
    "\n",
    "- the model repeatedly outputting `<sostok>`\n",
    "- inability to begin sequences with meaningful content\n",
    "\n",
    "The EOS token **must remain in the targets**, because:\n",
    "\n",
    "- it teaches the model **when to stop writing**\n",
    "- without it, generation may become too long or infinite\n",
    "\n",
    "LSTM encoder-decoder models:\n",
    "\n",
    "- receive the final hidden state as initial context\n",
    "- do **not** use masked attention\n",
    "- often ignore the first timestep in loss computation\n",
    "\n",
    "So `<sostok>` in targets is less harmful there.\n",
    "\n",
    "---\n",
    "\n",
    "Thanks to the Self-Attention mechanism, Transformers **capture global context efficiently** and typically produce **more coherent and fluent summaries**, especially for longer texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e58e9be",
   "metadata": {
    "papermill": {
     "duration": 0.011624,
     "end_time": "2025-11-13T15:03:44.644997",
     "exception": false,
     "start_time": "2025-11-13T15:03:44.633373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Preparing Transformer Inputs\n",
    "\n",
    "To train the Transformer in an encoder–decoder setup, we need to properly structure the input data:\n",
    "\n",
    "- The **encoder input** is the full tokenized article (`x_train`)\n",
    "- The **decoder input** is the summary sequence **shifted right**, starting with `<sostok>`\n",
    "- The **decoder target** is the same summary **shifted left**, ending with `<eostok>`\n",
    "\n",
    "This shifting ensures that at each timestep the decoder learns to predict the **next** word using:\n",
    "1. The previously processed summary tokens  \n",
    "2. Attention over the encoder output  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeefc97a",
   "metadata": {
    "papermill": {
     "duration": 0.011562,
     "end_time": "2025-11-13T15:03:44.668262",
     "exception": false,
     "start_time": "2025-11-13T15:03:44.656700",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In text summarization, token-level accuracy can be misleading because it only measures whether each predicted token matches the ground truth at the same position. It does not capture semantic meaning, fluency, word order, or relevance, and it can be inflated by common tokens like padding or start/end markers. A model can have high accuracy while producing poor summaries. Better evaluation uses metrics like ROUGE-1, ROUGE-2, and ROUGE-L, which measure overlap of unigrams, bigrams, and longest common subsequences between generated and reference summaries. During training, it is better to monitor validation loss and evaluate summaries qualitatively or with ROUGE rather than relying on token accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab39298",
   "metadata": {
    "papermill": {
     "duration": 0.011475,
     "end_time": "2025-11-13T15:03:44.691559",
     "exception": false,
     "start_time": "2025-11-13T15:03:44.680084",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243a995f",
   "metadata": {
    "papermill": {
     "duration": 0.011464,
     "end_time": "2025-11-13T15:03:44.714711",
     "exception": false,
     "start_time": "2025-11-13T15:03:44.703247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Note importanti:\n",
    "\n",
    "Look-ahead mask a inference non serve se generi un token alla volta (greedy decoding step-by-step).\n",
    "\n",
    "Padding mask dell’encoder serve al decoder per ignorare i pad token dell’input.\n",
    "\n",
    "Quando fai l’inference dovrai generare token uno per uno, aggiornando dec_input_inf ad ogni step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a538e87f",
   "metadata": {
    "papermill": {
     "duration": 0.011518,
     "end_time": "2025-11-13T15:03:44.738046",
     "exception": false,
     "start_time": "2025-11-13T15:03:44.726528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Generated summary: ripete continuamente parole (cannot cannot cannot, power power power…) → questo è un loop di ripetizione, tipico dei modelli seq2seq che non hanno abbastanza regolarizzazione sulla generazione.\n",
    "\n",
    "Generated beam search summary: testo quasi completamente fuori tema → indica che il modello non ha appreso bene il contenuto semantico e il beam search amplifica le frasi che appaiono più “probabili” a livello di token, ma non corrette.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea5885b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:03:44.763054Z",
     "iopub.status.busy": "2025-11-13T15:03:44.762307Z",
     "iopub.status.idle": "2025-11-13T15:03:44.766906Z",
     "shell.execute_reply": "2025-11-13T15:03:44.766028Z"
    },
    "papermill": {
     "duration": 0.018873,
     "end_time": "2025-11-13T15:03:44.768631",
     "exception": false,
     "start_time": "2025-11-13T15:03:44.749758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_trans.head(1)\n",
    "df_tmp = df_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2446aaad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:03:44.794152Z",
     "iopub.status.busy": "2025-11-13T15:03:44.793435Z",
     "iopub.status.idle": "2025-11-13T15:04:16.866595Z",
     "shell.execute_reply": "2025-11-13T15:04:16.865616Z"
    },
    "papermill": {
     "duration": 32.098767,
     "end_time": "2025-11-13T15:04:16.879361",
     "exception": false,
     "start_time": "2025-11-13T15:03:44.780594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary in X = 133951\n",
      "Size of vocabulary in Y = 54074\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "df_trans = df_tmp\n",
    "\n",
    "# Data processing\n",
    "df_trans['cleaned_text'] = df_trans['text'].apply(clean_text)\n",
    "df_trans['cleaned_summary'] = df_trans['summary'].apply(clean_text)\n",
    "\n",
    "df_trans = df_trans.dropna(subset=['cleaned_text', 'cleaned_summary'])\n",
    "df_trans = df_trans.drop_duplicates(subset=['cleaned_text', 'cleaned_summary'])\n",
    "\n",
    "#df_trans['cleaned_summary'] = df_trans['cleaned_summary'].apply(lambda x: '<SOS> ' + x + ' <EOS>')\n",
    "df_trans['cleaned_summary'] = df_trans['cleaned_summary'].apply(lambda x: '<SOS> ' + x.strip() + ' <EOS>')\n",
    "\n",
    "mask = (df_trans['cleaned_text'].str.split().str.len() <= max_text_len) & \\\n",
    "       (df_trans['cleaned_summary'].str.split().str.len() <= max_summary_len)\n",
    "df_trans = df_trans.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# Tokenizer\n",
    "oov_token = '<unk>'\n",
    "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
    "\n",
    "max_vocab_text = 4000000\n",
    "max_vocab_summary = 2000000\n",
    "\n",
    "x_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=max_vocab_text, \n",
    "    oov_token=oov_token, \n",
    "    filters=filters, \n",
    "    lower=True\n",
    ")\n",
    "x_tokenizer.fit_on_texts(df_trans['cleaned_text'])\n",
    "\n",
    "y_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=max_vocab_summary, \n",
    "    oov_token=oov_token, \n",
    "    filters=filters, \n",
    "    lower=True\n",
    ")\n",
    "y_tokenizer.fit_on_texts(df_trans['cleaned_summary'])\n",
    "\n",
    "inputs = x_tokenizer.texts_to_sequences(df_trans['cleaned_text'])\n",
    "targets = y_tokenizer.texts_to_sequences(df_trans['cleaned_summary'])\n",
    "\n",
    "# Padding\n",
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=max_text_len, padding='post', truncating='post')\n",
    "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=max_summary_len, padding='post', truncating='post')\n",
    "\n",
    "inputs = tf.cast(inputs, dtype=tf.int64)\n",
    "targets = tf.cast(targets, dtype=tf.int64)\n",
    "\n",
    "# Vocab. size (+1 for padding token)\n",
    "x_voc_size = min(max_vocab_text, len(x_tokenizer.word_index) + 1)\n",
    "y_voc_size = min(max_vocab_summary, len(y_tokenizer.word_index) + 1)\n",
    "\n",
    "print(f\"Size of vocabulary in X = {x_voc_size}\")\n",
    "print(f\"Size of vocabulary in Y = {y_voc_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "020abcac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:04:16.905426Z",
     "iopub.status.busy": "2025-11-13T15:04:16.905079Z",
     "iopub.status.idle": "2025-11-13T15:04:16.952478Z",
     "shell.execute_reply": "2025-11-13T15:04:16.951143Z"
    },
    "papermill": {
     "duration": 0.062541,
     "end_time": "2025-11-13T15:04:16.954217",
     "exception": false,
     "start_time": "2025-11-13T15:04:16.891676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(10000).batch(64)\n",
    "\n",
    "def get_angles(position, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return position * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis],\n",
    "        np.arange(d_model)[np.newaxis, :],\n",
    "        d_model\n",
    "    )\n",
    "\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    #dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    dk = tf.cast(tf.shape(k)[-1], q.dtype)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(concat_attention)\n",
    "            \n",
    "        return output, attention_weights\n",
    "    \n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])\n",
    "\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "    \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "        return out2\n",
    "\n",
    "\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "    \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "    \n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "    \n",
    "        x = self.embedding(x)\n",
    "        #x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        #x += self.pos_encoding[:, :seq_len, :]\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, x.dtype)) \n",
    "        x += tf.cast(self.pos_encoding[:, :seq_len, :], x.dtype)\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "        \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "    \n",
    "        x = self.embedding(x)\n",
    "        #x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        #x += self.pos_encoding[:, :seq_len, :]\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, x.dtype))              \n",
    "        x += tf.cast(self.pos_encoding[:, :seq_len, :], x.dtype)      \n",
    "\n",
    "    \n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](\n",
    "                x, \n",
    "                enc_output, \n",
    "                training=training, \n",
    "                look_ahead_mask=look_ahead_mask, \n",
    "                padding_mask=padding_mask\n",
    "            )\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "    \n",
    "        return x, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training=False, enc_padding_mask=None, look_ahead_mask=None, dec_padding_mask=None):\n",
    "        enc_output = self.encoder(x=inp, training=training, mask=enc_padding_mask)\n",
    "        dec_output, attention_weights = self.decoder(x=tar, enc_output=enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1b45c7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:04:16.980151Z",
     "iopub.status.busy": "2025-11-13T15:04:16.979760Z",
     "iopub.status.idle": "2025-11-13T15:04:16.991925Z",
     "shell.execute_reply": "2025-11-13T15:04:16.991006Z"
    },
    "papermill": {
     "duration": 0.026821,
     "end_time": "2025-11-13T15:04:16.993640",
     "exception": false,
     "start_time": "2025-11-13T15:04:16.966819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 256\n",
    "dff = 1024\n",
    "num_heads = 8\n",
    "dropout_rate = 0.2\n",
    "\n",
    "num_layers = 2       \n",
    "d_model = 128        \n",
    "dff = 512            \n",
    "num_heads = 4        \n",
    "dropout_rate = 0.1 \n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32) \n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4d9bac5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:04:17.020054Z",
     "iopub.status.busy": "2025-11-13T15:04:17.019707Z",
     "iopub.status.idle": "2025-11-13T15:04:17.028336Z",
     "shell.execute_reply": "2025-11-13T15:04:17.027288Z"
    },
    "papermill": {
     "duration": 0.024254,
     "end_time": "2025-11-13T15:04:17.030105",
     "exception": false,
     "start_time": "2025-11-13T15:04:17.005851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "    #accuracies = tf.cast(accuracies, dtype= tf.float32)\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1b55643",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:04:17.056446Z",
     "iopub.status.busy": "2025-11-13T15:04:17.056106Z",
     "iopub.status.idle": "2025-11-13T15:04:21.595785Z",
     "shell.execute_reply": "2025-11-13T15:04:21.594765Z"
    },
    "papermill": {
     "duration": 4.554535,
     "end_time": "2025-11-13T15:04:21.597519",
     "exception": false,
     "start_time": "2025-11-13T15:04:17.042984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)               │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">17,542,272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)               │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,450,624</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54074</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,975,546</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder (\u001b[38;5;33mEncoder\u001b[0m)               │ ?                      │    \u001b[38;5;34m17,542,272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder (\u001b[38;5;33mDecoder\u001b[0m)               │ ?                      │     \u001b[38;5;34m7,450,624\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m54074\u001b[0m)         │     \u001b[38;5;34m6,975,546\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,968,442</span> (121.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31,968,442\u001b[0m (121.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,968,442</span> (121.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31,968,442\u001b[0m (121.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=x_voc_size,\n",
    "    target_vocab_size=y_voc_size,\n",
    "    pe_input=1000,\n",
    "    pe_target=1000,\n",
    "    rate=dropout_rate\n",
    ")\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "\n",
    "checkpoint_path = \"/kaggle/working/checkpoints\" \n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "# if ckpt_manager.latest_checkpoint:\n",
    "#     ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "#     print(f'Checkpoint restored from {ckpt_manager.latest_checkpoint}')\n",
    "\n",
    "\n",
    "\n",
    "dummy_input = tf.constant([[1]*max_text_len], dtype=tf.int64)\n",
    "dummy_target = tf.constant([[1]*max_summary_len], dtype=tf.int64)\n",
    "pred, _ = transformer(dummy_input, dummy_target, training=False)\n",
    "transformer.summary()\n",
    "\n",
    "\n",
    "\n",
    "# ckpt = tf.train.Checkpoint(transformer=transformer)\n",
    "# ckpt.restore(\"/kaggle/input/trans-chpt30/ckpt-30\").assert_existing_objects_matched()\n",
    "# print(\"Checkpoint restored\")\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(\n",
    "            inp, \n",
    "            tar_inp, \n",
    "            training=True, \n",
    "            enc_padding_mask=enc_padding_mask, \n",
    "            look_ahead_mask=combined_mask, \n",
    "            dec_padding_mask=dec_padding_mask\n",
    "        )\n",
    "\n",
    "\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_accuracy.update_state(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee97f766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T15:04:21.624586Z",
     "iopub.status.busy": "2025-11-13T15:04:21.623775Z",
     "iopub.status.idle": "2025-11-13T21:00:26.026931Z",
     "shell.execute_reply": "2025-11-13T21:00:26.025741Z"
    },
    "papermill": {
     "duration": 21364.43345,
     "end_time": "2025-11-13T21:00:26.043706",
     "exception": false,
     "start_time": "2025-11-13T15:04:21.610256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/numpy_compat.py:47: RuntimeWarning: overflow encountered in cast\n",
      "  return np.array(values, copy=copy, order=order).astype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Accuracy 0.0000 Loss nan Time 21364.39 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patience = 3  \n",
    "best_loss = float('inf')\n",
    "wait = 0\n",
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    train_loss.reset_state()\n",
    "    train_accuracy.reset_state()\n",
    "\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        train_step(inp, tar)\n",
    "      \n",
    "    current_loss = train_loss.result()\n",
    "    \n",
    "    if current_loss < best_loss:\n",
    "        best_loss = current_loss\n",
    "        wait = 0\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Accuracy {train_accuracy.result():.4f} Loss {current_loss:.4f} Time {time.time() - start:.2f} seconds\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5a7c80e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T21:00:26.071049Z",
     "iopub.status.busy": "2025-11-13T21:00:26.070628Z",
     "iopub.status.idle": "2025-11-13T21:00:26.085219Z",
     "shell.execute_reply": "2025-11-13T21:00:26.083860Z"
    },
    "papermill": {
     "duration": 0.030838,
     "end_time": "2025-11-13T21:00:26.087315",
     "exception": false,
     "start_time": "2025-11-13T21:00:26.056477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize(input_article, beam_width=3):\n",
    "    input_article = x_tokenizer.texts_to_sequences([input_article])\n",
    "    input_article = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        input_article, maxlen=max_text_len, padding='post', truncating='post')\n",
    "    encoder_input = tf.expand_dims(input_article[0], 0)\n",
    "\n",
    "    sequences = [([y_tokenizer.word_index['<sos>']], 0.0)]\n",
    "\n",
    "    completed_sequences = []\n",
    "\n",
    "    for _ in range(max_summary_len):\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            if seq[-1] == y_tokenizer.word_index['<eos>']:\n",
    "                completed_sequences.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            output = tf.expand_dims(seq, 0)\n",
    "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "\n",
    "            predictions, _ = transformer(\n",
    "                encoder_input, \n",
    "                output,\n",
    "                training=False,\n",
    "                enc_padding_mask=enc_padding_mask,\n",
    "                look_ahead_mask=combined_mask,\n",
    "                dec_padding_mask=dec_padding_mask\n",
    "            )\n",
    "\n",
    "            logits = predictions[:, -1, :]\n",
    "            log_probs = tf.math.log(tf.nn.softmax(logits))\n",
    "            top_k = tf.math.top_k(log_probs, k=beam_width)\n",
    "\n",
    "            for i in range(beam_width):\n",
    "                token = int(top_k.indices[0, i])\n",
    "                candidate_score = score + float(top_k.values[0, i])\n",
    "                candidate_seq = seq + [token]\n",
    "                all_candidates.append((candidate_seq, candidate_score))\n",
    "\n",
    "        sequences = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)[:beam_width]\n",
    "\n",
    "        if not sequences:\n",
    "            break\n",
    "\n",
    "    completed_sequences.extend(sequences)\n",
    "    best_seq = max(completed_sequences, key=lambda tup: tup[1])[0]\n",
    "\n",
    "    return y_tokenizer.sequences_to_texts([best_seq])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1aa0273d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T21:00:26.258049Z",
     "iopub.status.busy": "2025-11-13T21:00:26.257663Z",
     "iopub.status.idle": "2025-11-13T21:02:27.126023Z",
     "shell.execute_reply": "2025-11-13T21:02:27.124767Z"
    },
    "papermill": {
     "duration": 120.884442,
     "end_time": "2025-11-13T21:02:27.127723",
     "exception": false,
     "start_time": "2025-11-13T21:00:26.243281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text : <news_summary_more> Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers. \n",
      "Real Headline : <SOS> upGrad learner switches to career in ML & Al with 90% salary hike <EOS> \n",
      "Summary : <sos> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Text : <news_summary_more> Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more. \n",
      "Real Headline : <SOS> Delhi techie wins free food from Swiggy for one year on CRED <EOS> \n",
      "Summary : <sos> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Text : <news_summary_more> New Zealand defeated India by 8 wickets in the fourth ODI at Hamilton on Thursday to win their first match of the five-match ODI series. India lost an international match under Rohit Sharma's captaincy after 12 consecutive victories dating back to March 2018. The match witnessed India getting all out for 92, their seventh lowest total in ODI cricket history. \n",
      "Real Headline : <SOS> New Zealand end Rohit Sharma-led India's 12-match winning streak <EOS> \n",
      "Summary : <sos> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Text : <news_summary_more> With Aegon Life iTerm Insurance plan, customers can enjoy tax benefits on your premiums paid and save up to Ã¢ÂÂ¹46,800^ on taxes. The plan provides life cover up to the age of 100 years. Also, customers have options to insure against Critical Illnesses, Disability and Accidental Death Benefit Rider with a life cover up to the age of 80 years. \n",
      "Real Headline : <SOS> Aegon life iTerm insurance plan helps customers save tax <EOS> \n",
      "Summary : <sos> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Text : <news_summary_more> Speaking about the sexual harassment allegations against Rajkumar Hirani, Sonam Kapoor said, \"I've known Hirani for many years...What if it's not true, the [#MeToo] movement will get derailed.\" \"In the #MeToo movement, I always believe a woman. But in this case, we need to reserve our judgment,\" she added. Hirani has been accused by an assistant who worked in 'Sanju'. \n",
      "Real Headline : <SOS> Have known Hirani for yrs, what if MeToo claims are not true: Sonam <EOS> \n",
      "Summary : <sos> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Text : <news_summary_more> Pakistani singer Rahat Fateh Ali Khan has denied receiving any notice from the Enforcement Directorate over allegedly smuggling foreign currency out of India. \"It would have been better if the authorities would have served the notice first if any and then publicised this,\" reads a press release issued on behalf of Rahat. The statement further called the allegation \"bizarre\". \n",
      "Real Headline : <SOS> Rahat Fateh Ali Khan denies getting notice for smuggling currency <EOS> \n",
      "Summary : <sos> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Text : <news_summary_more> India recorded their lowest ODI total in New Zealand after getting all out for 92 runs in 30.5 overs in the fourth ODI at Hamilton on Thursday. Seven of India's batsmen were dismissed for single-digit scores, while their number ten batsman Yuzvendra Chahal top-scored with 18*(37). India's previous lowest ODI total in New Zealand was 108. \n",
      "Real Headline : <SOS> India get all out for 92, their lowest ODI total in New Zealand <EOS> \n",
      "Summary : <sos> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Text : <news_summary_more> Weeks after ex-CBI Director Alok Verma told the Department of Personnel and Training to consider him retired, the Home Ministry asked him to join work on the last day of his fixed tenure as Director on Thursday. The ministry directed him to immediately join as DG, Fire Services, the post he was transferred to after his removal as CBI chief. \n",
      "Real Headline : <SOS> Govt directs Alok Verma to join work 1 day before his retirement <EOS> \n",
      "Summary : <sos> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Text : <news_summary_more> Andhra Pradesh CM N Chandrababu Naidu has said, \"When I met then US President Bill Clinton, I addressed him as Mr Clinton, not as 'sir'. (PM Narendra) Modi is my junior in politics...I addressed him as sir 10 times.\" \"I did this...to satisfy his ego in the hope that he will do justice to the state,\" he added. \n",
      "Real Headline : <SOS> Called PM Modi 'sir' 10 times to satisfy his ego: Andhra CM <EOS> \n",
      "Summary : <sos> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n",
      "Text : <news_summary_more> Congress candidate Shafia Zubair won the Ramgarh Assembly seat in Rajasthan, by defeating BJP's Sukhwant Singh with a margin of 12,228 votes in the bypoll. With this victory, Congress has taken its total to 100 seats in the 200-member assembly. The election to the Ramgarh seat was delayed due to the death of sitting MLA and BSP candidate Laxman Singh. \n",
      "Real Headline : <SOS> Cong wins Ramgarh bypoll in Rajasthan, takes total to 100 seats <EOS> \n",
      "Summary : <sos> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    text = df_trans['cleaned_text'][i]\n",
    "    real_summary = df_trans['cleaned_summary'][i]\n",
    "    print(f\"\\nText : {text} \\nReal Headline : {real_summary} \\nSummary : {summarize(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c9a08",
   "metadata": {
    "papermill": {
     "duration": 0.013411,
     "end_time": "2025-11-13T21:02:27.155072",
     "exception": false,
     "start_time": "2025-11-13T21:02:27.141661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Credits\n",
    "\n",
    "- **\"Implementing Seq2Seq Models for Text Summarization With Keras\"**  \n",
    "  *by Samhita Alla* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15ab425",
   "metadata": {
    "papermill": {
     "duration": 0.013651,
     "end_time": "2025-11-13T21:02:27.183204",
     "exception": false,
     "start_time": "2025-11-13T21:02:27.169553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 33526,
     "isSourceIdPinned": false,
     "sourceId": 44284,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1895,
     "sourceId": 791838,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8714086,
     "sourceId": 13699161,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 477297,
     "modelInstanceId": 461540,
     "sourceId": 614196,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21616.684109,
   "end_time": "2025-11-13T21:02:30.040693",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-13T15:02:13.356584",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
