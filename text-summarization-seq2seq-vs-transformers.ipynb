{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tommasofacchin/text-summarization-seq2seq-vs-transformers?scriptVersionId=278296018\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"93fe7383","metadata":{"papermill":{"duration":0.007915,"end_time":"2025-11-15T12:50:16.86987","exception":false,"start_time":"2025-11-15T12:50:16.861955","status":"completed"},"tags":[]},"source":["# Project Overview\n","\n","This project focuses on **text summarization** using two approaches: a traditional **Seq2Seq model** with LSTM and a **Transformer-based model**. The goal is to see how each model performs and understand the difference between step-by-step sequence processing and attention-based processing.\n","\n","### Steps in the Project\n","1. **Dataset Preparation**  \n","   - Load the XSum dataset with articles and summaries.  \n","   - Tokenize and pad sequences so they can be fed into the models.\n","\n","2. **Seq2Seq Model (LSTM)**  \n","   - Build an encoder-decoder model without attention.\n","   - Train it to generate summaries from the input articles.  \n","\n","3. **Transformer Model**  \n","   - Build a Transformer-based encoder-decoder model.  \n","   - Use self-attention to capture relationships between all tokens.  \n","   - Train on the same dataset to generate summaries.\n","\n","4. **Comparison**  \n","   - Compare the two models using metrics like ROUGE.  \n","   - Look at differences in summary quality, speed, and how well they handle long sequences.\n"]},{"cell_type":"markdown","id":"701e872a","metadata":{"papermill":{"duration":0.006351,"end_time":"2025-11-15T12:50:16.883045","exception":false,"start_time":"2025-11-15T12:50:16.876694","status":"completed"},"tags":[]},"source":["# Seq2Seq and Encoder-Decoder\n","\n","## What is a Seq2Seq Model\n","A sequence-to-sequence (Seq2Seq) model is designed to take an input sequence and produce an output sequence. It’s widely used in tasks like machine translation, text summarization, and chatbots.\n","\n","**Example:**  \n","Input: \"Hello, how are you?\"  \n","Output: \"Ciao, come stai?\"\n","\n","---\n","\n","## Encoder-Decoder Architecture (Expanded)\n","\n","A typical Seq2Seq model has two main parts: the **encoder** and the **decoder**. The design allows the model to process sequences of variable length.  \n","\n","### Encoder\n","The encoder reads the input sequence and compresses it into a set of hidden states or a context vector. This vector captures the important information from the input and has a fixed size, though it does not need to match the decoder's size. The hidden states can either be passed as a whole to the decoder or connected at every decoding step.  \n","\n","At each step, the encoder updates its hidden state based on the previous hidden state and the current input. In mathematical terms, for a simple RNN:\n","\n","$$\n","H_t^{encoder} = \\phi(W_{HH} \\cdot H_{t-1}^{encoder} + W_{HX} \\cdot X_t)\n","$$\n","\n","Where:  \n","- $H_t^{encoder}$ = hidden state at time $t$ in the encoder  \n","- $X_t$ = input at time $t$  \n","- $W_{HH}$ = weight matrix connecting hidden states  \n","- $W_{HX}$ = weight matrix connecting input to hidden states  \n","- $\\phi$ = activation function (e.g., tanh or ReLU)\n","\n","---\n","\n","### Decoder\n","The decoder generates the output sequence one token at a time. Its initial hidden state is set to the final hidden state of the encoder. For a simple RNN decoder:\n","\n","$$\n","H_t^{decoder} = \\phi(W_{HH} \\cdot H_{t-1}^{decoder} + W_{HY} \\cdot Y_{t-1})\n","$$\n","\n","The output at each step is computed as:\n","\n","$$\n","Y_t = W_{HY} \\cdot H_t^{decoder}\n","$$\n","\n","Where:  \n","- $H_t^{decoder}$ = hidden state at time $t$ in the decoder  \n","- $Y_t$ = output at time $t$  \n","- $W_{HY}$ = weight matrix connecting decoder hidden state to output  \n","\n","### Implementation Notes\n","- Encoders and decoders are typically implemented with **RNNs, LSTMs, or GRUs**.  \n","- The input and output vectors are of fixed size, but the encoder and decoder can have different hidden dimensions.  \n","- During training, **teacher forcing** is often used, providing the correct previous token to the decoder instead of its own prediction.  \n","\n","---\n","\n","## Tokenization\n","\n","Before feeding text into a Seq2Seq or Transformer model, the raw text must be converted into numerical form.  \n","This is done through **tokenization**, which splits text into smaller units (tokens) such as words or subwords.  \n","\n","Each token is then mapped to a unique integer using a **vocabulary** built from the dataset.  \n","The model processes these integers rather than the raw text.\n","\n","**Example:**\n","\n","Input text: `\"Transformers improve summarization.\"`  \n","Tokens: `[\"transformers\", \"improve\", \"summarization\", \".\"]`  \n","Token IDs: `[201, 57, 1342, 4]`\n","\n","### Why Tokenization Matters\n","- Converts variable-length text into consistent, model-readable sequences.  \n","- Helps capture word frequency and context relationships.  \n","- Reduces vocabulary size when using subword tokenization (e.g., Byte Pair Encoding).  \n","\n","In this project, tokenization is part of preprocessing and includes:\n","- **Lowercasing** the text  \n","- **Removing special characters and URLs**  \n","- **Splitting into tokens by spaces**  \n","- Adding **start (`sostok`)** and **end (`eostok`)** tokens to mark summary boundaries  \n","\n","After tokenization, sequences will later be converted to integer IDs, padded or truncated to a fixed length\n","\n","---\n","\n","# Transformers\n","Transformers can be seen as an evolution of Seq2Seq models. Instead of processing sequences step by step like LSTMs or GRUs, they rely entirely on **attention mechanisms** to process all tokens in parallel and capture relationships between them.\n","\n","### Attention in Transformers\n","Attention is the core mechanism that allows Transformers to focus on relevant parts of the input sequence when producing a representation for each token. It works by comparing each token to all others and weighting them according to importance.\n","\n","#### How Attention Works\n","Each token in the sequence is represented by three vectors:\n","- **Query (Q):** what this token is looking for  \n","- **Key (K):** what information this token contains  \n","- **Value (V):** the actual information of the token  \n","\n","The attention score between two tokens is computed as the similarity between the Query of one token and the Key of another. This determines how much attention one token should pay to another. Mathematically, the attention weights are computed using a scaled dot-product:\n","\n","$$\n","\\text{Attention}(Q, K, V) = \\text{softmax}\\Big(\\frac{QK^T}{\\sqrt{d_k}}\\Big) V\n","$$\n","\n","Where $d_k$ is the dimensionality of the Key vectors.\n","\n","- The **softmax** ensures that the weights sum to 1.  \n","- Each token’s output is a weighted sum of all Value vectors, allowing it to incorporate context from the entire sequence.\n","\n","#### Multi-Head Attention\n","Instead of computing attention just once, Transformers use **multiple attention heads** in parallel. Each head can learn to focus on different types of relationships, such as:\n","- Syntactic relationships (e.g., subject-verb connections)  \n","- Semantic relationships (e.g., synonyms or related concepts)  \n","\n","The outputs of all heads are concatenated and projected to form the final representation for each token.\n","\n","#### Intuition\n","Imagine reading a sentence and highlighting all the words that are important for understanding each token. Each word “attends” to other words in the sentence that matter most for its meaning. Multi-head attention lets the model do this from multiple perspectives simultaneously.\n","\n","### Key Components of Transformers\n","- **Encoder-Decoder Structure:** Like Seq2Seq models, Transformers have an encoder that processes the input and a decoder that generates the output. Both use layers of self-attention and feed-forward networks.  \n","- **Positional Encoding:** Since Transformers don’t process tokens sequentially, they add positional information so the model knows the order of tokens.  \n","- **Feed-Forward Layers:** After attention, each token passes through fully connected layers for additional transformation.\n","\n","### Advantages over LSTM/GRU Seq2Seq\n","- Processes sequences **in parallel**, speeding up training.  \n","- Handles **long sequences** more effectively with attention.  \n","- Captures **complex relationships** between tokens regardless of distance.  \n","- Scales easily to **very deep models** and large datasets.\n","\n","### Use Cases\n","Transformers are the backbone of many state-of-the-art models for tasks such as:\n","- Machine translation (e.g., T5, MarianMT)  \n","- Text summarization (e.g., BART, Pegasus)  \n","- Question answering and chatbots (e.g., GPT, BERT-based models)"]},{"cell_type":"code","execution_count":1,"id":"a73dd135","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:50:16.897288Z","iopub.status.busy":"2025-11-15T12:50:16.897028Z","iopub.status.idle":"2025-11-15T12:50:23.780874Z","shell.execute_reply":"2025-11-15T12:50:23.779835Z"},"papermill":{"duration":6.892668,"end_time":"2025-11-15T12:50:23.782365","exception":false,"start_time":"2025-11-15T12:50:16.889697","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\r\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\r\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\r\n"]}],"source":["import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","!pip install openpyxl\n","!pip install sentencepiece"]},{"cell_type":"markdown","id":"a77be4c6","metadata":{"papermill":{"duration":0.006623,"end_time":"2025-11-15T12:50:23.796914","exception":false,"start_time":"2025-11-15T12:50:23.790291","status":"completed"},"tags":[]},"source":["# Data Preparation\n","\n","Prepare and clean the dataset for the summarization model:\n","\n","- **Load datasets:** Read two CSV files containing news articles and their summaries.\n","- **Combine datasets:** Merge datasets while selecting relevant `text` and `summary` columns.\n","- **Text cleaning:**  \n","  - Convert text to lowercase.  \n","  - Remove special characters.  \n","  - Replace URLs with domain names.  \n","  - Reduce multiple spaces.\n","- **Tokenization:** Split cleaned text into tokens (words) and add `_START_` and `_END_` tokens for summaries.\n","- **Handle missing values:** Drop rows with missing `text` values.\n","- **Analyze sequence lengths:** Calculate word counts for texts and summaries.\n","- **Limit sequence lengths:** Restrict `text` to 100 words and `summary` to 15 words.\n","- **Add model tokens:** Prepend `sostok` and append `eostok` to all summaries to mark start and end for the model.\n"]},{"cell_type":"code","execution_count":2,"id":"4d34175d","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:50:23.812271Z","iopub.status.busy":"2025-11-15T12:50:23.811499Z","iopub.status.idle":"2025-11-15T12:50:32.708935Z","shell.execute_reply":"2025-11-15T12:50:32.708089Z"},"papermill":{"duration":8.906574,"end_time":"2025-11-15T12:50:32.710311","exception":false,"start_time":"2025-11-15T12:50:23.803737","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["(4514, 6)\n","(98401, 2)\n","(55104, 5)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","pd.set_option('display.max_colwidth', None)\n","\n","summary = pd.read_csv('/kaggle/input/news-summary/news_summary.csv', encoding='iso-8859-1')\n","summary_more = pd.read_csv('/kaggle/input/news-summary/news_summary_more.csv', encoding='iso-8859-1')\n","summary_ind = pd.read_excel(\"/kaggle/input/inshorts-news-data/Inshorts Cleaned Data.xlsx\")\n","\n","summary['text'] = '<news_summary> ' + summary['text']\n","summary_more['text'] = '<news_summary_more> ' + summary_more['text']\n","summary_ind['Short'] = '<news_ind> ' + summary_ind['Short']\n","\n","print(summary.shape)\n","print(summary_more.shape)\n","print(summary_ind.shape)"]},{"cell_type":"code","execution_count":3,"id":"3cca795c","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:50:32.724799Z","iopub.status.busy":"2025-11-15T12:50:32.724484Z","iopub.status.idle":"2025-11-15T12:50:32.742531Z","shell.execute_reply":"2025-11-15T12:50:32.741849Z"},"papermill":{"duration":0.026463,"end_time":"2025-11-15T12:50:32.743753","exception":false,"start_time":"2025-11-15T12:50:32.71729","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>date</th>\n","      <th>headlines</th>\n","      <th>read_more</th>\n","      <th>text</th>\n","      <th>ctext</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Chhavi Tyagi</td>\n","      <td>03 Aug 2017,Thursday</td>\n","      <td>Daman &amp; Diu revokes mandatory Rakshabandhan in offices order</td>\n","      <td>http://www.hindustantimes.com/india-news/rakshabandhan-compulsory-in-daman-and-diu-women-employees-to-tie-rakhis-to-male-colleagues/story-E5h5U1ZDJii5zFpLXWRkhJ.html?utm_source=inshorts&amp;utm_medium=referral&amp;utm_campaign=fullarticle</td>\n","      <td>&lt;news_summary&gt; The Administration of Union Territory Daman and Diu has revoked its order that made it compulsory for women to tie rakhis to their male colleagues on the occasion of Rakshabandhan on August 7. The administration was forced to withdraw the decision within 24 hours of issuing the circular after it received flak from employees and was slammed on social media.</td>\n","      <td>The Daman and Diu administration on Wednesday withdrew a circular that asked women staff to tie rakhis on male colleagues after the order triggered a backlash from employees and was ripped apart on social media.The union territory?s administration was forced to retreat within 24 hours of issuing the circular that made it compulsory for its staff to celebrate Rakshabandhan at workplace.?It has been decided to celebrate the festival of Rakshabandhan on August 7. In this connection, all offices/ departments shall remain open and celebrate the festival collectively at a suitable time wherein all the lady staff shall tie rakhis to their colleagues,? the order, issued on August 1 by Gurpreet Singh, deputy secretary (personnel), had said.To ensure that no one skipped office, an attendance report was to be sent to the government the next evening.The two notifications ? one mandating the celebration of Rakshabandhan (left) and the other withdrawing the mandate (right) ? were issued by the Daman and Diu administration a day apart. The circular was withdrawn through a one-line order issued late in the evening by the UT?s department of personnel and administrative reforms.?The circular is ridiculous. There are sensitivities involved. How can the government dictate who I should tie rakhi to? We should maintain the professionalism of a workplace? an official told Hindustan Times earlier in the day. She refused to be identified.The notice was issued on Daman and Diu administrator and former Gujarat home minister Praful Kodabhai Patel?s direction, sources said.Rakshabandhan, a celebration of the bond between brothers and sisters, is one of several Hindu festivities and rituals that are no longer confined of private, family affairs but have become tools to push politic al ideologies.In 2014, the year BJP stormed to power at the Centre, Rashtriya Swayamsevak Sangh (RSS) chief Mohan Bhagwat said the festival had ?national significance? and should be celebrated widely ?to protect Hindu culture and live by the values enshrined in it?. The RSS is the ideological parent of the ruling BJP.Last year, women ministers in the Modi government went to the border areas to celebrate the festival with soldiers. A year before, all cabinet ministers were asked to go to their constituencies for the festival.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         author                  date  \\\n","0  Chhavi Tyagi  03 Aug 2017,Thursday   \n","\n","                                                      headlines  \\\n","0  Daman & Diu revokes mandatory Rakshabandhan in offices order   \n","\n","                                                                                                                                                                                                                                 read_more  \\\n","0  http://www.hindustantimes.com/india-news/rakshabandhan-compulsory-in-daman-and-diu-women-employees-to-tie-rakhis-to-male-colleagues/story-E5h5U1ZDJii5zFpLXWRkhJ.html?utm_source=inshorts&utm_medium=referral&utm_campaign=fullarticle    \n","\n","                                                                                                                                                                                                                                                                                                                                                                                    text  \\\n","0  <news_summary> The Administration of Union Territory Daman and Diu has revoked its order that made it compulsory for women to tie rakhis to their male colleagues on the occasion of Rakshabandhan on August 7. The administration was forced to withdraw the decision within 24 hours of issuing the circular after it received flak from employees and was slammed on social media.   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ctext  \n","0  The Daman and Diu administration on Wednesday withdrew a circular that asked women staff to tie rakhis on male colleagues after the order triggered a backlash from employees and was ripped apart on social media.The union territory?s administration was forced to retreat within 24 hours of issuing the circular that made it compulsory for its staff to celebrate Rakshabandhan at workplace.?It has been decided to celebrate the festival of Rakshabandhan on August 7. In this connection, all offices/ departments shall remain open and celebrate the festival collectively at a suitable time wherein all the lady staff shall tie rakhis to their colleagues,? the order, issued on August 1 by Gurpreet Singh, deputy secretary (personnel), had said.To ensure that no one skipped office, an attendance report was to be sent to the government the next evening.The two notifications ? one mandating the celebration of Rakshabandhan (left) and the other withdrawing the mandate (right) ? were issued by the Daman and Diu administration a day apart. The circular was withdrawn through a one-line order issued late in the evening by the UT?s department of personnel and administrative reforms.?The circular is ridiculous. There are sensitivities involved. How can the government dictate who I should tie rakhi to? We should maintain the professionalism of a workplace? an official told Hindustan Times earlier in the day. She refused to be identified.The notice was issued on Daman and Diu administrator and former Gujarat home minister Praful Kodabhai Patel?s direction, sources said.Rakshabandhan, a celebration of the bond between brothers and sisters, is one of several Hindu festivities and rituals that are no longer confined of private, family affairs but have become tools to push politic al ideologies.In 2014, the year BJP stormed to power at the Centre, Rashtriya Swayamsevak Sangh (RSS) chief Mohan Bhagwat said the festival had ?national significance? and should be celebrated widely ?to protect Hindu culture and live by the values enshrined in it?. The RSS is the ideological parent of the ruling BJP.Last year, women ministers in the Modi government went to the border areas to celebrate the festival with soldiers. A year before, all cabinet ministers were asked to go to their constituencies for the festival.  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["summary.head(1)"]},{"cell_type":"code","execution_count":4,"id":"7aeed281","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:50:32.758718Z","iopub.status.busy":"2025-11-15T12:50:32.758331Z","iopub.status.idle":"2025-11-15T12:50:32.764472Z","shell.execute_reply":"2025-11-15T12:50:32.763884Z"},"papermill":{"duration":0.014543,"end_time":"2025-11-15T12:50:32.765506","exception":false,"start_time":"2025-11-15T12:50:32.750963","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>headlines</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>upGrad learner switches to career in ML &amp; Al with 90% salary hike</td>\n","      <td>&lt;news_summary_more&gt; Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                           headlines  \\\n","0  upGrad learner switches to career in ML & Al with 90% salary hike   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                          text  \n","0  <news_summary_more> Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["summary_more.head(1)"]},{"cell_type":"code","execution_count":5,"id":"adee685b","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:50:32.78014Z","iopub.status.busy":"2025-11-15T12:50:32.779956Z","iopub.status.idle":"2025-11-15T12:50:32.789945Z","shell.execute_reply":"2025-11-15T12:50:32.789319Z"},"papermill":{"duration":0.018375,"end_time":"2025-11-15T12:50:32.790977","exception":false,"start_time":"2025-11-15T12:50:32.772602","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Headline</th>\n","      <th>Short</th>\n","      <th>Source</th>\n","      <th>Time</th>\n","      <th>Publish Date</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4 ex-bank officials booked for cheating bank of ₹209 crore</td>\n","      <td>&lt;news_ind&gt; The CBI on Saturday booked four former officials of Syndicate Bank and six others for cheating, forgery, criminal conspiracy and causing ₹209 crore loss to the state-run bank. The accused had availed home loans and credit from Syndicate Bank on the basis of forged and fabricated documents. These funds were fraudulently transferred to the companies owned by the accused persons.</td>\n","      <td>The New Indian Express</td>\n","      <td>09:25:00</td>\n","      <td>2017-03-26</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                     Headline  \\\n","0  4 ex-bank officials booked for cheating bank of ₹209 crore   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                    Short  \\\n","0  <news_ind> The CBI on Saturday booked four former officials of Syndicate Bank and six others for cheating, forgery, criminal conspiracy and causing ₹209 crore loss to the state-run bank. The accused had availed home loans and credit from Syndicate Bank on the basis of forged and fabricated documents. These funds were fraudulently transferred to the companies owned by the accused persons.   \n","\n","                  Source      Time  Publish Date  \n","0  The New Indian Express  09:25:00   2017-03-26  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["summary_ind.head(1)"]},{"cell_type":"code","execution_count":6,"id":"d9c7424f","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:50:32.80652Z","iopub.status.busy":"2025-11-15T12:50:32.805945Z","iopub.status.idle":"2025-11-15T12:50:32.871815Z","shell.execute_reply":"2025-11-15T12:50:32.871134Z"},"papermill":{"duration":0.074819,"end_time":"2025-11-15T12:50:32.872951","exception":false,"start_time":"2025-11-15T12:50:32.798132","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["(158019, 2)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>&lt;news_summary_more&gt; Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.</td>\n","      <td>upGrad learner switches to career in ML &amp; Al with 90% salary hike</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>&lt;news_summary_more&gt; Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.</td>\n","      <td>Delhi techie wins free food from Swiggy for one year on CRED</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                                                                                                                                                                                                                                                                                                                                          text  \\\n","0  <news_summary_more> Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.   \n","1                              <news_summary_more> Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.   \n","\n","                                                             summary  \n","0  upGrad learner switches to career in ML & Al with 90% salary hike  \n","1       Delhi techie wins free food from Swiggy for one year on CRED  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["summary = summary.iloc[:, 0:6]\n","summary_more = summary_more.iloc[:, 0:2]\n","\n","# To increase the intake of possible text values to build a reliable model\n","summary['text'] = (\n","    summary['author'] + ' ' +\n","    summary['date'] + ' ' +\n","    summary['read_more'] + ' ' +\n","    summary['text'] + ' ' +\n","    summary['ctext']\n",")\n","\n","\n","df = pd.DataFrame()\n","\n","df['text'] = pd.concat([summary_more['text'], summary['text'], summary_ind['Short']], ignore_index=True)\n","df['summary'] = pd.concat([summary_more['headlines'], summary['headlines'], summary_ind['Headline']], ignore_index=True)\n","\n","df_trans = df\n","\n","print(df.shape)\n","df.head(2)"]},{"cell_type":"code","execution_count":7,"id":"41b5f993","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:50:32.888945Z","iopub.status.busy":"2025-11-15T12:50:32.88834Z","iopub.status.idle":"2025-11-15T12:50:38.176372Z","shell.execute_reply":"2025-11-15T12:50:38.175787Z"},"papermill":{"duration":5.297434,"end_time":"2025-11-15T12:50:38.177826","exception":false,"start_time":"2025-11-15T12:50:32.880392","status":"completed"},"tags":[]},"outputs":[],"source":["import re\n","\n","def clean_text(text):\n","    text = str(text)\n","    text = re.sub(r'https?://\\S+', '', text)  \n","    text = re.sub(r'\\s+', ' ', text).strip()  \n","    return text\n","\n","\n","# Tokenization: split text by spaces\n","def tokenize_texts(texts):\n","    return [' '.join(clean_text(t).split()) for t in texts]\n","\n","\n","processed_text = tokenize_texts(df['text'])\n","#processed_summary = ['_START_ ' + s + ' _END_' for s in tokenize_texts(df['summary'])]\n","processed_summary = ['sostok ' + s + ' eostok' for s in tokenize_texts(df['summary'])]\n","\n","\n","df['cleaned_text'] = pd.Series(processed_text)\n","df['cleaned_summary'] = pd.Series(processed_summary)"]},{"cell_type":"code","execution_count":8,"id":"7d3708e3","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:50:38.197523Z","iopub.status.busy":"2025-11-15T12:50:38.19584Z","iopub.status.idle":"2025-11-15T12:50:38.629318Z","shell.execute_reply":"2025-11-15T12:50:38.628598Z"},"papermill":{"duration":0.443539,"end_time":"2025-11-15T12:50:38.630471","exception":false,"start_time":"2025-11-15T12:50:38.186932","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["NaN dropped: 118\n","(157886, 4)\n"]}],"source":["print(f\"NaN dropped: {df.isna().sum().sum()}\")\n","\n","#df = df.dropna(subset=['text'])\n","df = df.dropna(subset=['cleaned_text', 'cleaned_summary'])\n","df = df.drop_duplicates(subset=['cleaned_text', 'cleaned_summary'])\n","\n","print(df.shape)"]},{"cell_type":"code","execution_count":9,"id":"81237f5b","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:50:38.647251Z","iopub.status.busy":"2025-11-15T12:50:38.646742Z","iopub.status.idle":"2025-11-15T12:50:39.90647Z","shell.execute_reply":"2025-11-15T12:50:39.905868Z"},"papermill":{"duration":1.269193,"end_time":"2025-11-15T12:50:39.907702","exception":false,"start_time":"2025-11-15T12:50:38.638509","status":"completed"},"tags":[]},"outputs":[{"data":{"image/png":"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\n","text/plain":["<Figure size 640x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","\n","text_count = []\n","summary_count = []\n","\n","for sent in df['cleaned_text']:\n","    text_count.append(len(sent.split()))\n","    \n","for sent in df['cleaned_summary']:\n","    summary_count.append(len(sent.split()))\n","\n","graph_df = pd.DataFrame() \n","\n","graph_df['text'] = text_count\n","graph_df['summary'] = summary_count\n","\n","graph_df.hist(bins = 30)\n","plt.show()"]},{"cell_type":"code","execution_count":10,"id":"2fd57a44","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:50:39.925066Z","iopub.status.busy":"2025-11-15T12:50:39.92444Z","iopub.status.idle":"2025-11-15T12:50:40.514474Z","shell.execute_reply":"2025-11-15T12:50:40.51349Z"},"papermill":{"duration":0.599882,"end_time":"2025-11-15T12:50:40.515855","exception":false,"start_time":"2025-11-15T12:50:39.915973","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Percentage of texts with less than 100 words : 0.9724991449526874\n"]}],"source":["cnt = 0\n","for i in df['cleaned_text']:\n","    if len(i.split()) <= 100:\n","        cnt = cnt + 1\n","print(f\"Percentage of texts with less than 100 words : {cnt / len(df['cleaned_text'])}\")"]},{"cell_type":"code","execution_count":11,"id":"99a715f3","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:50:40.5338Z","iopub.status.busy":"2025-11-15T12:50:40.533483Z","iopub.status.idle":"2025-11-15T12:50:43.28608Z","shell.execute_reply":"2025-11-15T12:50:43.285434Z"},"papermill":{"duration":2.76328,"end_time":"2025-11-15T12:50:43.287549","exception":false,"start_time":"2025-11-15T12:50:40.524269","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>summary</th>\n","      <th>cleaned_text</th>\n","      <th>cleaned_summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>&lt;news_summary_more&gt; Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.</td>\n","      <td>upGrad learner switches to career in ML &amp; Al with 90% salary hike</td>\n","      <td>&lt;news_summary_more&gt; Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.</td>\n","      <td>sostok sostok upGrad learner switches to career in ML &amp; Al with 90% salary hike eostok eostok</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>&lt;news_summary_more&gt; Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.</td>\n","      <td>Delhi techie wins free food from Swiggy for one year on CRED</td>\n","      <td>&lt;news_summary_more&gt; Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.</td>\n","      <td>sostok sostok Delhi techie wins free food from Swiggy for one year on CRED eostok eostok</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                                                                                                                                                                                                                                                                                                                                          text  \\\n","0  <news_summary_more> Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.   \n","1                              <news_summary_more> Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.   \n","\n","                                                             summary  \\\n","0  upGrad learner switches to career in ML & Al with 90% salary hike   \n","1       Delhi techie wins free food from Swiggy for one year on CRED   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                  cleaned_text  \\\n","0  <news_summary_more> Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.   \n","1                              <news_summary_more> Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.   \n","\n","                                                                                 cleaned_summary  \n","0  sostok sostok upGrad learner switches to career in ML & Al with 90% salary hike eostok eostok  \n","1       sostok sostok Delhi techie wins free food from Swiggy for one year on CRED eostok eostok  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["max_text_len = 100\n","max_summary_len = 20\n","\n","df['cleaned_text'] = df['cleaned_text'].astype(str)\n","df['cleaned_summary'] = df['cleaned_summary'].astype(str)\n","\n","mask = (df['cleaned_text'].str.split().str.len() <= max_text_len) & \\\n","       (df['cleaned_summary'].str.split().str.len() <= max_summary_len)\n","\n","df = df.loc[mask].reset_index(drop=True)\n","\n","# Add start and end tokens to each summary\n","df['cleaned_summary'] = df['cleaned_summary'].apply(lambda x: 'sostok ' + x + ' eostok')\n","\n","df.head(2)"]},{"cell_type":"markdown","id":"6aef0c3b","metadata":{"papermill":{"duration":0.009264,"end_time":"2025-11-15T12:50:43.306649","exception":false,"start_time":"2025-11-15T12:50:43.297385","status":"completed"},"tags":[]},"source":["# **Tokenization**\n","\n","This block prepares the text data for a sequence-to-sequence model:\n","\n","- **Split dataset**: Separates `text` and `summary` into training and validation sets to evaluate model performance on unseen data.  \n","- **Initialize tokenizers**: Converts words into integer indices, which neural networks can process.  \n","- **Analyze rare words**: Computes the percentage of words appearing less than `thresh` times to identify infrequent words that might add noise.  \n","- **Limit vocabulary to frequent words**: Reduces vocabulary size by ignoring rare words, which improves training efficiency and prevents overfitting.  \n","- **Convert texts to sequences**: Maps each word in the texts to its corresponding integer index.  \n","- **Pad sequences**: Ensures all sequences have the same length, necessary for batch processing in neural networks.  \n","- **Compute final vocabulary size**: Includes the padding token to correctly define the input dimension for the model embedding layer."]},{"cell_type":"code","execution_count":12,"id":"ab52f5df","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:50:43.389793Z","iopub.status.busy":"2025-11-15T12:50:43.389065Z","iopub.status.idle":"2025-11-15T12:51:05.93052Z","shell.execute_reply":"2025-11-15T12:51:05.929905Z"},"papermill":{"duration":22.616364,"end_time":"2025-11-15T12:51:05.931967","exception":false,"start_time":"2025-11-15T12:50:43.315603","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-11-15 12:50:45.992251: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1763211046.177870      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1763211046.233034      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer \n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","x_train, x_val, y_train, y_val = train_test_split(\n","    np.array(df[\"cleaned_text\"]),\n","    np.array(df[\"cleaned_summary\"]),\n","    test_size=0.1,\n","    random_state=0,\n","    shuffle=True,\n",")\n","\n","x_tokenizer = Tokenizer(oov_token=\"<unk>\") \n","x_tokenizer.fit_on_texts(list(x_train))\n","\n","y_tokenizer = Tokenizer(oov_token=\"<unk>\")   \n","y_tokenizer.fit_on_texts(list(y_train))"]},{"cell_type":"code","execution_count":13,"id":"fccef9e5","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:51:05.950405Z","iopub.status.busy":"2025-11-15T12:51:05.949411Z","iopub.status.idle":"2025-11-15T12:51:05.999892Z","shell.execute_reply":"2025-11-15T12:51:05.999033Z"},"papermill":{"duration":0.060362,"end_time":"2025-11-15T12:51:06.000988","exception":false,"start_time":"2025-11-15T12:51:05.940626","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["% of rare words in X vocabulary: 57.17%\n","% of rare words in Y vocabulary: 55.67%\n"]}],"source":["thresh = 3\n","\n","def rare_word_stats(tokenizer, thresh):\n","    \"\"\"Return total and rare word counts for a given tokenizer.\"\"\"\n","    total_cnt = len(tokenizer.word_counts)\n","    rare_cnt = sum(1 for word, count in tokenizer.word_counts.items() if count < thresh)\n","    return total_cnt, rare_cnt\n","\n","\n","x_tot_cnt, x_cnt = rare_word_stats(x_tokenizer, thresh)\n","y_tot_cnt, y_cnt = rare_word_stats(y_tokenizer, thresh)\n","\n","print(f\"% of rare words in X vocabulary: {(x_cnt / x_tot_cnt) * 100:.2f}%\")\n","print(f\"% of rare words in Y vocabulary: {(y_cnt / y_tot_cnt) * 100:.2f}%\")"]},{"cell_type":"code","execution_count":14,"id":"4834eb61","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:51:06.018313Z","iopub.status.busy":"2025-11-15T12:51:06.018083Z","iopub.status.idle":"2025-11-15T12:51:22.252263Z","shell.execute_reply":"2025-11-15T12:51:22.251432Z"},"papermill":{"duration":16.244184,"end_time":"2025-11-15T12:51:22.253559","exception":false,"start_time":"2025-11-15T12:51:06.009375","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Size of vocabulary in X = 53389\n","Size of vocabulary in Y = 22916\n"]}],"source":["# Create tokenizers considering only frequent words\n","x_tokenizer = Tokenizer(num_words = x_tot_cnt - x_cnt) \n","x_tokenizer.fit_on_texts(list(x_train))\n","\n","y_tokenizer = Tokenizer(num_words=y_tot_cnt-y_cnt) \n","y_tokenizer.fit_on_texts(list(y_train))\n","\n","# Convert text to sequences of integers\n","x_train_seq = x_tokenizer.texts_to_sequences(x_train) \n","x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n","\n","y_train_seq = y_tokenizer.texts_to_sequences(y_train) \n","y_val_seq = y_tokenizer.texts_to_sequences(y_val) \n","\n","# Pad sequences\n","x_train = pad_sequences(x_train_seq,  maxlen=max_text_len, padding='post')\n","x_val = pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n","\n","y_train = pad_sequences(y_train_seq, maxlen=max_summary_len, padding='post')\n","y_val = pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n","\n","# Vocab. size (+1 for padding token)\n","x_voc_size = x_tokenizer.num_words + 1\n","y_voc_size = y_tokenizer.num_words + 1\n","\n","print(f\"Size of vocabulary in X = {x_voc_size}\")\n","print(f\"Size of vocabulary in Y = {y_voc_size}\")"]},{"cell_type":"code","execution_count":15,"id":"ef10667f","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:51:22.271252Z","iopub.status.busy":"2025-11-15T12:51:22.271007Z","iopub.status.idle":"2025-11-15T12:51:22.335411Z","shell.execute_reply":"2025-11-15T12:51:22.33481Z"},"papermill":{"duration":0.07475,"end_time":"2025-11-15T12:51:22.336797","exception":false,"start_time":"2025-11-15T12:51:22.262047","status":"completed"},"tags":[]},"outputs":[],"source":["# Finally remove from the dataset empty summaries that contain only the 'START' and 'END' tokens\n","\n","x_train = x_train[np.sum(y_train != 0, axis=1) > 2]\n","y_train = y_train[np.sum(y_train != 0, axis=1) > 2]\n","\n","x_val = x_val[np.sum(y_val != 0, axis=1) > 2]\n","y_val = y_val[np.sum(y_val != 0, axis=1) > 2]"]},{"cell_type":"markdown","id":"e852ad18","metadata":{"papermill":{"duration":0.008372,"end_time":"2025-11-15T12:51:22.354162","exception":false,"start_time":"2025-11-15T12:51:22.34579","status":"completed"},"tags":[]},"source":["# **Seq2seq model using LSTM**\n","\n","### Encoder-Decoder Architecture with LSTM\n","\n","During training, the model takes **two inputs**:  \n","1. The encoder input (`text`) – the original text sequence.  \n","2. The decoder input (`summary`) – the summary shifted by one token (so that the model learns to predict the next word).  \n","\n","The **target output** is the summary sequence shifted forward by one token. The model learns to predict the next word in the summary based on the previous words. During inference, the trained model generates summaries one word at a time, using the previously predicted words as input.\n","\n","---\n","\n","**Encoder**  \n","- The encoder accepts sequences of text with a fixed length (`max_text_len`).  \n","- The text is first passed through an **Embedding layer** that maps each word index to a dense vector of size `(embedding_dim)`.  \n","- The embedded sequence is then processed by **three stacked LSTM layers**:  \n","  - Each layer outputs the **full sequence of hidden states** (for possible attention or stacking) and the **last hidden and cell states**.  \n","  - The last hidden and cell states from the final LSTM are used to initialize the decoder.  \n","- Stacking multiple LSTMs allows the encoder to **capture both local patterns and long-range dependencies** in the text.\n","\n","---\n","\n","**Decoder**  \n","- The decoder input (shifted summary) is passed through an **Embedding layer** of size `(summary vocabulary size x embedding_dim)`.  \n","- A single **LSTM** processes the embedded sequence, using the **encoder's last hidden and cell states** as its initial state.  \n","- The LSTM output is passed through a **TimeDistributed Dense layer** with **softmax activation**, which predicts the probability of each word in the vocabulary at each time step.  \n","\n","This architecture ensures that the decoder can generate the summary step by step, **learning the sequence of words conditioned on the input text**.\n"]},{"cell_type":"code","execution_count":16,"id":"7afddbaf","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:51:22.372453Z","iopub.status.busy":"2025-11-15T12:51:22.372207Z","iopub.status.idle":"2025-11-15T12:51:24.513796Z","shell.execute_reply":"2025-11-15T12:51:24.513078Z"},"papermill":{"duration":2.152469,"end_time":"2025-11-15T12:51:24.515095","exception":false,"start_time":"2025-11-15T12:51:22.362626","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["I0000 00:00:1763211083.068296      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10,677,800</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">601,200</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]             │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">721,200</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]             │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,583,200</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">721,200</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]             │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">601,200</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]             │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ time_distributed    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">6,897,716</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">22916</span>)            │            │                   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m200\u001b[0m)  │ \u001b[38;5;34m10,677,800\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n","│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m,      │    \u001b[38;5;34m601,200\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n","│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n","│                     │ \u001b[38;5;34m300\u001b[0m)]             │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m,      │    \u001b[38;5;34m721,200\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n","│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n","│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n","│                     │ \u001b[38;5;34m300\u001b[0m)]             │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m) │  \u001b[38;5;34m4,583,200\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n","│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m,      │    \u001b[38;5;34m721,200\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n","│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n","│                     │ \u001b[38;5;34m300\u001b[0m)]             │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m601,200\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n","│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n","│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n","│                     │ \u001b[38;5;34m300\u001b[0m)]             │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ time_distributed    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m6,897,716\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m22916\u001b[0m)            │            │                   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,803,516</span> (94.62 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,803,516\u001b[0m (94.62 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,803,516</span> (94.62 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m24,803,516\u001b[0m (94.62 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.callbacks import EarlyStopping\n","#from tensorflow.keras import mixed_precision\n","\n","#mixed_precision.set_global_policy('mixed_float16')\n","\n","latent_dim = 300\n","embedding_dim = 200\n","\n","# Encoder\n","encoder_inputs = Input(shape=(max_text_len, ))\n","\n","# Embedding layer\n","enc_emb = Embedding(x_voc_size, embedding_dim,\n","                    trainable=True)(encoder_inputs)\n","\n","# Encoder LSTM 1\n","encoder_lstm1 = LSTM(latent_dim, return_sequences=True,\n","                     return_state=True, dropout=0.4,\n","                     recurrent_dropout=0.4)\n","(encoder_output1, state_h1, state_c1) = encoder_lstm1(enc_emb)\n","\n","# Encoder LSTM 2\n","encoder_lstm2 = LSTM(latent_dim, return_sequences=True,\n","                     return_state=True, dropout=0.4,\n","                     recurrent_dropout=0.4)\n","(encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)\n","\n","# Encoder LSTM 3\n","encoder_lstm3 = LSTM(latent_dim, return_state=True,\n","                     return_sequences=True, dropout=0.4,\n","                     recurrent_dropout=0.4)\n","(encoder_outputs, state_h, state_c) = encoder_lstm3(encoder_output2)\n","\n","# Decoder\n","decoder_inputs = Input(shape=(None, ))\n","\n","# Embedding layer\n","dec_emb_layer = Embedding(y_voc_size, embedding_dim, trainable=True)\n","dec_emb = dec_emb_layer(decoder_inputs)\n","\n","# Decoder LSTM\n","decoder_lstm = LSTM(latent_dim, return_sequences=True,\n","                    return_state=True, dropout=0.4,\n","                    recurrent_dropout=0.2)\n","(decoder_outputs, decoder_fwd_state, decoder_back_state) = \\\n","    decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n","\n","# Dense layer\n","decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax'))\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Model\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","model.summary()"]},{"cell_type":"markdown","id":"8cc0a19e","metadata":{"papermill":{"duration":0.010171,"end_time":"2025-11-15T12:51:24.535774","exception":false,"start_time":"2025-11-15T12:51:24.525603","status":"completed"},"tags":[]},"source":["## **Training the model** "]},{"cell_type":"code","execution_count":17,"id":"7de721b2","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:51:24.555275Z","iopub.status.busy":"2025-11-15T12:51:24.55499Z","iopub.status.idle":"2025-11-15T12:51:26.396067Z","shell.execute_reply":"2025-11-15T12:51:26.395426Z"},"papermill":{"duration":1.852209,"end_time":"2025-11-15T12:51:26.397394","exception":false,"start_time":"2025-11-15T12:51:24.545185","status":"completed"},"tags":[]},"outputs":[],"source":["x_train_lstm = x_train\n","x_val_lstm = x_val\n","y_train_lstm = y_train\n","y_val_lstm = y_val\n","\n","MODEL_INPUT_PATH = \"/kaggle/input/lstm-keras-summarization/keras/default/1/model_lstm.keras\" \n","MODEL_PATH = \"/kaggle/working/model_lstm.keras\" \n","\n","\n","if os.path.exists(MODEL_INPUT_PATH):\n","    model = load_model(MODEL_INPUT_PATH)\n","\n","else:\n","    model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n","    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n","\n","    history = model.fit(\n","        [x_train, y_train[:, :-1]],\n","        y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:, 1:],\n","        epochs=50,\n","        callbacks=[es],\n","        batch_size=128,\n","        validation_data=(\n","            [x_val, y_val[:, :-1]],\n","            y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:, 1:]\n","        )\n","    )\n","\n","    model.save(MODEL_PATH)\n","\n","    plt.plot(history.history['loss'], label='train')\n","    plt.plot(history.history['val_loss'], label='test')\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"markdown","id":"0a7a7df6","metadata":{"papermill":{"duration":0.008838,"end_time":"2025-11-15T12:51:26.415506","exception":false,"start_time":"2025-11-15T12:51:26.406668","status":"completed"},"tags":[]},"source":["## **Predict**\n","\n","Once the Seq2Seq model has been trained, we can use it to **generate summaries** for new input texts.  \n","This stage is known as **inference**, the model no longer learns, but uses its learned parameters to predict the most likely output sequence (summary).\n","\n","---\n","\n","### 1. Preparing the Mapping Dictionaries\n","Before generating predictions, we rebuild the word–token mappings from the tokenizers:\n","- `reverse_x_word_index`: converts article tokens → words  \n","- `reverse_y_word_index`: converts summary tokens → words  \n","- `y_word_index`: converts summary words → tokens  \n","\n","These dictionaries let us translate between the model’s numeric predictions and readable text.\n","\n","---\n","\n","### 2. Building Inference Models\n","During training, the encoder and decoder work together in a single model.  \n","At inference time, we separate them:\n","\n","- The **encoder model** processes the input text once and produces context vectors (`encoder_outputs`, `state_h`, `state_c`) — a compressed representation of the input.  \n","- The **decoder model** generates the summary **one word at a time**, taking as input the previous word and its previous internal states.\n","\n","This setup allows the decoder to iteratively predict each next token until the end-of-sequence marker (`eostok`) is reached.\n","\n","---\n","\n","### 3. The Decoding Process\n","The function `decode_sequence()` handles the actual text generation:\n","\n","1. **Encode the input sequence** using the encoder model to obtain its internal states.  \n","2. **Initialize** the decoder with the special start token (`sostok`).  \n","3. **Iteratively predict** the next word:\n","   - Feed the previous word and the latest decoder states into the model.\n","   - Pick the word with the highest probability (`argmax`).\n","   - Stop when the `eostok` token is predicted or the maximum summary length is reached.\n","4. **Concatenate** all predicted tokens into a readable summary.\n","\n","This process simulates how the model “writes” one word at a time, using its internal memory to maintain context.\n","\n","---\n","\n","### 4. Converting Sequences to Text\n","Two helper functions make the predictions human-readable:\n","- `seq2text()` converts numeric article sequences back into words.\n","- `seq2summary()` converts numeric summary sequences back into words, excluding special tokens (`sostok`, `eostok`).\n"]},{"cell_type":"code","execution_count":18,"id":"ab900663","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:51:26.433857Z","iopub.status.busy":"2025-11-15T12:51:26.433615Z","iopub.status.idle":"2025-11-15T12:51:26.437415Z","shell.execute_reply":"2025-11-15T12:51:26.43671Z"},"papermill":{"duration":0.014295,"end_time":"2025-11-15T12:51:26.438485","exception":false,"start_time":"2025-11-15T12:51:26.42419","status":"completed"},"tags":[]},"outputs":[],"source":["# reverse_y_word_index: summary token → word\n","# reverse_x_word_index: article token → word\n","# y_word_index: summary word → token\n","\n","reverse_y_word_index = y_tokenizer.index_word\n","reverse_x_word_index = x_tokenizer.index_word\n","y_word_index = y_tokenizer.word_index"]},{"cell_type":"code","execution_count":19,"id":"1ec9f48d","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:51:26.456449Z","iopub.status.busy":"2025-11-15T12:51:26.456257Z","iopub.status.idle":"2025-11-15T12:51:26.467307Z","shell.execute_reply":"2025-11-15T12:51:26.466743Z"},"papermill":{"duration":0.021246,"end_time":"2025-11-15T12:51:26.468364","exception":false,"start_time":"2025-11-15T12:51:26.447118","status":"completed"},"tags":[]},"outputs":[],"source":["# Inference Models\n","encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs,\n","                      state_h, state_c])\n","\n","# Below tensors will hold the states of the previous time step\n","decoder_state_input_h = Input(shape=(latent_dim, ))\n","decoder_state_input_c = Input(shape=(latent_dim, ))\n","decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim))\n","\n","# Get the embeddings of the decoder sequence\n","dec_emb2 = dec_emb_layer(decoder_inputs)\n","\n","# To predict the next word in the sequence, set the initial states to the states from the previous time step\n","(decoder_outputs2, state_h2, state_c2) = decoder_lstm(dec_emb2,\n","        initial_state=[decoder_state_input_h, decoder_state_input_c])\n","\n","# A dense softmax layer to generate prob dist. over the target vocabulary\n","decoder_outputs2 = decoder_dense(decoder_outputs2)\n","\n","# Final decoder model\n","#decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n","#                       decoder_state_input_h, decoder_state_input_c],\n","#                       [decoder_outputs2] + [state_h2, state_c2])\n","#TPU error\n","decoder_model = Model(\n","    [decoder_inputs, decoder_state_input_h, decoder_state_input_c],\n","    [decoder_outputs2, state_h2, state_c2]\n",")\n"]},{"cell_type":"code","execution_count":20,"id":"98df36d4","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:51:26.486917Z","iopub.status.busy":"2025-11-15T12:51:26.486704Z","iopub.status.idle":"2025-11-15T12:51:26.493046Z","shell.execute_reply":"2025-11-15T12:51:26.492372Z"},"papermill":{"duration":0.016782,"end_time":"2025-11-15T12:51:26.494083","exception":false,"start_time":"2025-11-15T12:51:26.477301","status":"completed"},"tags":[]},"outputs":[],"source":["# Convert sequence to summary\n","def seq2summary(input_seq):\n","    newString = ''\n","    for i in input_seq:\n","        if i != 0 and i != y_word_index['sostok'] and i \\\n","            != y_word_index['eostok']:\n","            newString = newString + reverse_y_word_index[i] + ' '\n","\n","    return newString\n","\n","# Convert sequence to text\n","def seq2text(input_seq):\n","    newString = ''\n","    for i in input_seq:\n","        if i != 0:\n","            newString = newString + reverse_x_word_index[i] + ' '\n","\n","    return newString\n","\n","def decode_sequence(input_seq):\n","\n","    # Encode the input as state vectors.\n","    (e_out, e_h, e_c) = encoder_model.predict(input_seq, verbose=0)\n","\n","    # Generate empty target sequence of length 1\n","    y_seq = np.zeros((1, 1))\n","\n","    # Populate the first word of target sequence with the start word.\n","    y_seq[0, 0] = y_word_index['sostok']\n","\n","    stop_condition = False\n","    decoded_sentence = ''\n","\n","    while not stop_condition:\n","        (output_tokens, h, c) = decoder_model.predict([y_seq]\n","                + [e_out, e_h, e_c], verbose=0)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_token = reverse_y_word_index[sampled_token_index]\n","\n","        if sampled_token != 'eostok':\n","            decoded_sentence += ' ' + sampled_token\n","\n","        # Exit condition: either hit max length or find the stop word.\n","        if sampled_token == 'eostok' or len(decoded_sentence.split()) \\\n","            >= max_summary_len - 1:\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1)\n","        y_seq = np.zeros((1, 1))\n","        y_seq[0, 0] = sampled_token_index\n","\n","        # Update internal states\n","        (e_h, e_c) = (h, c)\n","\n","    return decoded_sentence"]},{"cell_type":"code","execution_count":21,"id":"2acff17b","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:51:26.512743Z","iopub.status.busy":"2025-11-15T12:51:26.512157Z","iopub.status.idle":"2025-11-15T12:51:26.515323Z","shell.execute_reply":"2025-11-15T12:51:26.514789Z"},"papermill":{"duration":0.01359,"end_time":"2025-11-15T12:51:26.516346","exception":false,"start_time":"2025-11-15T12:51:26.502756","status":"completed"},"tags":[]},"outputs":[],"source":["# for i in range(0, 5):\n","#     print ('Review:', seq2text(x_train[i]))\n","#     print ('Original summary:', seq2summary(y_train[i]))\n","#     print ('Predicted summary:', decode_sequence(x_train[i].reshape(1, max_text_len)))\n","#     print('\\n')"]},{"cell_type":"markdown","id":"d9901e06","metadata":{"papermill":{"duration":0.008633,"end_time":"2025-11-15T12:51:26.533608","exception":false,"start_time":"2025-11-15T12:51:26.524975","status":"completed"},"tags":[]},"source":["# Transformer Model with Self-Attention\n","\n","Similar to the Seq2Seq architecture, the Transformer follows an **encoder–decoder structure**, but instead of recurrent layers it relies entirely on **Multi-Head Self-Attention**.  \n","This allows the model to process all tokens **in parallel** and learn relationships between words regardless of their distance in the sequence.\n","\n","During training, the model takes **two inputs**:  \n","1. The encoder input (`text`) – the tokenized article.  \n","2. The decoder input (`summary`) – the summary shifted by one token.  \n","\n","The **target output** is the summary shifted forward by one position. The decoder learns to predict each word based on the previously generated ones and the encoded representation of the full text.\n","\n","---\n","\n","**Encoder**  \n","- The input article sequence (`max_text_len`) is first transformed using an **Embedding layer**.  \n","- A **Positional Encoding** is added to preserve the order of words (since attention has no notion of sequence order by itself).  \n","- The embedded input is processed by one or more **Multi-Head Self-Attention** blocks:\n","  - Each word attends to **all other words** in the input\n","  - Relationships between distant tokens are captured more effectively than in RNNs  \n","- A **Feed-Forward Network** (FFN) refines the contextual representations.\n","- **Residual connections** and **Layer Normalization** improve gradient flow and training stability.\n","\n","---\n","\n","**Decoder**  \n","- Similar positional embeddings are applied to the shifted summary tokens.  \n","- The decoder uses two attention mechanisms:\n","  1. **Masked Self-Attention**: ensures the model cannot “peek” at future words when predicting the next token.\n","  2. **Encoder-Decoder Attention**: allows the decoder to focus on relevant parts of the input article.\n","- A **Feed-Forward Network** further processes the attended features.\n","- A final **Dense layer with Softmax** outputs a probability distribution over all words in the vocabulary at each time step.\n","\n","---\n","\n","**sostok and eostok**  \n","In sequence-to-sequence tasks such as abstractive text summarization, **special tokens** are essential for controlling how a model generates text:\n","\n","- `<sostok>` → marks the **start** of the output sequence  \n","- `<eostok>` → marks the **end** of the sequence  \n","\n","However, these tokens **do not** play the same role during training across different architectures.\n","\n","Transformers use **masked self-attention** in the decoder, meaning that at time *t* the model can only attend to **previous tokens**.\n","Therefore:\n","\n","- `<sostok>` must be present **only in the decoder input**  \n","- `<sostok>` must be removed from the decoder target  \n","\n","Predicting a start token would make no sense and causes failure modes such as:\n","\n","- the model repeatedly outputting `<sostok>`\n","- inability to begin sequences with meaningful content\n","\n","The EOS token **must remain in the targets**, because:\n","\n","- it teaches the model **when to stop writing**\n","- without it, generation may become too long or infinite\n","\n","LSTM encoder-decoder models:\n","\n","- receive the final hidden state as initial context\n","- do **not** use masked attention\n","- often ignore the first timestep in loss computation\n","\n","So `<sostok>` in targets is less harmful there.\n","\n","---\n","\n","Thanks to the Self-Attention mechanism, Transformers **capture global context efficiently** and typically produce **more coherent and fluent summaries**, especially for longer texts."]},{"cell_type":"markdown","id":"4a44ebd8","metadata":{"papermill":{"duration":0.008479,"end_time":"2025-11-15T12:51:26.550629","exception":false,"start_time":"2025-11-15T12:51:26.54215","status":"completed"},"tags":[]},"source":["### Preparing Transformer Inputs\n","\n","To train the Transformer in an encoder–decoder setup, we need to properly structure the input data:\n","\n","- The **encoder input** is the full tokenized article (`x_train`)\n","- The **decoder input** is the summary sequence **shifted right**, starting with `<sostok>`\n","- The **decoder target** is the same summary **shifted left**, ending with `<eostok>`\n","\n","This shifting ensures that at each timestep the decoder learns to predict the **next** word using:\n","1. The previously processed summary tokens  \n","2. Attention over the encoder output  "]},{"cell_type":"markdown","id":"46780659","metadata":{"papermill":{"duration":0.008513,"end_time":"2025-11-15T12:51:26.567822","exception":false,"start_time":"2025-11-15T12:51:26.559309","status":"completed"},"tags":[]},"source":["In text summarization, token-level accuracy can be misleading because it only measures whether each predicted token matches the ground truth at the same position. It does not capture semantic meaning, fluency, word order, or relevance, and it can be inflated by common tokens like padding or start/end markers. A model can have high accuracy while producing poor summaries. Better evaluation uses metrics like ROUGE-1, ROUGE-2, and ROUGE-L, which measure overlap of unigrams, bigrams, and longest common subsequences between generated and reference summaries. During training, it is better to monitor validation loss and evaluate summaries qualitatively or with ROUGE rather than relying on token accuracy."]},{"cell_type":"markdown","id":"133158c7","metadata":{"papermill":{"duration":0.008534,"end_time":"2025-11-15T12:51:26.584909","exception":false,"start_time":"2025-11-15T12:51:26.576375","status":"completed"},"tags":[]},"source":["## Predict"]},{"cell_type":"markdown","id":"e6323488","metadata":{"papermill":{"duration":0.008394,"end_time":"2025-11-15T12:51:26.601843","exception":false,"start_time":"2025-11-15T12:51:26.593449","status":"completed"},"tags":[]},"source":["Note importanti:\n","\n","Look-ahead mask a inference non serve se generi un token alla volta (greedy decoding step-by-step).\n","\n","Padding mask dell’encoder serve al decoder per ignorare i pad token dell’input.\n","\n","Quando fai l’inference dovrai generare token uno per uno, aggiornando dec_input_inf ad ogni step."]},{"cell_type":"markdown","id":"523d9132","metadata":{"papermill":{"duration":0.008666,"end_time":"2025-11-15T12:51:26.619017","exception":false,"start_time":"2025-11-15T12:51:26.610351","status":"completed"},"tags":[]},"source":["Generated summary: ripete continuamente parole (cannot cannot cannot, power power power…) → questo è un loop di ripetizione, tipico dei modelli seq2seq che non hanno abbastanza regolarizzazione sulla generazione.\n","\n","Generated beam search summary: testo quasi completamente fuori tema → indica che il modello non ha appreso bene il contenuto semantico e il beam search amplifica le frasi che appaiono più “probabili” a livello di token, ma non corrette.\n"]},{"cell_type":"markdown","id":"98e91a9f","metadata":{"papermill":{"duration":0.008362,"end_time":"2025-11-15T12:51:26.635981","exception":false,"start_time":"2025-11-15T12:51:26.627619","status":"completed"},"tags":[]},"source":["Limiting the vocabulary in a Transformer is important because it reduces the size of the embedding matrices and the final softmax layer, making the model faster and lighter. It also helps prevent overfitting by removing extremely rare words that add noise rather than useful information. A smaller vocabulary uses less memory and often leads to more stable training, which can be important when working with limited hardware. However, reducing the vocabulary also means losing information, because words outside the limit are replaced with an unknown token. This can harm tasks like summarization, where specific terms, names, or technical words matter. A limited vocabulary also restricts what the model can generate, since it can only output words it knows.\n","\n","I tried limiting the vocabulary, but it ended up harming the model’s performance."]},{"cell_type":"markdown","id":"d58b2c43","metadata":{"papermill":{"duration":0.008437,"end_time":"2025-11-15T12:51:26.652858","exception":false,"start_time":"2025-11-15T12:51:26.644421","status":"completed"},"tags":[]},"source":["We wanted to continue training a Transformer model after the first epoch without losing the optimizer state. The issue was that creating a new optimizer reset the step count, making the learning rate extremely small due to the warmup schedule. To fix this, we restored a full checkpoint including both the model and optimizer. This ensures the weights, optimizer moments, and step count are preserved, so the learning rate continues correctly. Training can now continue from where it left off, and checkpoints can be saved after each epoch to resume seamlessly in future sessions."]},{"cell_type":"code","execution_count":22,"id":"456a0216","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:51:26.671114Z","iopub.status.busy":"2025-11-15T12:51:26.670859Z","iopub.status.idle":"2025-11-15T12:51:26.674581Z","shell.execute_reply":"2025-11-15T12:51:26.673997Z"},"papermill":{"duration":0.014258,"end_time":"2025-11-15T12:51:26.675559","exception":false,"start_time":"2025-11-15T12:51:26.661301","status":"completed"},"tags":[]},"outputs":[],"source":["df_trans.head(1)\n","df_tmp = df_trans"]},{"cell_type":"code","execution_count":23,"id":"07292325","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:51:26.693481Z","iopub.status.busy":"2025-11-15T12:51:26.693273Z","iopub.status.idle":"2025-11-15T12:51:31.646682Z","shell.execute_reply":"2025-11-15T12:51:31.645952Z"},"papermill":{"duration":4.964204,"end_time":"2025-11-15T12:51:31.648239","exception":false,"start_time":"2025-11-15T12:51:26.684035","status":"completed"},"tags":[]},"outputs":[],"source":["import tensorflow as tf\n","\n","df_trans = df_tmp\n","\n","# Data processing\n","df_trans['cleaned_text'] = df_trans['text'].apply(clean_text)\n","df_trans['cleaned_summary'] = df_trans['summary'].apply(clean_text)\n","\n","df_trans = df_trans.dropna(subset=['cleaned_text', 'cleaned_summary'])\n","df_trans = df_trans.drop_duplicates(subset=['cleaned_text', 'cleaned_summary'])\n","\n","# df_trans['cleaned_summary'] = df_trans['cleaned_summary'].apply(lambda x: '<SOS> ' + x + ' <EOS>')\n","\n","# mask = (df_trans['cleaned_text'].str.split().str.len() <= max_text_len) & \\\n","#        (df_trans['cleaned_summary'].str.split().str.len() <= max_summary_len)\n","# df_trans = df_trans.loc[mask].reset_index(drop=True)\n","\n","# # Tokenizer\n","# oov_token = '<unk>'\n","# filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n","\n","\n","# x_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n","# x_tokenizer.fit_on_texts(df_trans['cleaned_text'])\n","\n","# y_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)\n","# y_tokenizer.fit_on_texts(df_trans['cleaned_summary'])\n","\n","# inputs = x_tokenizer.texts_to_sequences(df_trans['cleaned_text'])\n","# targets = y_tokenizer.texts_to_sequences(df_trans['cleaned_summary'])\n","\n","# # Padding\n","# inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=max_text_len, padding='post', truncating='post')\n","# targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=max_summary_len, padding='post', truncating='post')\n","\n","# inputs = tf.cast(inputs, dtype=tf.int64)\n","# targets = tf.cast(targets, dtype=tf.int64)\n","\n","# # Vocab. size (+1 for padding token)\n","# x_voc_size = len(x_tokenizer.word_index) + 1\n","# y_voc_size = len(y_tokenizer.word_index) + 1\n","\n","# print(f\"Size of vocabulary in X = {x_voc_size}\")\n","# print(f\"Size of vocabulary in Y = {y_voc_size}\")"]},{"cell_type":"code","execution_count":24,"id":"ee91bd73","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:51:31.668332Z","iopub.status.busy":"2025-11-15T12:51:31.667773Z","iopub.status.idle":"2025-11-15T12:52:16.46546Z","shell.execute_reply":"2025-11-15T12:52:16.464687Z"},"papermill":{"duration":44.818096,"end_time":"2025-11-15T12:52:16.475699","exception":false,"start_time":"2025-11-15T12:51:31.657603","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["text_sample.txt loaded\n","summary_sample.txt loaded\n","sp_text.model loaded\n","sp_summary.model loaded\n","Size of vocabulary in X = 20000\n","Size of vocabulary in Y = 20000\n"]}],"source":["import sentencepiece as spm\n","\n","text_file = \"text_sample.txt\"\n","summary_file = \"summary_sample.txt\"\n","text_model_file = \"sp_text.model\"\n","summary_model_file = \"sp_summary.model\"\n","\n","\n","# Text\n","if os.path.exists(f\"/kaggle/input/sentencepiece/{text_file}\"):\n","    print(f\"{text_file} loaded\")\n","    text_path = f\"/kaggle/input/sentencepiece/{text_file}\"\n","else:\n","    text_path = f\"/kaggle/working/{text_file}\"\n","    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n","        for t in df_trans['cleaned_text']:\n","            f.write(t + \"\\n\")\n","\n","# Summary\n","if os.path.exists(f\"/kaggle/input/sentencepiece/{summary_file}\"):\n","    print(f\"{summary_file} loaded\")\n","    summary_path = f\"/kaggle/input/sentencepiece/{summary_file}\"\n","else:\n","    summary_path = f\"/kaggle/working/{summary_file}\"\n","    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n","        for t in df_trans['cleaned_summary']:\n","            f.write(t + \"\\n\")\n","\n","# Text model\n","if os.path.exists(f\"/kaggle/input/sentencepiece/{text_model_file}\"):\n","    print(f\"{text_model_file} loaded\")\n","    text_model_path = f\"/kaggle/input/sentencepiece/{text_model_file}\"\n","else:\n","    text_model_path = f\"/kaggle/working/{text_model_file}\"\n","    spm.SentencePieceTrainer.Train(\n","        f'--input={text_path} '\n","        f'--model_prefix=/kaggle/working/sp_text '\n","        '--vocab_size=20000 --model_type=bpe '\n","        '--user_defined_symbols=<SOS>,<EOS> --num_threads=8'\n","    )\n","\n","# Summary model\n","if os.path.exists(f\"/kaggle/input/sentencepiece/{summary_model_file}\"):\n","    print(f\"{summary_model_file} loaded\")\n","    summary_model_path = f\"/kaggle/input/sentencepiece/{summary_model_file}\"\n","else:\n","    summary_model_path = f\"/kaggle/working/{summary_model_file}\"\n","    spm.SentencePieceTrainer.Train(\n","        f'--input={summary_path} '\n","        f'--model_prefix=/kaggle/working/sp_summary '\n","        '--vocab_size=20000 --model_type=bpe '\n","        '--user_defined_symbols=<SOS>,<EOS> --num_threads=8'\n","    )\n","\n","sp_text = spm.SentencePieceProcessor()\n","sp_text.load(text_model_path)\n","\n","sp_summary = spm.SentencePieceProcessor()\n","sp_summary.load(summary_model_path)\n","\n","inputs = [sp_text.encode_as_ids(t) for t in df_trans['cleaned_text']]\n","targets = [sp_summary.encode_as_ids(t) for t in df_trans['cleaned_summary']]\n","\n","inputs = pad_sequences(inputs, maxlen=max_text_len, padding='post', truncating='post')\n","targets = pad_sequences(targets, maxlen=max_summary_len, padding='post', truncating='post')\n","\n","inputs = tf.cast(inputs, dtype=tf.int64)\n","targets = tf.cast(targets, dtype=tf.int64)\n","\n","x_voc_size = sp_text.get_piece_size()\n","y_voc_size = sp_summary.get_piece_size()\n","\n","print(f\"Size of vocabulary in X = {x_voc_size}\")\n","print(f\"Size of vocabulary in Y = {y_voc_size}\")"]},{"cell_type":"code","execution_count":25,"id":"9e1ae429","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:52:16.494685Z","iopub.status.busy":"2025-11-15T12:52:16.49443Z","iopub.status.idle":"2025-11-15T12:52:17.677181Z","shell.execute_reply":"2025-11-15T12:52:17.676602Z"},"papermill":{"duration":1.193894,"end_time":"2025-11-15T12:52:17.67849","exception":false,"start_time":"2025-11-15T12:52:16.484596","status":"completed"},"tags":[]},"outputs":[],"source":["#dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(10000).batch(32)\n","dataset = (tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(10000, seed=42, reshuffle_each_iteration=False).batch(32))\n","\n","\n","def get_angles(position, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n","    return position * angle_rates\n","\n","def positional_encoding(position, d_model):\n","    angle_rads = get_angles(\n","        np.arange(position)[:, np.newaxis],\n","        np.arange(d_model)[np.newaxis, :],\n","        d_model\n","    )\n","\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = angle_rads[np.newaxis, ...]\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","def create_padding_mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","    return seq[:, tf.newaxis, tf.newaxis, :]\n","\n","def create_look_ahead_mask(size):\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask\n","\n","def scaled_dot_product_attention(q, k, v, mask):\n","    matmul_qk = tf.matmul(q, k, transpose_b=True)\n","\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    #dk = tf.cast(tf.shape(k)[-1], q.dtype)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)  \n","\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n","\n","    output = tf.matmul(attention_weights, v)\n","    return output, attention_weights\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.wq = tf.keras.layers.Dense(d_model)\n","        self.wk = tf.keras.layers.Dense(d_model)\n","        self.wv = tf.keras.layers.Dense(d_model)\n","\n","        self.dense = tf.keras.layers.Dense(d_model)\n","        \n","    def split_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","    \n","    def call(self, v, k, q, mask):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q)\n","        k = self.wk(k)\n","        v = self.wv(v)\n","\n","        q = self.split_heads(q, batch_size)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","\n","        scaled_attention, attention_weights = scaled_dot_product_attention(\n","            q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","        \n","        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n","        output = self.dense(concat_attention)\n","            \n","        return output, attention_weights\n","    \n","def point_wise_feed_forward_network(d_model, dff):\n","    return tf.keras.Sequential([\n","        tf.keras.layers.Dense(dff, activation='relu'),\n","        tf.keras.layers.Dense(d_model)\n","    ])\n","\n","\n","class EncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(EncoderLayer, self).__init__()\n","\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","    \n","    def call(self, x, training=False, mask=None):\n","        attn_output, _ = self.mha(x, x, x, mask)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output)\n","    \n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        out2 = self.layernorm2(out1 + ffn_output)\n","    \n","        return out2\n","\n","\n","\n","class DecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(DecoderLayer, self).__init__()\n","\n","        self.mha1 = MultiHeadAttention(d_model, num_heads)\n","        self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","        self.dropout3 = tf.keras.layers.Dropout(rate)\n","    \n","    \n","    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n","        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n","        attn1 = self.dropout1(attn1, training=training)\n","        out1 = self.layernorm1(attn1 + x)\n","    \n","        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n","        attn2 = self.dropout2(attn2, training=training)\n","        out2 = self.layernorm2(attn2 + out1)\n","    \n","        ffn_output = self.ffn(out2)\n","        ffn_output = self.dropout3(ffn_output, training=training)\n","        out3 = self.layernorm3(ffn_output + out2)\n","    \n","        return out3, attn_weights_block1, attn_weights_block2\n","\n","\n","\n","class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n","        super(Encoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n","\n","        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","        \n","    def call(self, x, training=False, mask=None):\n","        seq_len = tf.shape(x)[1]\n","    \n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","        #x *= tf.math.sqrt(tf.cast(self.d_model, x.dtype)) \n","        #x += tf.cast(self.pos_encoding[:, :seq_len, :], x.dtype)\n","\n","        x = self.dropout(x, training=training)\n","    \n","        for i in range(self.num_layers):\n","            x = self.enc_layers[i](x, training=training, mask=mask)\n","    \n","        return x\n","    \n","class Decoder(tf.keras.layers.Layer):\n","        \n","    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n","        super(Decoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","\n","        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","    \n","    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n","        seq_len = tf.shape(x)[1]\n","        attention_weights = {}\n","    \n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","        #x *= tf.math.sqrt(tf.cast(self.d_model, x.dtype))              \n","        #x += tf.cast(self.pos_encoding[:, :seq_len, :], x.dtype)      \n","\n","    \n","        x = self.dropout(x, training=training)\n","    \n","        for i in range(self.num_layers):\n","            x, block1, block2 = self.dec_layers[i](\n","                x, \n","                enc_output, \n","                training=training, \n","                look_ahead_mask=look_ahead_mask, \n","                padding_mask=padding_mask\n","            )\n","            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n","            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n","    \n","        return x, attention_weights\n","\n","\n","\n","\n","class Transformer(tf.keras.Model):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n","        super(Transformer, self).__init__()\n","\n","        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n","        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n","        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","    \n","    def call(self, inp, tar, training=False, enc_padding_mask=None, look_ahead_mask=None, dec_padding_mask=None):\n","        enc_output = self.encoder(x=inp, training=training, mask=enc_padding_mask)\n","        dec_output, attention_weights = self.decoder(x=tar, enc_output=enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)\n","        final_output = self.final_layer(dec_output)\n","        return final_output, attention_weights"]},{"cell_type":"code","execution_count":26,"id":"2a02ddbd","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:52:17.699491Z","iopub.status.busy":"2025-11-15T12:52:17.699222Z","iopub.status.idle":"2025-11-15T12:52:17.705807Z","shell.execute_reply":"2025-11-15T12:52:17.705193Z"},"papermill":{"duration":0.019103,"end_time":"2025-11-15T12:52:17.706956","exception":false,"start_time":"2025-11-15T12:52:17.687853","status":"completed"},"tags":[]},"outputs":[],"source":["import time\n","\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n","\n","\n","def accuracy_function(real, pred):\n","    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n","    #accuracies = tf.cast(accuracies, dtype= tf.float32)\n","\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    accuracies = tf.math.logical_and(mask, accuracies)\n","\n","    accuracies = tf.cast(accuracies, dtype=tf.float32)\n","    mask = tf.cast(mask, dtype=tf.float32)\n","    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n","\n","def create_masks(inp, tar):\n","    enc_padding_mask = create_padding_mask(inp)\n","    dec_padding_mask = create_padding_mask(inp)\n","\n","    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","    dec_target_padding_mask = create_padding_mask(tar)\n","    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","  \n","    return enc_padding_mask, combined_mask, dec_padding_mask"]},{"cell_type":"code","execution_count":27,"id":"247067fb","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:52:17.726491Z","iopub.status.busy":"2025-11-15T12:52:17.726231Z","iopub.status.idle":"2025-11-15T12:52:17.731807Z","shell.execute_reply":"2025-11-15T12:52:17.731075Z"},"papermill":{"duration":0.01659,"end_time":"2025-11-15T12:52:17.732934","exception":false,"start_time":"2025-11-15T12:52:17.716344","status":"completed"},"tags":[]},"outputs":[],"source":["num_layers = 4\n","d_model = 256\n","dff = 1024\n","num_heads = 8\n","dropout_rate = 0.2\n","\n","# num_layers = 2       \n","# d_model = 128        \n","# dff = 512            \n","# num_heads = 4        \n","# dropout_rate = 0.1 \n","\n","class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=2000):\n","        super(CustomSchedule, self).__init__()\n","\n","        self.d_model = d_model\n","        self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","        self.warmup_steps = warmup_steps\n","    \n","    def __call__(self, step):\n","        step = tf.cast(step, tf.float32) \n","        step = tf.maximum(step, 1.0)\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps ** -1.5)\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","\n","\n","\n","#learning_rate = CustomSchedule(d_model)\n","\n","#optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"]},{"cell_type":"code","execution_count":28,"id":"f0dd1f5b","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:52:17.752252Z","iopub.status.busy":"2025-11-15T12:52:17.751512Z","iopub.status.idle":"2025-11-15T12:52:25.2703Z","shell.execute_reply":"2025-11-15T12:52:25.269516Z"},"papermill":{"duration":7.52945,"end_time":"2025-11-15T12:52:25.271514","exception":false,"start_time":"2025-11-15T12:52:17.742064","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"transformer\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)               │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,279,040</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)               │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">9,333,760</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,140,000</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ encoder (\u001b[38;5;33mEncoder\u001b[0m)               │ ?                      │     \u001b[38;5;34m8,279,040\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ decoder (\u001b[38;5;33mDecoder\u001b[0m)               │ ?                      │     \u001b[38;5;34m9,333,760\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_65 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20000\u001b[0m)         │     \u001b[38;5;34m5,140,000\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,752,800</span> (86.80 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m22,752,800\u001b[0m (86.80 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,752,800</span> (86.80 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m22,752,800\u001b[0m (86.80 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Checkpoint restored from /kaggle/input/checkpoint5/ckpt-5\n","Learning rate: 0.000397919706\n"]}],"source":["first_epoch = 0\n","\n","transformer = Transformer(\n","    num_layers=num_layers,\n","    d_model=d_model,\n","    num_heads=num_heads,\n","    dff=dff,\n","    input_vocab_size=x_voc_size,\n","    target_vocab_size=y_voc_size,\n","    pe_input=1000,\n","    pe_target=1000,\n","    rate=dropout_rate\n",")\n","\n","dummy_input = tf.constant([[1]*max_text_len], dtype=tf.int64)\n","dummy_target = tf.constant([[1]*max_summary_len], dtype=tf.int64)\n","_ = transformer(dummy_input, dummy_target, training=False)\n","transformer.summary()\n","\n","#optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","learning_rate = CustomSchedule(d_model)\n","optimizer = tf.keras.optimizers.Adam(\n","    learning_rate=learning_rate,\n","    beta_1=0.9,\n","    beta_2=0.98,\n","    epsilon=1e-9\n",")\n","\n","\n","\n","checkpoint_path = \"/kaggle/working/checkpoints\"\n","ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n","#ckpt = tf.train.Checkpoint(transformer=transformer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n","\n","\n","curr_ckpt = 5\n","checkpoint_path_input = f\"/kaggle/input/checkpoint{curr_ckpt}/ckpt-{curr_ckpt}\"\n","if os.path.exists(checkpoint_path_input + \".index\"):\n","    #ckpt.restore(checkpoint_path_input).assert_existing_objects_matched()\n","    status = ckpt.restore(checkpoint_path_input)\n","    status.expect_partial()\n","    #status.assert_existing_objects_matched()  \n","    print(f\"Checkpoint restored from {checkpoint_path_input}\")\n","    first_epoch = curr_ckpt\n","\n","tf.print(\"Learning rate:\", optimizer.learning_rate)\n","\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n","\n","\n","@tf.function\n","def train_step(inp, tar):\n","    tar_inp = tar[:, :-1]\n","    tar_real = tar[:, 1:]\n","\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","\n","    with tf.GradientTape() as tape:\n","        predictions, _ = transformer(\n","            inp, \n","            tar_inp, \n","            training=True, \n","            enc_padding_mask=enc_padding_mask, \n","            look_ahead_mask=combined_mask, \n","            dec_padding_mask=dec_padding_mask\n","        )\n","\n","\n","        loss = loss_function(tar_real, predictions)\n","\n","    gradients = tape.gradient(loss, transformer.trainable_variables)    \n","    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","\n","    train_loss.update_state(loss)\n","    train_accuracy.update_state(accuracy_function(tar_real, predictions))"]},{"cell_type":"code","execution_count":29,"id":"d60c7ed7","metadata":{"execution":{"iopub.execute_input":"2025-11-15T12:52:25.291677Z","iopub.status.busy":"2025-11-15T12:52:25.291406Z","iopub.status.idle":"2025-11-15T13:14:22.406777Z","shell.execute_reply":"2025-11-15T13:14:22.406016Z"},"papermill":{"duration":1317.126641,"end_time":"2025-11-15T13:14:22.407996","exception":false,"start_time":"2025-11-15T12:52:25.281355","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 6 Batch 100/4934 Accuracy 0.0224 Loss 8.7218 Time elapsed 44.66s\n","Epoch 6 Batch 200/4934 Accuracy 0.0238 Loss 8.3534 Time elapsed 53.20s\n","Epoch 6 Batch 300/4934 Accuracy 0.0252 Loss 8.1983 Time elapsed 61.73s\n","Epoch 6 Batch 400/4934 Accuracy 0.0394 Loss 7.9980 Time elapsed 70.27s\n","Epoch 6 Batch 500/4934 Accuracy 0.0567 Loss 7.7883 Time elapsed 78.81s\n","Epoch 6 Batch 600/4934 Accuracy 0.0706 Loss 7.6006 Time elapsed 87.34s\n","Epoch 6 Batch 700/4934 Accuracy 0.0820 Loss 7.4325 Time elapsed 95.88s\n","Epoch 6 Batch 800/4934 Accuracy 0.0909 Loss 7.2943 Time elapsed 104.42s\n","Epoch 6 Batch 900/4934 Accuracy 0.0989 Loss 7.1696 Time elapsed 112.96s\n","Epoch 6 Batch 1000/4934 Accuracy 0.1058 Loss 7.0624 Time elapsed 121.50s\n","Epoch 6 Batch 1100/4934 Accuracy 0.1114 Loss 6.9748 Time elapsed 130.03s\n","Epoch 6 Batch 1200/4934 Accuracy 0.1162 Loss 6.8968 Time elapsed 138.57s\n","Epoch 6 Batch 1300/4934 Accuracy 0.1207 Loss 6.8271 Time elapsed 147.11s\n","Epoch 6 Batch 1400/4934 Accuracy 0.1250 Loss 6.7625 Time elapsed 155.64s\n","Epoch 6 Batch 1500/4934 Accuracy 0.1284 Loss 6.7070 Time elapsed 164.18s\n","Epoch 6 Batch 1600/4934 Accuracy 0.1315 Loss 6.6582 Time elapsed 172.72s\n","Epoch 6 Batch 1700/4934 Accuracy 0.1344 Loss 6.6149 Time elapsed 181.26s\n","Epoch 6 Batch 1800/4934 Accuracy 0.1370 Loss 6.5752 Time elapsed 189.79s\n","Epoch 6 Batch 1900/4934 Accuracy 0.1394 Loss 6.5396 Time elapsed 198.33s\n","Epoch 6 Batch 2000/4934 Accuracy 0.1416 Loss 6.5062 Time elapsed 206.87s\n","Epoch 6 Batch 2100/4934 Accuracy 0.1437 Loss 6.4752 Time elapsed 215.41s\n","Epoch 6 Batch 2200/4934 Accuracy 0.1458 Loss 6.4442 Time elapsed 223.95s\n","Epoch 6 Batch 2300/4934 Accuracy 0.1476 Loss 6.4182 Time elapsed 232.49s\n","Epoch 6 Batch 2400/4934 Accuracy 0.1493 Loss 6.3931 Time elapsed 241.03s\n","Epoch 6 Batch 2500/4934 Accuracy 0.1509 Loss 6.3696 Time elapsed 249.56s\n","Epoch 6 Batch 2600/4934 Accuracy 0.1524 Loss 6.3488 Time elapsed 258.10s\n","Epoch 6 Batch 2700/4934 Accuracy 0.1538 Loss 6.3288 Time elapsed 266.63s\n","Epoch 6 Batch 2800/4934 Accuracy 0.1551 Loss 6.3098 Time elapsed 275.17s\n","Epoch 6 Batch 2900/4934 Accuracy 0.1562 Loss 6.2915 Time elapsed 283.71s\n","Epoch 6 Batch 3000/4934 Accuracy 0.1570 Loss 6.2742 Time elapsed 292.25s\n","Epoch 6 Batch 3100/4934 Accuracy 0.1575 Loss 6.2589 Time elapsed 300.79s\n","Epoch 6 Batch 3200/4934 Accuracy 0.1578 Loss 6.2470 Time elapsed 309.32s\n","Epoch 6 Batch 3300/4934 Accuracy 0.1581 Loss 6.2350 Time elapsed 317.86s\n","Epoch 6 Batch 3400/4934 Accuracy 0.1584 Loss 6.2247 Time elapsed 326.41s\n","Epoch 6 Batch 3500/4934 Accuracy 0.1588 Loss 6.2135 Time elapsed 334.95s\n","Epoch 6 Batch 3600/4934 Accuracy 0.1592 Loss 6.2049 Time elapsed 343.49s\n","Epoch 6 Batch 3700/4934 Accuracy 0.1595 Loss 6.1965 Time elapsed 352.03s\n","Epoch 6 Batch 3800/4934 Accuracy 0.1597 Loss 6.1884 Time elapsed 360.57s\n","Epoch 6 Batch 3900/4934 Accuracy 0.1598 Loss 6.1815 Time elapsed 369.11s\n","Epoch 6 Batch 4000/4934 Accuracy 0.1601 Loss 6.1749 Time elapsed 377.65s\n","Epoch 6 Batch 4100/4934 Accuracy 0.1604 Loss 6.1684 Time elapsed 386.19s\n","Epoch 6 Batch 4200/4934 Accuracy 0.1606 Loss 6.1624 Time elapsed 394.73s\n","Epoch 6 Batch 4300/4934 Accuracy 0.1610 Loss 6.1555 Time elapsed 403.27s\n","Epoch 6 Batch 4400/4934 Accuracy 0.1613 Loss 6.1483 Time elapsed 411.81s\n","Epoch 6 Batch 4500/4934 Accuracy 0.1617 Loss 6.1415 Time elapsed 420.35s\n","Epoch 6 Batch 4600/4934 Accuracy 0.1621 Loss 6.1344 Time elapsed 428.89s\n","Epoch 6 Batch 4700/4934 Accuracy 0.1625 Loss 6.1273 Time elapsed 437.43s\n","Epoch 6 Batch 4800/4934 Accuracy 0.1630 Loss 6.1201 Time elapsed 445.96s\n","Epoch 6 Batch 4900/4934 Accuracy 0.1635 Loss 6.1139 Time elapsed 454.50s\n","Epoch 6 Batch 4934/4934 Accuracy 0.1636 Loss 6.1119 Time elapsed 470.58s\n","Epoch 6 Accuracy 0.1636 Loss 6.1119 Time 472.56 seconds\n","\n","Epoch 7 Batch 100/4934 Accuracy 0.1937 Loss 5.7159 Time elapsed 8.54s\n","Epoch 7 Batch 200/4934 Accuracy 0.1957 Loss 5.6690 Time elapsed 17.08s\n","Epoch 7 Batch 300/4934 Accuracy 0.1989 Loss 5.6192 Time elapsed 25.62s\n","Epoch 7 Batch 400/4934 Accuracy 0.2009 Loss 5.5849 Time elapsed 34.15s\n","Epoch 7 Batch 500/4934 Accuracy 0.2026 Loss 5.5531 Time elapsed 42.68s\n","Epoch 7 Batch 600/4934 Accuracy 0.2032 Loss 5.5362 Time elapsed 51.22s\n","Epoch 7 Batch 700/4934 Accuracy 0.2047 Loss 5.5166 Time elapsed 59.76s\n","Epoch 7 Batch 800/4934 Accuracy 0.2054 Loss 5.5043 Time elapsed 68.30s\n","Epoch 7 Batch 900/4934 Accuracy 0.2072 Loss 5.4855 Time elapsed 76.84s\n","Epoch 7 Batch 1000/4934 Accuracy 0.2083 Loss 5.4714 Time elapsed 85.37s\n","Epoch 7 Batch 1100/4934 Accuracy 0.2091 Loss 5.4623 Time elapsed 93.91s\n","Epoch 7 Batch 1200/4934 Accuracy 0.2102 Loss 5.4526 Time elapsed 102.45s\n","Epoch 7 Batch 1300/4934 Accuracy 0.2109 Loss 5.4440 Time elapsed 110.98s\n","Epoch 7 Batch 1400/4934 Accuracy 0.2118 Loss 5.4348 Time elapsed 119.52s\n","Epoch 7 Batch 1500/4934 Accuracy 0.2124 Loss 5.4282 Time elapsed 128.06s\n","Epoch 7 Batch 1600/4934 Accuracy 0.2129 Loss 5.4232 Time elapsed 136.60s\n","Epoch 7 Batch 1700/4934 Accuracy 0.2132 Loss 5.4206 Time elapsed 145.14s\n","Epoch 7 Batch 1800/4934 Accuracy 0.2136 Loss 5.4177 Time elapsed 153.68s\n","Epoch 7 Batch 1900/4934 Accuracy 0.2139 Loss 5.4157 Time elapsed 162.21s\n","Epoch 7 Batch 2000/4934 Accuracy 0.2142 Loss 5.4140 Time elapsed 170.75s\n","Epoch 7 Batch 2100/4934 Accuracy 0.2145 Loss 5.4123 Time elapsed 179.29s\n","Epoch 7 Batch 2200/4934 Accuracy 0.2150 Loss 5.4088 Time elapsed 187.83s\n","Epoch 7 Batch 2300/4934 Accuracy 0.2152 Loss 5.4083 Time elapsed 196.36s\n","Epoch 7 Batch 2400/4934 Accuracy 0.2154 Loss 5.4067 Time elapsed 204.90s\n","Epoch 7 Batch 2500/4934 Accuracy 0.2156 Loss 5.4060 Time elapsed 213.44s\n","Epoch 7 Batch 2600/4934 Accuracy 0.2157 Loss 5.4061 Time elapsed 221.98s\n","Epoch 7 Batch 2700/4934 Accuracy 0.2159 Loss 5.4058 Time elapsed 230.52s\n","Epoch 7 Batch 2800/4934 Accuracy 0.2160 Loss 5.4052 Time elapsed 239.06s\n","Epoch 7 Batch 2900/4934 Accuracy 0.2161 Loss 5.4035 Time elapsed 247.59s\n","Epoch 7 Batch 3000/4934 Accuracy 0.2159 Loss 5.4016 Time elapsed 256.13s\n","Epoch 7 Batch 3100/4934 Accuracy 0.2155 Loss 5.4011 Time elapsed 264.66s\n","Epoch 7 Batch 3200/4934 Accuracy 0.2150 Loss 5.4034 Time elapsed 273.20s\n","Epoch 7 Batch 3300/4934 Accuracy 0.2146 Loss 5.4047 Time elapsed 281.74s\n","Epoch 7 Batch 3400/4934 Accuracy 0.2140 Loss 5.4075 Time elapsed 290.27s\n","Epoch 7 Batch 3500/4934 Accuracy 0.2136 Loss 5.4089 Time elapsed 298.81s\n","Epoch 7 Batch 3600/4934 Accuracy 0.2132 Loss 5.4121 Time elapsed 307.35s\n","Epoch 7 Batch 3700/4934 Accuracy 0.2126 Loss 5.4156 Time elapsed 315.88s\n","Epoch 7 Batch 3800/4934 Accuracy 0.2122 Loss 5.4187 Time elapsed 324.42s\n","Epoch 7 Batch 3900/4934 Accuracy 0.2118 Loss 5.4227 Time elapsed 332.96s\n","Epoch 7 Batch 4000/4934 Accuracy 0.2114 Loss 5.4264 Time elapsed 341.50s\n","Epoch 7 Batch 4100/4934 Accuracy 0.2111 Loss 5.4299 Time elapsed 350.04s\n","Epoch 7 Batch 4200/4934 Accuracy 0.2107 Loss 5.4337 Time elapsed 358.57s\n","Epoch 7 Batch 4300/4934 Accuracy 0.2105 Loss 5.4360 Time elapsed 367.11s\n","Epoch 7 Batch 4400/4934 Accuracy 0.2103 Loss 5.4378 Time elapsed 375.65s\n","Epoch 7 Batch 4500/4934 Accuracy 0.2102 Loss 5.4395 Time elapsed 384.19s\n","Epoch 7 Batch 4600/4934 Accuracy 0.2101 Loss 5.4411 Time elapsed 392.73s\n","Epoch 7 Batch 4700/4934 Accuracy 0.2100 Loss 5.4422 Time elapsed 401.27s\n","Epoch 7 Batch 4800/4934 Accuracy 0.2100 Loss 5.4428 Time elapsed 409.81s\n","Epoch 7 Batch 4900/4934 Accuracy 0.2100 Loss 5.4445 Time elapsed 418.35s\n","Epoch 7 Batch 4934/4934 Accuracy 0.2100 Loss 5.4450 Time elapsed 421.26s\n","Epoch 7 Accuracy 0.2100 Loss 5.4450 Time 422.26 seconds\n","\n","Epoch 8 Batch 100/4934 Accuracy 0.2151 Loss 5.4267 Time elapsed 8.53s\n","Epoch 8 Batch 200/4934 Accuracy 0.2174 Loss 5.3820 Time elapsed 17.07s\n","Epoch 8 Batch 300/4934 Accuracy 0.2208 Loss 5.3365 Time elapsed 25.60s\n","Epoch 8 Batch 400/4934 Accuracy 0.2223 Loss 5.3043 Time elapsed 34.13s\n","Epoch 8 Batch 500/4934 Accuracy 0.2242 Loss 5.2751 Time elapsed 42.67s\n","Epoch 8 Batch 600/4934 Accuracy 0.2249 Loss 5.2602 Time elapsed 51.20s\n","Epoch 8 Batch 700/4934 Accuracy 0.2263 Loss 5.2423 Time elapsed 59.74s\n","Epoch 8 Batch 800/4934 Accuracy 0.2270 Loss 5.2326 Time elapsed 68.28s\n","Epoch 8 Batch 900/4934 Accuracy 0.2286 Loss 5.2166 Time elapsed 76.81s\n","Epoch 8 Batch 1000/4934 Accuracy 0.2296 Loss 5.2039 Time elapsed 85.35s\n","Epoch 8 Batch 1100/4934 Accuracy 0.2301 Loss 5.1962 Time elapsed 93.88s\n","Epoch 8 Batch 1200/4934 Accuracy 0.2310 Loss 5.1888 Time elapsed 102.42s\n","Epoch 8 Batch 1300/4934 Accuracy 0.2314 Loss 5.1822 Time elapsed 110.95s\n","Epoch 8 Batch 1400/4934 Accuracy 0.2322 Loss 5.1736 Time elapsed 119.49s\n","Epoch 8 Batch 1500/4934 Accuracy 0.2326 Loss 5.1688 Time elapsed 128.03s\n","Epoch 8 Batch 1600/4934 Accuracy 0.2330 Loss 5.1645 Time elapsed 136.56s\n","Epoch 8 Batch 1700/4934 Accuracy 0.2331 Loss 5.1629 Time elapsed 145.10s\n","Epoch 8 Batch 1800/4934 Accuracy 0.2333 Loss 5.1615 Time elapsed 153.64s\n","Epoch 8 Batch 1900/4934 Accuracy 0.2336 Loss 5.1607 Time elapsed 162.18s\n","Epoch 8 Batch 2000/4934 Accuracy 0.2339 Loss 5.1601 Time elapsed 170.71s\n","Epoch 8 Batch 2100/4934 Accuracy 0.2341 Loss 5.1600 Time elapsed 179.25s\n","Epoch 8 Batch 2200/4934 Accuracy 0.2345 Loss 5.1579 Time elapsed 187.79s\n","Epoch 8 Batch 2300/4934 Accuracy 0.2346 Loss 5.1587 Time elapsed 196.33s\n","Epoch 8 Batch 2400/4934 Accuracy 0.2349 Loss 5.1581 Time elapsed 204.86s\n","Epoch 8 Batch 2500/4934 Accuracy 0.2349 Loss 5.1587 Time elapsed 213.40s\n","Epoch 8 Batch 2600/4934 Accuracy 0.2350 Loss 5.1604 Time elapsed 221.94s\n","Epoch 8 Batch 2700/4934 Accuracy 0.2351 Loss 5.1613 Time elapsed 230.48s\n","Epoch 8 Batch 2800/4934 Accuracy 0.2351 Loss 5.1617 Time elapsed 239.01s\n","Epoch 8 Batch 2900/4934 Accuracy 0.2352 Loss 5.1605 Time elapsed 247.55s\n","Epoch 8 Batch 3000/4934 Accuracy 0.2350 Loss 5.1591 Time elapsed 256.08s\n","Epoch 8 Batch 3100/4934 Accuracy 0.2346 Loss 5.1585 Time elapsed 264.62s\n","Epoch 8 Batch 3200/4934 Accuracy 0.2341 Loss 5.1609 Time elapsed 273.16s\n","Epoch 8 Batch 3300/4934 Accuracy 0.2336 Loss 5.1628 Time elapsed 281.69s\n","Epoch 8 Batch 3400/4934 Accuracy 0.2330 Loss 5.1661 Time elapsed 290.28s\n","Epoch 8 Batch 3500/4934 Accuracy 0.2326 Loss 5.1681 Time elapsed 298.81s\n","Epoch 8 Batch 3600/4934 Accuracy 0.2321 Loss 5.1722 Time elapsed 307.36s\n","Epoch 8 Batch 3700/4934 Accuracy 0.2316 Loss 5.1763 Time elapsed 315.90s\n","Epoch 8 Batch 3800/4934 Accuracy 0.2310 Loss 5.1801 Time elapsed 324.44s\n","Epoch 8 Batch 3900/4934 Accuracy 0.2305 Loss 5.1847 Time elapsed 332.98s\n","Epoch 8 Batch 4000/4934 Accuracy 0.2301 Loss 5.1891 Time elapsed 341.52s\n","Epoch 8 Batch 4100/4934 Accuracy 0.2297 Loss 5.1935 Time elapsed 350.06s\n","Epoch 8 Batch 4200/4934 Accuracy 0.2294 Loss 5.1980 Time elapsed 358.60s\n","Epoch 8 Batch 4300/4934 Accuracy 0.2291 Loss 5.2009 Time elapsed 367.14s\n","Epoch 8 Batch 4400/4934 Accuracy 0.2290 Loss 5.2034 Time elapsed 375.68s\n","Epoch 8 Batch 4500/4934 Accuracy 0.2288 Loss 5.2057 Time elapsed 384.22s\n","Epoch 8 Batch 4600/4934 Accuracy 0.2286 Loss 5.2079 Time elapsed 392.76s\n","Epoch 8 Batch 4700/4934 Accuracy 0.2285 Loss 5.2099 Time elapsed 401.30s\n","Epoch 8 Batch 4800/4934 Accuracy 0.2285 Loss 5.2110 Time elapsed 409.84s\n","Epoch 8 Batch 4900/4934 Accuracy 0.2285 Loss 5.2134 Time elapsed 418.38s\n","Epoch 8 Batch 4934/4934 Accuracy 0.2285 Loss 5.2142 Time elapsed 421.29s\n","Epoch 8 Accuracy 0.2285 Loss 5.2142 Time 422.29 seconds\n","\n"]}],"source":["patience = 3  \n","best_loss = float('inf')\n","wait = 0\n","total_batches = tf.data.experimental.cardinality(dataset).numpy()\n","epochs = 3\n","\n","\n","for epoch in range(first_epoch, first_epoch + epochs):\n","    start = time.time()\n","    start_epoch = time.time()\n","    train_loss.reset_state()\n","    train_accuracy.reset_state()\n","\n","\n","    for (batch, (inp, tar)) in enumerate(dataset):\n","        start_batch = time.time()\n","        train_step(inp, tar)\n","        \n","        if (batch + 1) % 100 == 0 or (batch + 1) == total_batches:\n","            elapsed_batch = time.time() - start_epoch\n","            print(f\"Epoch {epoch+1} Batch {batch+1}/{total_batches} \"\n","                  f\"Accuracy {train_accuracy.result():.4f} \"\n","                  f\"Loss {train_loss.result():.4f} \"\n","                  f\"Time elapsed {elapsed_batch:.2f}s\")\n","\n","    current_loss = train_loss.result()\n","    \n","    if current_loss < best_loss:\n","        best_loss = current_loss\n","        wait = 0\n","        ckpt_save_path = ckpt_manager.save()\n","    else:\n","        wait += 1\n","        if wait >= patience:\n","            print(f\"Early stopping triggered at epoch {epoch+1}\")\n","            break\n","\n","    print(f'Epoch {epoch + 1} Accuracy {train_accuracy.result():.4f} Loss {current_loss:.4f} Time {time.time() - start:.2f} seconds\\n')"]},{"cell_type":"code","execution_count":30,"id":"e23c50bf","metadata":{"execution":{"iopub.execute_input":"2025-11-15T13:14:22.439433Z","iopub.status.busy":"2025-11-15T13:14:22.438936Z","iopub.status.idle":"2025-11-15T13:14:22.446676Z","shell.execute_reply":"2025-11-15T13:14:22.445955Z"},"papermill":{"duration":0.024392,"end_time":"2025-11-15T13:14:22.447768","exception":false,"start_time":"2025-11-15T13:14:22.423376","status":"completed"},"tags":[]},"outputs":[],"source":["def summarize(input_article, beam_width=3):\n","    input_article = sp_text.encode_as_ids(input_article)\n","    input_article = tf.keras.preprocessing.sequence.pad_sequences([input_article], maxlen=max_text_len, padding='post', truncating='post')\n","    encoder_input = tf.expand_dims(input_article[0], 0)\n","\n","    sos_id = sp_summary.piece_to_id('<SOS>')\n","    eos_id = sp_summary.piece_to_id('<EOS>')\n","\n","    sequences = [([sos_id], 0.0)]\n","    completed_sequences = []\n","\n","    for _ in range(max_summary_len):\n","        all_candidates = []\n","        for seq, score in sequences:\n","            if seq[-1] == eos_id:\n","                completed_sequences.append((seq, score))\n","                continue\n","\n","            output = tf.expand_dims(seq, 0)\n","            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n","\n","            predictions, _ = transformer(\n","                encoder_input, \n","                output,\n","                training=False,\n","                enc_padding_mask=enc_padding_mask,\n","                look_ahead_mask=combined_mask,\n","                dec_padding_mask=dec_padding_mask\n","            )\n","\n","            logits = predictions[:, -1, :]\n","            log_probs = tf.math.log(tf.nn.softmax(logits))\n","            top_k = tf.math.top_k(log_probs, k=beam_width)\n","\n","            for i in range(beam_width):\n","                token = int(top_k.indices[0, i])\n","                candidate_score = score + float(top_k.values[0, i])\n","                candidate_seq = seq + [token]\n","                all_candidates.append((candidate_seq, candidate_score))\n","\n","        sequences = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)[:beam_width]\n","\n","        if not sequences:\n","            break\n","\n","    completed_sequences.extend(sequences)\n","    best_seq = max(completed_sequences, key=lambda tup: tup[1])[0]\n","\n","    return sp_summary.decode_ids(best_seq)\n"]},{"cell_type":"code","execution_count":31,"id":"8810ba15","metadata":{"execution":{"iopub.execute_input":"2025-11-15T13:14:22.479325Z","iopub.status.busy":"2025-11-15T13:14:22.479058Z","iopub.status.idle":"2025-11-15T13:16:01.09629Z","shell.execute_reply":"2025-11-15T13:16:01.095367Z"},"papermill":{"duration":98.634815,"end_time":"2025-11-15T13:16:01.097668","exception":false,"start_time":"2025-11-15T13:14:22.462853","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Text : <news_summary_more> Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers. \n","Real Headline : upGrad learner switches to career in ML & Al with 90% salary hike \n","Summary : <SOS> to be made on this day in India: Reports of 2016 in India&#39;s 1 year-end\n","\n","Text : <news_summary_more> Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more. \n","Real Headline : Delhi techie wins free food from Swiggy for one year on CRED \n","Summary : <SOS> to be made to be made to be made to be made to be made to be made to be\n","\n","Text : <news_summary_more> New Zealand defeated India by 8 wickets in the fourth ODI at Hamilton on Thursday to win their first match of the five-match ODI series. India lost an international match under Rohit Sharma's captaincy after 12 consecutive victories dating back to March 2018. The match witnessed India getting all out for 92, their seventh lowest total in ODI cricket history. \n","Real Headline : New Zealand end Rohit Sharma-led India's 12-match winning streak \n","Summary : <SOS> City beat Australia by 2 wickets in T20I series 2-0 series series series series series series series series\n","\n","Text : <news_summary_more> With Aegon Life iTerm Insurance plan, customers can enjoy tax benefits on your premiums paid and save up to Ã¢ÂÂ¹46,800^ on taxes. The plan provides life cover up to the age of 100 years. Also, customers have options to insure against Critical Illnesses, Disability and Accidental Death Benefit Rider with a life cover up to the age of 80 years. \n","Real Headline : Aegon life iTerm insurance plan helps customers save tax \n","Summary : <SOS> to pay $1k to be auctioned for $1 mn to people: Report to US bank staff to CEO\n","\n","Text : <news_summary_more> Speaking about the sexual harassment allegations against Rajkumar Hirani, Sonam Kapoor said, \"I've known Hirani for many years...What if it's not true, the [#MeToo] movement will get derailed.\" \"In the #MeToo movement, I always believe a woman. But in this case, we need to reserve our judgment,\" she added. Hirani has been accused by an assistant who worked in 'Sanju'. \n","Real Headline : Have known Hirani for yrs, what if MeToo claims are not true: Sonam \n","Summary : <SOS>ing me is not not not not a big friend: Sunny Leone on divorce row row row row row\n","\n","Text : <news_summary_more> Pakistani singer Rahat Fateh Ali Khan has denied receiving any notice from the Enforcement Directorate over allegedly smuggling foreign currency out of India. \"It would have been better if the authorities would have served the notice first if any and then publicised this,\" reads a press release issued on behalf of Rahat. The statement further called the allegation \"bizarre\". \n","Real Headline : Rahat Fateh Ali Khan denies getting notice for smuggling currency \n","Summary : <SOS>ing to give money to me: Court to Salman Khan on SC verdict row row row row row row\n","\n","Text : <news_summary_more> India recorded their lowest ODI total in New Zealand after getting all out for 92 runs in 30.5 overs in the fourth ODI at Hamilton on Thursday. Seven of India's batsmen were dismissed for single-digit scores, while their number ten batsman Yuzvendra Chahal top-scored with 18*(37). India's previous lowest ODI total in New Zealand was 108. \n","Real Headline : India get all out for 92, their lowest ODI total in New Zealand \n","Summary : <SOS> India&#39;s 2nd highest T20I T20I runs in T20I series in a\n","\n","Text : <news_summary_more> Weeks after ex-CBI Director Alok Verma told the Department of Personnel and Training to consider him retired, the Home Ministry asked him to join work on the last day of his fixed tenure as Director on Thursday. The ministry directed him to immediately join as DG, Fire Services, the post he was transferred to after his removal as CBI chief. \n","Real Headline : Govt directs Alok Verma to join work 1 day before his retirement \n","Summary : <SOS>ing to be held in Panama Papers case case: Govt to SC to HC judge ex-Prez&#39;\n","\n","Text : <news_summary_more> Andhra Pradesh CM N Chandrababu Naidu has said, \"When I met then US President Bill Clinton, I addressed him as Mr Clinton, not as 'sir'. (PM Narendra) Modi is my junior in politics...I addressed him as sir 10 times.\" \"I did this...to satisfy his ego in the hope that he will do justice to the state,\" he added. \n","Real Headline : Called PM Modi 'sir' 10 times to satisfy his ego: Andhra CM \n","Summary : <SOS>ing PM Modi&#39;s speech on Twitter today: BJP MP MP PM Modi&#39;s wife&#39;s\n","\n","Text : <news_summary_more> Congress candidate Shafia Zubair won the Ramgarh Assembly seat in Rajasthan, by defeating BJP's Sukhwant Singh with a margin of 12,228 votes in the bypoll. With this victory, Congress has taken its total to 100 seats in the 200-member assembly. The election to the Ramgarh seat was delayed due to the death of sitting MLA and BSP candidate Laxman Singh. \n","Real Headline : Cong wins Ramgarh bypoll in Rajasthan, takes total to 100 seats \n","Summary : <SOS>ing to be held in UP polls: BJP MLA to UP CM Badal in UP CM to BJP win\n"]}],"source":["for i in range(10):\n","    text = df_trans['cleaned_text'][i]\n","    real_summary = df_trans['cleaned_summary'][i]\n","    print(f\"\\nText : {text} \\nReal Headline : {real_summary} \\nSummary : {summarize(text)}\")"]},{"cell_type":"markdown","id":"d72bc4af","metadata":{"papermill":{"duration":0.016059,"end_time":"2025-11-15T13:16:01.130496","exception":false,"start_time":"2025-11-15T13:16:01.114437","status":"completed"},"tags":[]},"source":["# Credits\n","\n","- **\"Implementing Seq2Seq Models for Text Summarization With Keras\"**  \n","  *by Samhita Alla* "]},{"cell_type":"code","execution_count":32,"id":"b60d9b49","metadata":{"execution":{"iopub.execute_input":"2025-11-15T13:16:01.164226Z","iopub.status.busy":"2025-11-15T13:16:01.16351Z","iopub.status.idle":"2025-11-15T13:16:01.168616Z","shell.execute_reply":"2025-11-15T13:16:01.167966Z"},"papermill":{"duration":0.023588,"end_time":"2025-11-15T13:16:01.169765","exception":false,"start_time":"2025-11-15T13:16:01.146177","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working/checkpoints/ckpt-7.data-00000-of-00001\n","/kaggle/working/checkpoints/ckpt-7.index\n","/kaggle/working/checkpoints/ckpt-8.data-00000-of-00001\n","/kaggle/working/checkpoints/ckpt-6.index\n","/kaggle/working/checkpoints/checkpoint\n","/kaggle/working/checkpoints/ckpt-6.data-00000-of-00001\n","/kaggle/working/checkpoints/ckpt-8.index\n"]}],"source":["import os\n","import shutil\n","\n","checkpoint_dir = '/kaggle/working/checkpoints'\n","for root, dirs, files in os.walk(checkpoint_dir):\n","    for file in files:\n","        print(os.path.join(root, file))"]},{"cell_type":"code","execution_count":null,"id":"eea5a5ad","metadata":{"papermill":{"duration":0.015704,"end_time":"2025-11-15T13:16:01.20263","exception":false,"start_time":"2025-11-15T13:16:01.186926","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":33526,"isSourceIdPinned":false,"sourceId":44284,"sourceType":"datasetVersion"},{"datasetId":1895,"sourceId":791838,"sourceType":"datasetVersion"},{"datasetId":8737012,"sourceId":13732139,"sourceType":"datasetVersion"},{"datasetId":8738117,"sourceId":13733688,"sourceType":"datasetVersion"},{"datasetId":8740607,"sourceId":13737156,"sourceType":"datasetVersion"},{"datasetId":8744566,"sourceId":13743088,"sourceType":"datasetVersion"},{"modelId":477297,"modelInstanceId":461540,"sourceId":614196,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":31089,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":1551.877227,"end_time":"2025-11-15T13:16:04.640964","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-11-15T12:50:12.763737","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}