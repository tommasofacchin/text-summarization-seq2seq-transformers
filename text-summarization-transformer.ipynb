{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tommasofacchin/text-summarization-transformers-from-scratch?scriptVersionId=288220616\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"6a24b2c4","metadata":{"papermill":{"duration":0.004983,"end_time":"2025-12-24T13:52:00.757342","exception":false,"start_time":"2025-12-24T13:52:00.752359","status":"completed"},"tags":[]},"source":["# Seq2Seq and Encoder-Decoder\n","\n","## What is a Seq2Seq Model\n","A sequence-to-sequence (Seq2Seq) model is designed to take an input sequence and produce an output sequence. It’s widely used in tasks like machine translation, text summarization, and chatbots.\n","\n","**Example:**  \n","Input: \"Hello, how are you?\"  \n","Output: \"Ciao, come stai?\"\n","\n","---\n","\n","## Encoder-Decoder Architecture (Expanded)\n","\n","A typical Seq2Seq model has two main parts: the **encoder** and the **decoder**. The design allows the model to process sequences of variable length.  \n","\n","### Encoder\n","The encoder reads the input sequence and compresses it into a set of hidden states or a context vector. This vector captures the important information from the input and has a fixed size, though it does not need to match the decoder's size. The hidden states can either be passed as a whole to the decoder or connected at every decoding step.  \n","\n","At each step, the encoder updates its hidden state based on the previous hidden state and the current input. In mathematical terms, for a simple RNN:\n","\n","$$\n","H_t^{encoder} = \\phi(W_{HH} \\cdot H_{t-1}^{encoder} + W_{HX} \\cdot X_t)\n","$$\n","\n","Where:  \n","- $H_t^{encoder}$ = hidden state at time $t$ in the encoder  \n","- $X_t$ = input at time $t$  \n","- $W_{HH}$ = weight matrix connecting hidden states  \n","- $W_{HX}$ = weight matrix connecting input to hidden states  \n","- $\\phi$ = activation function (e.g., tanh or ReLU)\n","\n","---\n","\n","### Decoder\n","The decoder generates the output sequence one token at a time. Its initial hidden state is set to the final hidden state of the encoder. For a simple RNN decoder:\n","\n","$$\n","H_t^{decoder} = \\phi(W_{HH} \\cdot H_{t-1}^{decoder} + W_{HY} \\cdot Y_{t-1})\n","$$\n","\n","The output at each step is computed as:\n","\n","$$\n","Y_t = W_{HY} \\cdot H_t^{decoder}\n","$$\n","\n","Where:  \n","- $H_t^{decoder}$ = hidden state at time $t$ in the decoder  \n","- $Y_t$ = output at time $t$  \n","- $W_{HY}$ = weight matrix connecting decoder hidden state to output  \n","\n","### Implementation Notes\n","- Encoders and decoders are typically implemented with **RNNs, LSTMs, or GRUs**.  \n","- The input and output vectors are of fixed size, but the encoder and decoder can have different hidden dimensions.  \n","- During training, **teacher forcing** is often used, providing the correct previous token to the decoder instead of its own prediction.  \n","\n","---\n","\n","## Tokenization\n","\n","Before feeding text into a Seq2Seq or Transformer model, the raw text must be converted into numerical form.  \n","This is done through **tokenization**, which splits text into smaller units (tokens) such as words or subwords.  \n","\n","Each token is then mapped to a unique integer using a **vocabulary** built from the dataset.  \n","The model processes these integers rather than the raw text.\n","\n","**Example:**\n","\n","Input text: `\"Transformers improve summarization.\"`  \n","Tokens: `[\"transformers\", \"improve\", \"summarization\", \".\"]`  \n","Token IDs: `[201, 57, 1342, 4]`\n","\n","### Why Tokenization Matters\n","- Converts variable-length text into consistent, model-readable sequences.  \n","- Helps capture word frequency and context relationships.  \n","- Reduces vocabulary size when using subword tokenization (e.g., Byte Pair Encoding).  \n","\n","In this project, tokenization is part of preprocessing and includes:\n","- **Lowercasing** the text  \n","- **Removing special characters and URLs**  \n","- **Splitting into tokens by spaces**  \n","- Adding **start (`sostok`)** and **end (`eostok`)** tokens to mark summary boundaries  \n","\n","After tokenization, sequences will later be converted to integer IDs, padded or truncated to a fixed length\n","\n","---\n","\n","# Transformers\n","Transformers can be seen as an evolution of Seq2Seq models. Instead of processing sequences step by step like LSTMs or GRUs, they rely entirely on **attention mechanisms** to process all tokens in parallel and capture relationships between them.\n","\n","### Attention in Transformers\n","Attention is the core mechanism that allows Transformers to focus on relevant parts of the input sequence when producing a representation for each token. It works by comparing each token to all others and weighting them according to importance.\n","\n","#### How Attention Works\n","Each token in the sequence is represented by three vectors:\n","- **Query (Q):** what this token is looking for  \n","- **Key (K):** what information this token contains  \n","- **Value (V):** the actual information of the token  \n","\n","The attention score between two tokens is computed as the similarity between the Query of one token and the Key of another. This determines how much attention one token should pay to another. Mathematically, the attention weights are computed using a scaled dot-product:\n","\n","$$\n","\\text{Attention}(Q, K, V) = \\text{softmax}\\Big(\\frac{QK^T}{\\sqrt{d_k}}\\Big) V\n","$$\n","\n","Where $d_k$ is the dimensionality of the Key vectors.\n","\n","- The **softmax** ensures that the weights sum to 1.  \n","- Each token’s output is a weighted sum of all Value vectors, allowing it to incorporate context from the entire sequence.\n","\n","#### Multi-Head Attention\n","Instead of computing attention just once, Transformers use **multiple attention heads** in parallel. Each head can learn to focus on different types of relationships, such as:\n","- Syntactic relationships (e.g., subject-verb connections)  \n","- Semantic relationships (e.g., synonyms or related concepts)  \n","\n","The outputs of all heads are concatenated and projected to form the final representation for each token.\n","\n","#### Intuition\n","Imagine reading a sentence and highlighting all the words that are important for understanding each token. Each word “attends” to other words in the sentence that matter most for its meaning. Multi-head attention lets the model do this from multiple perspectives simultaneously.\n","\n","### Key Components of Transformers\n","- **Encoder-Decoder Structure:** Like Seq2Seq models, Transformers have an encoder that processes the input and a decoder that generates the output. Both use layers of self-attention and feed-forward networks.  \n","- **Positional Encoding:** Since Transformers don’t process tokens sequentially, they add positional information so the model knows the order of tokens.  \n","- **Feed-Forward Layers:** After attention, each token passes through fully connected layers for additional transformation.\n","\n","### Advantages over LSTM/GRU Seq2Seq\n","- Processes sequences **in parallel**, speeding up training.  \n","- Handles **long sequences** more effectively with attention.  \n","- Captures **complex relationships** between tokens regardless of distance.  \n","- Scales easily to **very deep models** and large datasets.\n","\n","### Use Cases\n","Transformers are the backbone of many state-of-the-art models for tasks such as:\n","- Machine translation (e.g., T5, MarianMT)  \n","- Text summarization (e.g., BART, Pegasus)  \n","- Question answering and chatbots (e.g., GPT, BERT-based models)"]},{"cell_type":"code","execution_count":1,"id":"75fbe65a","metadata":{"execution":{"iopub.execute_input":"2025-12-24T13:52:00.766895Z","iopub.status.busy":"2025-12-24T13:52:00.766416Z","iopub.status.idle":"2025-12-24T13:52:07.477769Z","shell.execute_reply":"2025-12-24T13:52:07.476719Z"},"papermill":{"duration":6.717676,"end_time":"2025-12-24T13:52:07.479143","exception":false,"start_time":"2025-12-24T13:52:00.761467","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\r\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\r\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\r\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install sentencepiece\n","%pip install openpyxl"]},{"cell_type":"markdown","id":"d69e7bfc","metadata":{"papermill":{"duration":0.004183,"end_time":"2025-12-24T13:52:07.487867","exception":false,"start_time":"2025-12-24T13:52:07.483684","status":"completed"},"tags":[]},"source":["# Data Preparation\n","\n","Prepare and clean the dataset for the summarization model:\n","\n","- **Load datasets:** Read two CSV files containing news articles and their summaries.\n","- **Combine datasets:** Merge datasets while selecting relevant `text` and `summary` columns.\n","- **Text cleaning:**  \n","  - Convert text to lowercase.  \n","  - Remove special characters.  \n","  - Replace URLs with domain names.  \n","  - Reduce multiple spaces.\n","- **Tokenization:** Split cleaned text into tokens (words) and add `_START_` and `_END_` tokens for summaries.\n","- **Handle missing values:** Drop rows with missing `text` values.\n","- **Analyze sequence lengths:** Calculate word counts for texts and summaries.\n","- **Limit sequence lengths:** Restrict `text` to 100 words and `summary` to 15 words.\n","- **Add model tokens:** Prepend `sostok` and append `eostok` to all summaries to mark start and end for the model.\n"]},{"cell_type":"code","execution_count":2,"id":"9edbcdd6","metadata":{"execution":{"iopub.execute_input":"2025-12-24T13:52:07.497212Z","iopub.status.busy":"2025-12-24T13:52:07.49693Z","iopub.status.idle":"2025-12-24T13:52:26.748161Z","shell.execute_reply":"2025-12-24T13:52:26.747288Z"},"papermill":{"duration":19.257357,"end_time":"2025-12-24T13:52:26.749336","exception":false,"start_time":"2025-12-24T13:52:07.491979","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-12-24 13:52:10.221086: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1766584330.398761      20 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1766584330.451488      20 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["(55104, 5)\n"]}],"source":["import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import pandas as pd\n","import numpy as np\n","import re\n","import time\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","pd.set_option('display.max_colwidth', None)\n","data = pd.read_excel(\"/kaggle/input/inshorts-news-data/Inshorts Cleaned Data.xlsx\")\n","\n","print(data.shape)"]},{"cell_type":"code","execution_count":3,"id":"4ed284af","metadata":{"execution":{"iopub.execute_input":"2025-12-24T13:52:26.759543Z","iopub.status.busy":"2025-12-24T13:52:26.759105Z","iopub.status.idle":"2025-12-24T13:52:26.789192Z","shell.execute_reply":"2025-12-24T13:52:26.788518Z"},"papermill":{"duration":0.036153,"end_time":"2025-12-24T13:52:26.79021","exception":false,"start_time":"2025-12-24T13:52:26.754057","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Headline</th>\n","      <th>Short</th>\n","      <th>Source</th>\n","      <th>Time</th>\n","      <th>Publish Date</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4 ex-bank officials booked for cheating bank of ₹209 crore</td>\n","      <td>The CBI on Saturday booked four former officials of Syndicate Bank and six others for cheating, forgery, criminal conspiracy and causing ₹209 crore loss to the state-run bank. The accused had availed home loans and credit from Syndicate Bank on the basis of forged and fabricated documents. These funds were fraudulently transferred to the companies owned by the accused persons.</td>\n","      <td>The New Indian Express</td>\n","      <td>09:25:00</td>\n","      <td>2017-03-26</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Supreme Court to go paperless in 6 months: CJI</td>\n","      <td>Chief Justice JS Khehar has said the Supreme Court will go paperless in six to seven months in a bid to save funds and make the judiciary eco-friendly. He further said the apex court will collect all the records electronically from the lower courts and the high courts so that there is no need to file hard copies.</td>\n","      <td>Outlook</td>\n","      <td>22:18:00</td>\n","      <td>2017-03-25</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>At least 3 killed, 30 injured in blast in Sylhet, Bangladesh</td>\n","      <td>At least three people were killed, including a policeman, while 30 others were wounded on Saturday evening in two explosions in Sylhet, Bangladesh. The explosions were targetted at people and police officials who were witnessing an over 30-hour-long gunfight between extremists and commandos. Earlier on Friday, a man had blown himself up in front of a checkpoint near Dhaka Airport.</td>\n","      <td>Hindustan Times</td>\n","      <td>23:39:00</td>\n","      <td>2017-03-25</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Why has Reliance been barred from trading in futures?</td>\n","      <td>Mukesh Ambani-led Reliance Industries (RIL) was barred from trading in futures market for a year over stake sale in Reliance Petroleum (RPL). In 2007, RIL sold 4.1% stake in RPL, but shares were first &amp;#39;short-sold&amp;#39; in futures market to avoid a fall in RPL stocks. Short sale means selling shares with plans to buy them back later at lower prices.</td>\n","      <td>Livemint</td>\n","      <td>23:08:00</td>\n","      <td>2017-03-25</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Was stopped from entering my own studio at Times Now: Arnab</td>\n","      <td>TV news anchor Arnab Goswami has said he was told he could not do the programme two days before leaving Times Now. &amp;#34;18th November was my last day, I was not allowed to enter my own studio,&amp;#34; Goswami added. &amp;#34;When you build an institution and are not allowed to enter your own studio, you feel sad,&amp;#34; the journalist further said.</td>\n","      <td>YouTube</td>\n","      <td>23:24:00</td>\n","      <td>2017-03-25</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                       Headline  \\\n","0    4 ex-bank officials booked for cheating bank of ₹209 crore   \n","1                Supreme Court to go paperless in 6 months: CJI   \n","2  At least 3 killed, 30 injured in blast in Sylhet, Bangladesh   \n","3         Why has Reliance been barred from trading in futures?   \n","4   Was stopped from entering my own studio at Times Now: Arnab   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                             Short  \\\n","0      The CBI on Saturday booked four former officials of Syndicate Bank and six others for cheating, forgery, criminal conspiracy and causing ₹209 crore loss to the state-run bank. The accused had availed home loans and credit from Syndicate Bank on the basis of forged and fabricated documents. These funds were fraudulently transferred to the companies owned by the accused persons.   \n","1                                                                       Chief Justice JS Khehar has said the Supreme Court will go paperless in six to seven months in a bid to save funds and make the judiciary eco-friendly. He further said the apex court will collect all the records electronically from the lower courts and the high courts so that there is no need to file hard copies.   \n","2  At least three people were killed, including a policeman, while 30 others were wounded on Saturday evening in two explosions in Sylhet, Bangladesh. The explosions were targetted at people and police officials who were witnessing an over 30-hour-long gunfight between extremists and commandos. Earlier on Friday, a man had blown himself up in front of a checkpoint near Dhaka Airport.   \n","3                                Mukesh Ambani-led Reliance Industries (RIL) was barred from trading in futures market for a year over stake sale in Reliance Petroleum (RPL). In 2007, RIL sold 4.1% stake in RPL, but shares were first &#39;short-sold&#39; in futures market to avoid a fall in RPL stocks. Short sale means selling shares with plans to buy them back later at lower prices.   \n","4                                            TV news anchor Arnab Goswami has said he was told he could not do the programme two days before leaving Times Now. &#34;18th November was my last day, I was not allowed to enter my own studio,&#34; Goswami added. &#34;When you build an institution and are not allowed to enter your own studio, you feel sad,&#34; the journalist further said.   \n","\n","                  Source      Time  Publish Date  \n","0  The New Indian Express  09:25:00   2017-03-26  \n","1                 Outlook  22:18:00   2017-03-25  \n","2         Hindustan Times  23:39:00   2017-03-25  \n","3                Livemint  23:08:00   2017-03-25  \n","4                 YouTube  23:24:00   2017-03-25  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data.head(5)"]},{"cell_type":"code","execution_count":4,"id":"461d664b","metadata":{"execution":{"iopub.execute_input":"2025-12-24T13:52:26.800218Z","iopub.status.busy":"2025-12-24T13:52:26.799963Z","iopub.status.idle":"2025-12-24T13:52:26.804233Z","shell.execute_reply":"2025-12-24T13:52:26.803615Z"},"papermill":{"duration":0.010325,"end_time":"2025-12-24T13:52:26.805305","exception":false,"start_time":"2025-12-24T13:52:26.79498","status":"completed"},"tags":[]},"outputs":[],"source":["def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r\"http\\S+\", \"\", text)\n","    text = text.replace(\"39\", \"'\")\n","    text = text.replace(\"34\", '\"')\n","    text = text.replace(\"’\", \"'\")\n","    text = re.sub(r\"[^a-z0-9 ,.'\\-?()\\\"]\", \" \", text)\n","    text = re.sub(r\"\\s*'\\s*\", \"'\", text)\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    \n","    return f\"<sos> {text} <eos>\"\n"]},{"cell_type":"code","execution_count":5,"id":"03434cf9","metadata":{"execution":{"iopub.execute_input":"2025-12-24T13:52:26.81627Z","iopub.status.busy":"2025-12-24T13:52:26.815702Z","iopub.status.idle":"2025-12-24T13:52:29.267952Z","shell.execute_reply":"2025-12-24T13:52:29.267173Z"},"papermill":{"duration":2.458474,"end_time":"2025-12-24T13:52:29.269103","exception":false,"start_time":"2025-12-24T13:52:26.810629","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["(55104, 2)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>&lt;sos&gt; gangster-turned-politician mukhtar ansari has won from the mau constituency in uttar pradesh after polling 96,793 votes, defeating the nearest candidate by over 8,000 votes. ansari, who was the sitting mla from the constituency, had allied with the mayawati-led bahujan samaj party before the elections. ansari has been accused of murdering a bjp mla. &lt;eos&gt;</td>\n","      <td>&lt;sos&gt; gangster-turned-politician mukhtar ansari wins by 8000 votes &lt;eos&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>&lt;sos&gt; indira gandhi has been the only woman till date to have presented the union budget of india in 1970-71. this came after indira gandhi, the then prime minister, took over the finance portfolio after morarji desai resigned as the minister of finance. so far, she has been the only woman finance minister of india. &lt;eos&gt;</td>\n","      <td>&lt;sos&gt; indira gandhi only woman to have presented the budget &lt;eos&gt;</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                                                                                                                                                                                                                                                                                                          text  \\\n","0  <sos> gangster-turned-politician mukhtar ansari has won from the mau constituency in uttar pradesh after polling 96,793 votes, defeating the nearest candidate by over 8,000 votes. ansari, who was the sitting mla from the constituency, had allied with the mayawati-led bahujan samaj party before the elections. ansari has been accused of murdering a bjp mla. <eos>   \n","1                                          <sos> indira gandhi has been the only woman till date to have presented the union budget of india in 1970-71. this came after indira gandhi, the then prime minister, took over the finance portfolio after morarji desai resigned as the minister of finance. so far, she has been the only woman finance minister of india. <eos>   \n","\n","                                                                    summary  \n","0  <sos> gangster-turned-politician mukhtar ansari wins by 8000 votes <eos>  \n","1         <sos> indira gandhi only woman to have presented the budget <eos>  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.DataFrame()\n","\n","df['text'] = data['Short'].apply(clean_text)\n","df['summary'] = data['Headline'].apply(clean_text)\n","\n","MAX_TEXT_LEN = 100\n","MAX_SUMMARY_LEN = 20\n","SAMPLE_SIZE = len(df)\n","\n","df = df.sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n","\n","print(df.shape)\n","df.head(2)"]},{"cell_type":"markdown","id":"78ddc501","metadata":{"papermill":{"duration":0.00619,"end_time":"2025-12-24T13:52:29.280911","exception":false,"start_time":"2025-12-24T13:52:29.274721","status":"completed"},"tags":[]},"source":["# Transformer Model with Self-Attention\n","\n","Similar to the Seq2Seq architecture, the Transformer follows an **encoder–decoder structure**, but instead of recurrent layers it relies entirely on **Multi-Head Self-Attention**.  \n","This allows the model to process all tokens **in parallel** and learn relationships between words regardless of their distance in the sequence.\n","\n","During training, the model takes **two inputs**:  \n","1. The encoder input (`text`) – the tokenized article.  \n","2. The decoder input (`summary`) – the summary shifted by one token.  \n","\n","The **target output** is the summary shifted forward by one position. The decoder learns to predict each word based on the previously generated ones and the encoded representation of the full text.\n","\n","---\n","\n","**Encoder**  \n","- The input article sequence (`MAX_TEXT_LEN`) is first transformed using an **Embedding layer**.  \n","- A **Positional Encoding** is added to preserve the order of words (since attention has no notion of sequence order by itself).  \n","- The embedded input is processed by one or more **Multi-Head Self-Attention** blocks:\n","  - Each word attends to **all other words** in the input\n","  - Relationships between distant tokens are captured more effectively than in RNNs  \n","- A **Feed-Forward Network** (FFN) refines the contextual representations.\n","- **Residual connections** and **Layer Normalization** improve gradient flow and training stability.\n","\n","---\n","\n","**Decoder**  \n","- Similar positional embeddings are applied to the shifted summary tokens.  \n","- The decoder uses two attention mechanisms:\n","  1. **Masked Self-Attention**: ensures the model cannot “peek” at future words when predicting the next token.\n","  2. **Encoder-Decoder Attention**: allows the decoder to focus on relevant parts of the input article.\n","- A **Feed-Forward Network** further processes the attended features.\n","- A final **Dense layer with Softmax** outputs a probability distribution over all words in the vocabulary at each time step.\n","\n","---\n","\n","**sostok and eostok**  \n","In sequence-to-sequence tasks such as abstractive text summarization, **special tokens** are essential for controlling how a model generates text:\n","\n","- `<sostok>` → marks the **start** of the output sequence  \n","- `<eostok>` → marks the **end** of the sequence  \n","\n","However, these tokens **do not** play the same role during training across different architectures.\n","\n","Transformers use **masked self-attention** in the decoder, meaning that at time *t* the model can only attend to **previous tokens**.\n","Therefore:\n","\n","- `<sostok>` must be present **only in the decoder input**  \n","- `<sostok>` must be removed from the decoder target  \n","\n","Predicting a start token would make no sense and causes failure modes such as:\n","\n","- the model repeatedly outputting `<sostok>`\n","- inability to begin sequences with meaningful content\n","\n","The EOS token **must remain in the targets**, because:\n","\n","- it teaches the model **when to stop writing**\n","- without it, generation may become too long or infinite\n","\n","LSTM encoder-decoder models:\n","\n","- receive the final hidden state as initial context\n","- do **not** use masked attention\n","- often ignore the first timestep in loss computation\n","\n","So `<sostok>` in targets is less harmful there.\n","\n","---\n","\n","Thanks to the Self-Attention mechanism, Transformers **capture global context efficiently** and typically produce **more coherent and fluent summaries**, especially for longer texts."]},{"cell_type":"markdown","id":"5fec836e","metadata":{"papermill":{"duration":0.004322,"end_time":"2025-12-24T13:52:29.28996","exception":false,"start_time":"2025-12-24T13:52:29.285638","status":"completed"},"tags":[]},"source":["### Preparing Transformer Inputs\n","\n","To train the Transformer in an encoder–decoder setup, we need to properly structure the input data:\n","\n","- The **encoder input** is the full tokenized article (`x_train`)\n","- The **decoder input** is the summary sequence **shifted right**, starting with `<sostok>`\n","- The **decoder target** is the same summary **shifted left**, ending with `<eostok>`\n","\n","This shifting ensures that at each timestep the decoder learns to predict the **next** word using:\n","1. The previously processed summary tokens  \n","2. Attention over the encoder output  "]},{"cell_type":"markdown","id":"f110f2c0","metadata":{"papermill":{"duration":0.004473,"end_time":"2025-12-24T13:52:29.298949","exception":false,"start_time":"2025-12-24T13:52:29.294476","status":"completed"},"tags":[]},"source":["In text summarization, token-level accuracy can be misleading because it only measures whether each predicted token matches the ground truth at the same position. It does not capture semantic meaning, fluency, word order, or relevance, and it can be inflated by common tokens like padding or start/end markers. A model can have high accuracy while producing poor summaries. Better evaluation uses metrics like ROUGE-1, ROUGE-2, and ROUGE-L, which measure overlap of unigrams, bigrams, and longest common subsequences between generated and reference summaries. During training, it is better to monitor validation loss and evaluate summaries qualitatively or with ROUGE rather than relying on token accuracy."]},{"cell_type":"markdown","id":"e5ac6b23","metadata":{"papermill":{"duration":0.004376,"end_time":"2025-12-24T13:52:29.307816","exception":false,"start_time":"2025-12-24T13:52:29.30344","status":"completed"},"tags":[]},"source":["## Predict"]},{"cell_type":"markdown","id":"597541a8","metadata":{"papermill":{"duration":0.004395,"end_time":"2025-12-24T13:52:29.317568","exception":false,"start_time":"2025-12-24T13:52:29.313173","status":"completed"},"tags":[]},"source":["Note importanti:\n","\n","Look-ahead mask a inference non serve se generi un token alla volta (greedy decoding step-by-step).\n","\n","Padding mask dell’encoder serve al decoder per ignorare i pad token dell’input.\n","\n","Quando fai l’inference dovrai generare token uno per uno, aggiornando dec_input_inf ad ogni step."]},{"cell_type":"markdown","id":"d2d8b7d2","metadata":{"papermill":{"duration":0.004595,"end_time":"2025-12-24T13:52:29.326576","exception":false,"start_time":"2025-12-24T13:52:29.321981","status":"completed"},"tags":[]},"source":["Generated summary: ripete continuamente parole (cannot cannot cannot, power power power…) → questo è un loop di ripetizione, tipico dei modelli seq2seq che non hanno abbastanza regolarizzazione sulla generazione.\n","\n","Generated beam search summary: testo quasi completamente fuori tema → indica che il modello non ha appreso bene il contenuto semantico e il beam search amplifica le frasi che appaiono più “probabili” a livello di token, ma non corrette.\n"]},{"cell_type":"markdown","id":"2d908f18","metadata":{"papermill":{"duration":0.004358,"end_time":"2025-12-24T13:52:29.335477","exception":false,"start_time":"2025-12-24T13:52:29.331119","status":"completed"},"tags":[]},"source":["Limiting the vocabulary in a Transformer is important because it reduces the size of the embedding matrices and the final softmax layer, making the model faster and lighter. It also helps prevent overfitting by removing extremely rare words that add noise rather than useful information. A smaller vocabulary uses less memory and often leads to more stable training, which can be important when working with limited hardware. However, reducing the vocabulary also means losing information, because words outside the limit are replaced with an unknown token. This can harm tasks like summarization, where specific terms, names, or technical words matter. A limited vocabulary also restricts what the model can generate, since it can only output words it knows.\n","\n","I tried limiting the vocabulary, but it ended up harming the model’s performance."]},{"cell_type":"markdown","id":"8674d1ef","metadata":{"papermill":{"duration":0.004327,"end_time":"2025-12-24T13:52:29.344193","exception":false,"start_time":"2025-12-24T13:52:29.339866","status":"completed"},"tags":[]},"source":["We wanted to continue training a Transformer model after the first epoch without losing the optimizer state. The issue was that creating a new optimizer reset the step count, making the learning rate extremely small due to the warmup schedule. To fix this, we restored a full checkpoint including both the model and optimizer. This ensures the weights, optimizer moments, and step count are preserved, so the learning rate continues correctly. Training can now continue from where it left off, and checkpoints can be saved after each epoch to resume seamlessly in future sessions."]},{"cell_type":"markdown","id":"7c959e50","metadata":{"papermill":{"duration":0.004512,"end_time":"2025-12-24T13:52:29.35318","exception":false,"start_time":"2025-12-24T13:52:29.348668","status":"completed"},"tags":[]},"source":["Using a single tokenizer for both input text and summaries is a design choice with clear advantages and trade-offs. The main benefit is consistency: the encoder and decoder operate on the same vocabulary and token representations, which makes cross-attention easier to learn and stabilizes training. The model does not need to internally translate between two different symbolic systems, so convergence is faster and behavior is more predictable. A shared tokenizer also reduces the total number of parameters, avoids duplicated tokens, and improves generalization, especially for named entities, numbers, and rare words that appear in both input and output. Debugging is simpler because there is only one source of truth for tokenization.\n","\n","The main drawback is that a single tokenizer is a compromise between two different distributions. Input texts are long and diverse, while summaries are short and dense. A shared vocabulary may allocate capacity to tokens that are useful for the input but rarely needed in the output, slightly reducing efficiency. It also offers less control over the style and structure of the generated summaries compared to using a specialized tokenizer for the output. In addition, special tokens such as start and end markers must be handled carefully to avoid interfering with the encoder input.\n","\n","In practice, a single tokenizer is usually the better choice for summarization, translation, and other sequence-to-sequence tasks, especially with small or medium datasets and custom or experimental tokenizers. Separate tokenizers can make sense for very large datasets or highly specialized outputs, but they increase complexity and instability. For most practical cases, a shared tokenizer provides better stability, faster learning, and fewer hidden failure modes."]},{"cell_type":"markdown","id":"2ae47076","metadata":{"papermill":{"duration":0.004265,"end_time":"2025-12-24T13:52:29.361911","exception":false,"start_time":"2025-12-24T13:52:29.357646","status":"completed"},"tags":[]},"source":["When resuming training from a checkpoint, the Transformer seems to start from zero only because the first batches always show very low accuracy. This is normal and not a sign of lost weights. The decoder struggles with the first tokens due to masking, so accuracy is naturally low at the start of each epoch. If the dataset uses a fixed shuffle and the same examples appear first every time, you will always see the same low accuracy at the beginning. The proof that the checkpoint is restored correctly is that the accuracy rises quickly after a few hundred batches, which would not happen if the model had really restarted from scratch."]},{"cell_type":"code","execution_count":6,"id":"869e5d09","metadata":{"execution":{"iopub.execute_input":"2025-12-24T13:52:29.371804Z","iopub.status.busy":"2025-12-24T13:52:29.371548Z","iopub.status.idle":"2025-12-24T13:52:32.045985Z","shell.execute_reply":"2025-12-24T13:52:32.045224Z"},"papermill":{"duration":2.680915,"end_time":"2025-12-24T13:52:32.047232","exception":false,"start_time":"2025-12-24T13:52:29.366317","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoder vocab: 95768 \n"," Decoder vocab: 38125\n"]}],"source":["filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n","oov_token = '<unk>'\n","\n","# Tokenizer\n","article_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n","summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)\n","\n","# article_tokenizer.fit_on_texts(df['text'])\n","# summary_tokenizer.fit_on_texts(df['summary'])\n","\n","# Fit\n","article_texts = df[\"text\"].astype(str).tolist()\n","summary_texts = df[\"summary\"].astype(str).tolist()\n","\n","article_tokenizer.fit_on_texts(article_texts)\n","summary_tokenizer.fit_on_texts(summary_texts)\n","\n","# Vocabularies\n","x_voc_size = len(article_tokenizer.word_index) + 1\n","y_voc_size = len(summary_tokenizer.word_index) + 1\n","print(f\"Encoder vocab: {x_voc_size} \\n Decoder vocab: {y_voc_size}\")"]},{"cell_type":"code","execution_count":7,"id":"c0af69d5","metadata":{"execution":{"iopub.execute_input":"2025-12-24T13:52:32.057832Z","iopub.status.busy":"2025-12-24T13:52:32.057626Z","iopub.status.idle":"2025-12-24T13:52:34.989111Z","shell.execute_reply":"2025-12-24T13:52:34.988262Z"},"papermill":{"duration":2.938117,"end_time":"2025-12-24T13:52:34.990419","exception":false,"start_time":"2025-12-24T13:52:32.052302","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["I0000 00:00:1766584354.921976      20 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"]}],"source":["# Sequences\n","inputs = article_tokenizer.texts_to_sequences(article_texts)\n","targets = summary_tokenizer.texts_to_sequences(summary_texts)\n","\n","# padding\n","inputs = pad_sequences(inputs, maxlen=MAX_TEXT_LEN, padding='post', truncating='post')\n","targets = pad_sequences(targets, maxlen=MAX_SUMMARY_LEN, padding='post', truncating='post')\n","\n","inputs = tf.cast(inputs, dtype=tf.int64)\n","targets = tf.cast(targets, dtype=tf.int64)"]},{"cell_type":"code","execution_count":8,"id":"fa66664e","metadata":{"execution":{"iopub.execute_input":"2025-12-24T13:52:35.001546Z","iopub.status.busy":"2025-12-24T13:52:35.001331Z","iopub.status.idle":"2025-12-24T13:52:36.455077Z","shell.execute_reply":"2025-12-24T13:52:36.454465Z"},"papermill":{"duration":1.460907,"end_time":"2025-12-24T13:52:36.456453","exception":false,"start_time":"2025-12-24T13:52:34.995546","status":"completed"},"tags":[]},"outputs":[],"source":["batch_size = 64\n","dataset = (tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(1000, seed=42, reshuffle_each_iteration=False).batch(batch_size))\n","\n","\n","def get_angles(position, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n","    return position * angle_rates\n","\n","def positional_encoding(position, d_model):\n","    angle_rads = get_angles(\n","        np.arange(position)[:, np.newaxis],\n","        np.arange(d_model)[np.newaxis, :],\n","        d_model\n","    )\n","\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = angle_rads[np.newaxis, ...]\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","def create_padding_mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","    return seq[:, tf.newaxis, tf.newaxis, :]\n","\n","def create_look_ahead_mask(size):\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask\n","\n","def scaled_dot_product_attention(q, k, v, mask):\n","    matmul_qk = tf.matmul(q, k, transpose_b=True)\n","\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)  \n","\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n","\n","    output = tf.matmul(attention_weights, v)\n","    return output, attention_weights\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.wq = tf.keras.layers.Dense(d_model)\n","        self.wk = tf.keras.layers.Dense(d_model)\n","        self.wv = tf.keras.layers.Dense(d_model)\n","\n","        self.dense = tf.keras.layers.Dense(d_model)\n","        \n","    def split_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","    \n","    def call(self, v, k, q, mask):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q)\n","        k = self.wk(k)\n","        v = self.wv(v)\n","\n","        q = self.split_heads(q, batch_size)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","\n","        scaled_attention, attention_weights = scaled_dot_product_attention(\n","            q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","        \n","        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n","        output = self.dense(concat_attention)\n","            \n","        return output, attention_weights\n","    \n","def point_wise_feed_forward_network(d_model, dff):\n","    return tf.keras.Sequential([\n","        tf.keras.layers.Dense(dff, activation='relu'),\n","        tf.keras.layers.Dense(d_model)\n","    ])\n","\n","\n","class EncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(EncoderLayer, self).__init__()\n","\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","    \n","    def call(self, x, training, mask):\n","        attn_output, _ = self.mha(x, x, x, mask)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output)\n","\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        out2 = self.layernorm2(out1 + ffn_output)\n","\n","        return out2\n","\n","\n","\n","class DecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(DecoderLayer, self).__init__()\n","\n","        self.mha1 = MultiHeadAttention(d_model, num_heads)\n","        self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","        self.dropout3 = tf.keras.layers.Dropout(rate)\n","    \n","    \n","    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n","        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n","        attn1 = self.dropout1(attn1, training=training)\n","        out1 = self.layernorm1(attn1 + x)\n","\n","        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n","        attn2 = self.dropout2(attn2, training=training)\n","        out2 = self.layernorm2(attn2 + out1)\n","\n","        ffn_output = self.ffn(out2)\n","        ffn_output = self.dropout3(ffn_output, training=training)\n","        out3 = self.layernorm3(ffn_output + out2)\n","\n","        return out3, attn_weights_block1, attn_weights_block2\n","\n","\n","\n","class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n","        super(Encoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n","\n","        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","        \n","    def call(self, x, training=False, mask=None):\n","        seq_len = tf.shape(x)[1]\n","    \n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","    \n","        for i in range(self.num_layers):\n","            x = self.enc_layers[i](x, training=training, mask=mask)\n","    \n","        return x\n","    \n","class Decoder(tf.keras.layers.Layer):\n","        \n","    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n","        super(Decoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","\n","        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","    \n","    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n","        seq_len = tf.shape(x)[1]\n","        attention_weights = {}\n","    \n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]  \n","\n","    \n","        x = self.dropout(x, training=training)\n","    \n","        for i in range(self.num_layers):\n","            x, block1, block2 = self.dec_layers[i](\n","                x, \n","                enc_output, \n","                training=training, \n","                look_ahead_mask=look_ahead_mask, \n","                padding_mask=padding_mask\n","            )\n","            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n","            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n","    \n","        return x, attention_weights\n","\n","\n","\n","\n","class Transformer(tf.keras.Model):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n","        super(Transformer, self).__init__()\n","\n","        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n","        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n","        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","    \n","    def call(self, inp, tar, training=False, enc_padding_mask=None, look_ahead_mask=None, dec_padding_mask=None):\n","        enc_output = self.encoder(x=inp, training=training, mask=enc_padding_mask)\n","        dec_output, attention_weights = self.decoder(x=tar, enc_output=enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)\n","        final_output = self.final_layer(dec_output)\n","        return final_output, attention_weights"]},{"cell_type":"code","execution_count":9,"id":"6cf6e252","metadata":{"execution":{"iopub.execute_input":"2025-12-24T13:52:36.467313Z","iopub.status.busy":"2025-12-24T13:52:36.467085Z","iopub.status.idle":"2025-12-24T13:52:36.475366Z","shell.execute_reply":"2025-12-24T13:52:36.4747Z"},"papermill":{"duration":0.014906,"end_time":"2025-12-24T13:52:36.476452","exception":false,"start_time":"2025-12-24T13:52:36.461546","status":"completed"},"tags":[]},"outputs":[],"source":["def smooth_labels(labels, vocab_size, smoothing=0.05):\n","    confidence = 1.0 - smoothing\n","    low_conf = smoothing / tf.cast(vocab_size - 1, tf.float32)\n","    labels_one_hot = tf.one_hot(labels, depth=vocab_size)\n","    return labels_one_hot * confidence + low_conf\n","\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n","\n","\n","def accuracy_function(real, pred):\n","    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n","\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    accuracies = tf.math.logical_and(mask, accuracies)\n","\n","    accuracies = tf.cast(accuracies, dtype=tf.float32)\n","    mask = tf.cast(mask, dtype=tf.float32)\n","    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n","\n","def create_masks(inp, tar):\n","    enc_padding_mask = create_padding_mask(inp)\n","    dec_padding_mask = create_padding_mask(inp)\n","\n","    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","    dec_target_padding_mask = create_padding_mask(tar)\n","    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","  \n","    return enc_padding_mask, combined_mask, dec_padding_mask\n","\n","\n","class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=4000, factor=1.0):\n","        super(CustomSchedule, self).__init__()\n","        self.d_model = d_model\n","        self.d_model = tf.cast(self.d_model, tf.float32)\n","        self.warmup_steps = warmup_steps\n","        self.factor = factor\n","    \n","    def __call__(self, step):\n","        step = tf.cast(step, tf.float32) \n","        step = tf.maximum(step, 1.0)\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps ** -1.5)\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2) * self.factor"]},{"cell_type":"code","execution_count":10,"id":"26e1a553","metadata":{"execution":{"iopub.execute_input":"2025-12-24T13:52:36.486578Z","iopub.status.busy":"2025-12-24T13:52:36.486371Z","iopub.status.idle":"2025-12-24T13:52:42.035626Z","shell.execute_reply":"2025-12-24T13:52:42.034876Z"},"papermill":{"duration":5.555518,"end_time":"2025-12-24T13:52:42.03676","exception":false,"start_time":"2025-12-24T13:52:36.481242","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"transformer\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)               │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">26,097,920</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)               │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,132,352</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38125</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">9,798,125</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ encoder (\u001b[38;5;33mEncoder\u001b[0m)               │ ?                      │    \u001b[38;5;34m26,097,920\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ decoder (\u001b[38;5;33mDecoder\u001b[0m)               │ ?                      │    \u001b[38;5;34m12,132,352\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m38125\u001b[0m)         │     \u001b[38;5;34m9,798,125\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">48,028,397</span> (183.21 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m48,028,397\u001b[0m (183.21 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">48,028,397</span> (183.21 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m48,028,397\u001b[0m (183.21 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["num_layers = 3   \n","d_model = 128        \n","dff = 512            \n","num_heads = 4        \n","dropout_rate = 0.1  \n","\n","d_model = 256 \n","dropout_rate = 0.2\n","\n","transformer = Transformer(\n","    num_layers=num_layers,\n","    d_model=d_model,\n","    num_heads=num_heads,\n","    dff=dff,\n","    input_vocab_size=x_voc_size,\n","    target_vocab_size=y_voc_size,\n","    pe_input=1000,\n","    pe_target=1000,\n","    rate=dropout_rate\n",")\n","\n","dummy_input = tf.constant([[1]*MAX_TEXT_LEN], dtype=tf.int64)\n","dummy_target = tf.constant([[1]*MAX_SUMMARY_LEN], dtype=tf.int64)\n","_ = transformer(dummy_input, dummy_target, training=False)\n","transformer.summary()"]},{"cell_type":"code","execution_count":11,"id":"2858c4ba","metadata":{"execution":{"iopub.execute_input":"2025-12-24T13:52:42.048691Z","iopub.status.busy":"2025-12-24T13:52:42.04829Z","iopub.status.idle":"2025-12-24T13:52:42.747119Z","shell.execute_reply":"2025-12-24T13:52:42.746424Z"},"papermill":{"duration":0.705984,"end_time":"2025-12-24T13:52:42.748252","exception":false,"start_time":"2025-12-24T13:52:42.042268","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Learning rate: 2.47052952e-07\n","Optimizer step: 0\n"]}],"source":["learning_rate = CustomSchedule(d_model, warmup_steps=4000, factor=1.0)\n","optimizer = tf.keras.optimizers.Adam(\n","    learning_rate=learning_rate,\n","    beta_1=0.9,\n","    beta_2=0.98,\n","    epsilon=1e-9\n",")\n","\n","\n","tf.print(\"Learning rate:\", optimizer.learning_rate)\n","print(\"Optimizer step:\", optimizer.iterations.numpy())\n","\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"]},{"cell_type":"code","execution_count":12,"id":"2b5b4f09","metadata":{"execution":{"iopub.execute_input":"2025-12-24T13:52:42.759669Z","iopub.status.busy":"2025-12-24T13:52:42.759445Z","iopub.status.idle":"2025-12-24T13:52:42.764656Z","shell.execute_reply":"2025-12-24T13:52:42.763988Z"},"papermill":{"duration":0.012149,"end_time":"2025-12-24T13:52:42.76583","exception":false,"start_time":"2025-12-24T13:52:42.753681","status":"completed"},"tags":[]},"outputs":[],"source":["@tf.function\n","def train_step(inp, tar):\n","    tar_inp = tar[:, :-1]\n","    tar_real = tar[:, 1:]\n","\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","\n","    with tf.GradientTape() as tape:\n","        predictions, _ = transformer(\n","            inp,\n","            tar_inp,\n","            training=True,\n","            enc_padding_mask=enc_padding_mask,\n","            look_ahead_mask=combined_mask,\n","            dec_padding_mask=dec_padding_mask,\n","        )\n","        loss = loss_function(tar_real, predictions)\n","\n","    gradients = tape.gradient(loss, transformer.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","\n","    train_loss(loss)\n","    train_accuracy(accuracy_function(tar_real, predictions))\n"]},{"cell_type":"code","execution_count":13,"id":"8e7ac291","metadata":{"execution":{"iopub.execute_input":"2025-12-24T13:52:42.77692Z","iopub.status.busy":"2025-12-24T13:52:42.77672Z","iopub.status.idle":"2025-12-24T15:57:34.024955Z","shell.execute_reply":"2025-12-24T15:57:34.024216Z"},"papermill":{"duration":7491.254998,"end_time":"2025-12-24T15:57:34.026126","exception":false,"start_time":"2025-12-24T13:52:42.771128","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 001 | Train Loss 8.3865 | Accuracy 0.1138 | Time 173.45s | LR 0.000213\n","Epoch 002 | Train Loss 6.9334 | Accuracy 0.1544 | Time 149.29s | LR 0.000425\n","Epoch 003 | Train Loss 6.1661 | Accuracy 0.1918 | Time 149.31s | LR 0.000638\n","Epoch 004 | Train Loss 5.5708 | Accuracy 0.2328 | Time 149.30s | LR 0.000851\n","Epoch 005 | Train Loss 5.1269 | Accuracy 0.2680 | Time 149.31s | LR 0.000953\n","Epoch 006 | Train Loss 4.7331 | Accuracy 0.3034 | Time 149.31s | LR 0.000870\n","Epoch 007 | Train Loss 4.3954 | Accuracy 0.3368 | Time 149.35s | LR 0.000805\n","Epoch 008 | Train Loss 4.1236 | Accuracy 0.3670 | Time 149.37s | LR 0.000753\n","Epoch 009 | Train Loss 3.8900 | Accuracy 0.3938 | Time 149.36s | LR 0.000710\n","Epoch 010 | Train Loss 3.6882 | Accuracy 0.4189 | Time 149.37s | LR 0.000674\n","Epoch 011 | Train Loss 3.5050 | Accuracy 0.4412 | Time 149.37s | LR 0.000642\n","Epoch 012 | Train Loss 3.3355 | Accuracy 0.4628 | Time 149.34s | LR 0.000615\n","Epoch 013 | Train Loss 3.1786 | Accuracy 0.4830 | Time 149.34s | LR 0.000591\n","Epoch 014 | Train Loss 3.0404 | Accuracy 0.5016 | Time 149.33s | LR 0.000569\n","Epoch 015 | Train Loss 2.9329 | Accuracy 0.5177 | Time 149.34s | LR 0.000550\n","Epoch 016 | Train Loss 2.8242 | Accuracy 0.5330 | Time 149.34s | LR 0.000532\n","Epoch 017 | Train Loss 2.7130 | Accuracy 0.5481 | Time 149.34s | LR 0.000517\n","Epoch 018 | Train Loss 2.5994 | Accuracy 0.5627 | Time 149.36s | LR 0.000502\n","Epoch 019 | Train Loss 2.4904 | Accuracy 0.5776 | Time 149.33s | LR 0.000489\n","Epoch 020 | Train Loss 2.3987 | Accuracy 0.5902 | Time 149.34s | LR 0.000476\n","Epoch 021 | Train Loss 2.3099 | Accuracy 0.6040 | Time 149.33s | LR 0.000465\n","Epoch 022 | Train Loss 2.2253 | Accuracy 0.6162 | Time 149.32s | LR 0.000454\n","Epoch 023 | Train Loss 2.1534 | Accuracy 0.6271 | Time 149.33s | LR 0.000444\n","Epoch 024 | Train Loss 2.0829 | Accuracy 0.6378 | Time 149.34s | LR 0.000435\n","Epoch 025 | Train Loss 2.0173 | Accuracy 0.6490 | Time 149.34s | LR 0.000426\n","Epoch 026 | Train Loss 1.9596 | Accuracy 0.6576 | Time 149.35s | LR 0.000418\n","Epoch 027 | Train Loss 1.9051 | Accuracy 0.6676 | Time 149.32s | LR 0.000410\n","Epoch 028 | Train Loss 1.8563 | Accuracy 0.6755 | Time 149.33s | LR 0.000403\n","Epoch 029 | Train Loss 1.8065 | Accuracy 0.6851 | Time 149.33s | LR 0.000396\n","Epoch 030 | Train Loss 1.7645 | Accuracy 0.6925 | Time 149.34s | LR 0.000389\n","Epoch 031 | Train Loss 1.7198 | Accuracy 0.7002 | Time 149.34s | LR 0.000383\n","Epoch 032 | Train Loss 1.6808 | Accuracy 0.7069 | Time 149.33s | LR 0.000377\n","Epoch 033 | Train Loss 1.6435 | Accuracy 0.7144 | Time 149.35s | LR 0.000371\n","Epoch 034 | Train Loss 1.6114 | Accuracy 0.7206 | Time 149.34s | LR 0.000365\n","Epoch 035 | Train Loss 1.5839 | Accuracy 0.7257 | Time 149.34s | LR 0.000360\n","Epoch 036 | Train Loss 1.5536 | Accuracy 0.7314 | Time 149.37s | LR 0.000355\n","Epoch 037 | Train Loss 1.5243 | Accuracy 0.7370 | Time 149.35s | LR 0.000350\n","Epoch 038 | Train Loss 1.4955 | Accuracy 0.7424 | Time 149.34s | LR 0.000346\n","Epoch 039 | Train Loss 1.4725 | Accuracy 0.7473 | Time 149.32s | LR 0.000341\n","Epoch 040 | Train Loss 1.4447 | Accuracy 0.7521 | Time 149.36s | LR 0.000337\n","Epoch 041 | Train Loss 1.4167 | Accuracy 0.7573 | Time 149.35s | LR 0.000333\n","Epoch 042 | Train Loss 1.3939 | Accuracy 0.7619 | Time 149.35s | LR 0.000329\n","Epoch 043 | Train Loss 1.3718 | Accuracy 0.7659 | Time 149.33s | LR 0.000325\n","Epoch 044 | Train Loss 1.3531 | Accuracy 0.7698 | Time 149.33s | LR 0.000321\n","Epoch 045 | Train Loss 1.3331 | Accuracy 0.7731 | Time 149.35s | LR 0.000318\n","Epoch 046 | Train Loss 1.3152 | Accuracy 0.7767 | Time 149.33s | LR 0.000314\n","Epoch 047 | Train Loss 1.2959 | Accuracy 0.7812 | Time 149.33s | LR 0.000311\n","Epoch 048 | Train Loss 1.2832 | Accuracy 0.7831 | Time 149.35s | LR 0.000307\n","Epoch 049 | Train Loss 1.2682 | Accuracy 0.7873 | Time 149.34s | LR 0.000304\n","Epoch 050 | Train Loss 1.2486 | Accuracy 0.7904 | Time 149.34s | LR 0.000301\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["patience = 5\n","best_loss = float('inf')\n","wait = 0\n","epochs = 50\n","\n","train_losses = []\n","\n","for epoch in range(epochs):\n","    start_epoch = time.time()\n","    train_loss.reset_state()\n","    train_accuracy.reset_state()\n","\n","    # Training\n","    for (batch, (inp, tar)) in enumerate(dataset):\n","        train_step(inp, tar)\n","\n","    current_train_loss = train_loss.result()\n","    train_losses.append(current_train_loss.numpy())\n","\n","    \n","    # Early stopping\n","    if current_train_loss < best_loss:\n","        best_loss = current_train_loss\n","        wait = 0\n","    else:\n","        wait += 1\n","        if wait >= patience:\n","            print(f\"Early stopping triggered at epoch {epoch+1}\")\n","            break\n","\n","    \n","    current_lr = learning_rate(optimizer.iterations).numpy()\n","    print(f'Epoch {epoch+1:03d} | Train Loss {current_train_loss:.4f} | '\n","          f'Accuracy {train_accuracy.result():.4f} | '\n","          f'Time {time.time() - start_epoch:.2f}s | LR {current_lr:.6f}')\n","\n","\n","\n","plt.plot(train_losses, label='Train Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":14,"id":"1d90bbb9","metadata":{"execution":{"iopub.execute_input":"2025-12-24T15:57:34.042343Z","iopub.status.busy":"2025-12-24T15:57:34.042123Z","iopub.status.idle":"2025-12-24T15:57:34.05093Z","shell.execute_reply":"2025-12-24T15:57:34.050139Z"},"papermill":{"duration":0.018522,"end_time":"2025-12-24T15:57:34.052217","exception":false,"start_time":"2025-12-24T15:57:34.033695","status":"completed"},"tags":[]},"outputs":[],"source":["def summarize(input_article, beam_width=3):\n","    input_ids = article_tokenizer.texts_to_sequences([input_article])\n","    input_ids = tf.keras.preprocessing.sequence.pad_sequences(\n","        input_ids, maxlen=MAX_TEXT_LEN, padding='post', truncating='post'\n","    )\n","    \n","    encoder_input = tf.cast(input_ids, tf.int32)\n","\n","    sos_id = summary_tokenizer.word_index['<sos>']\n","    eos_id = summary_tokenizer.word_index['<eos>']\n","\n","    sequences = [([sos_id], 0.0)]\n","    completed_sequences = []\n","\n","    for _ in range(MAX_SUMMARY_LEN):\n","        all_candidates = []\n","        for seq, score in sequences:\n","            if seq[-1] == eos_id:\n","                completed_sequences.append((seq, score))\n","                continue\n","\n","            output = tf.expand_dims(seq, 0)\n","            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n","\n","            predictions, _ = transformer(\n","                encoder_input,\n","                output,\n","                training=False,\n","                enc_padding_mask=enc_padding_mask,\n","                look_ahead_mask=combined_mask,\n","                dec_padding_mask=dec_padding_mask\n","            )\n","\n","            logits = predictions[:, -1, :]\n","            log_probs = tf.nn.log_softmax(logits)\n","\n","            for t in set(seq):\n","                if t not in (sos_id, eos_id):\n","                    log_probs = tf.tensor_scatter_nd_sub(\n","                        log_probs,\n","                        [[0, t]],\n","                        [0.8]\n","                    )\n","\n","            top_k = tf.math.top_k(log_probs, k=beam_width)\n","\n","            for i in range(beam_width):\n","                token = int(top_k.indices[0, i])\n","                candidate_score = score + float(top_k.values[0, i])\n","                candidate_seq = seq + [token]\n","\n","                length_penalty = ((5 + len(candidate_seq)) / 6) ** 1.1\n","                normalized_score = candidate_score / length_penalty\n","\n","                all_candidates.append((candidate_seq, normalized_score))\n","\n","        sequences = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)[:beam_width]\n","        if not sequences:\n","            break\n","\n","    completed_sequences.extend(sequences)\n","\n","    best_seq = max(completed_sequences, key=lambda tup: tup[1])[0]\n","\n","    decoded = np.expand_dims([t for t in best_seq if t != sos_id and t != eos_id], 0)\n","    decoded_text = summary_tokenizer.sequences_to_texts(decoded)[0]\n","\n","    return decoded_text.strip()"]},{"cell_type":"code","execution_count":15,"id":"52b7be8a","metadata":{"execution":{"iopub.execute_input":"2025-12-24T15:57:34.068469Z","iopub.status.busy":"2025-12-24T15:57:34.068057Z","iopub.status.idle":"2025-12-24T15:58:28.429448Z","shell.execute_reply":"2025-12-24T15:58:28.428566Z"},"papermill":{"duration":54.370426,"end_time":"2025-12-24T15:58:28.430585","exception":false,"start_time":"2025-12-24T15:57:34.060159","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Text: <sos> gangster-turned-politician mukhtar ansari has won from the mau constituency in uttar pradesh after polling 96,793 votes, defeating the nearest candidate by over 8,000 votes. ansari, who was the sitting mla from the constituency, had allied with the mayawati-led bahujan samaj party before the elections. ansari has been accused of murdering a bjp mla. <eos>\n","Real summary: <sos> gangster-turned-politician mukhtar ansari wins by 8000 votes <eos>\n","Pred summary: gangster turned politician mukhtar ansari wins by 96 000 votes\n","\n","Text: <sos> indira gandhi has been the only woman till date to have presented the union budget of india in 1970-71. this came after indira gandhi, the then prime minister, took over the finance portfolio after morarji desai resigned as the minister of finance. so far, she has been the only woman finance minister of india. <eos>\n","Real summary: <sos> indira gandhi only woman to have presented the budget <eos>\n","Pred summary: indira gandhi only woman to have presented the budget\n","\n","Text: <sos> actor adam sandler brought his 23-year-old doppelg nger max kessler for the premiere of his upcoming film'the do-over'. sandler noticed max after the 23-year-old posted a picture of himself alongside sandler on reddit. it was captioned, \" the name of adam sandler's character in...'the do-over'is max kessler. my name is max kessler... and i look just like him. \" <eos>\n","Real summary: <sos> sandler brings doppelg nger to film premiere <eos>\n","Pred summary: turkey brings doppelg nger to film premiere\n","\n","Text: <sos> the supreme court recently reminded the centre that aadhaar cannot be made mandatory for any services. the apex court also ordered the centre to remove its condition of making aadhaar mandatory in scholarship schemes for students. \" the aadhaar card scheme is purely voluntary and cannot be made mandatory till the matter is finally decided by this court, \" the sc added. <eos>\n","Real summary: <sos> aadhaar cannot be mandatory, sc reminds govt <eos>\n","Pred summary: govt cannot be mandatory sc rules aadhaar mandatory aadhaar sc\n","\n","Text: <sos> researchers at the university of stuttgart have built wall-climbing mini robots that work together to create architecture from carbon fibre. the robots carry carbon fibre thread spools that they pass back and forth after affixing to points on a wall. the researchers are planning to increase the number of robots, allowing them to attach fibre to ceilings and curved walls. <eos>\n","Real summary: <sos> robots create architecture with carbon fibre <eos>\n","Pred summary: robots create aap mn over wing fibre bn fibre artificial fibre aims to create it fibre plane tiles\n","\n","Text: <sos> actress-filmmaker pooja bhatt while sharing her opinion on karan johar's row over ae dil hai mushkil with mns chief raj thackeray called it \" bullying \" and not \" nationalism \" . \" it is schoolyard bullying at its best and worst, \" tweeted bhatt. she was responding to a tweet by journalist barkha dutt, which read, \" nationalism or blackmail? karanjohar bullied, raj thackeray emboldened. \" <eos>\n","Real summary: <sos> not nationalism, it's bullying pooja on johar, mns row <eos>\n","Pred summary: not nationalism it's bullying pooja on johar mns row\n","\n","Text: <sos> thousands of migrants were evacuated after a fire destroyed tents and makeshift shelters during violence among residents in greece's reportedly biggest migrant camp, on the island of lesbos, officials said. no one was injured, but the fire destroyed about 60 of the camp'moria', officials added. reports suggest clashes erupted following rumours of mass deportations of migrants to turkey. <eos>\n","Real summary: <sos> migrants evacuated after fire at greece's biggest camp <eos>\n","Pred summary: migrant migrants evacuated after fire at greece office\n","\n","Text: <sos> the world's best-performing initial public offering (ipo) by the hong kong-based luen wong group holdings limited is currently trading 6,715 above its offer price. the civil engineering company's stocks had jumped over 1,400 on the first trading day after its april ipo. luen wong had reported sales worth 41 million last year and is currently valued at 2.9 billion. <eos>\n","Real summary: <sos> world's best-performing ipo this year has gained over 6,000 <eos>\n","Pred summary: world's best selling performing ipo this year has gained by 6 000\n","\n","Text: <sos> an earthquake of magnitude 7.0 shook el salvador and nicaragua on thursday, an hour after a hurricane struck the caribbean coasts of nicaragua and costa rica. there were no immediate reports of damage following the earthquake, which occurred around 120 kilometres off the el salvador coast. a tsunami warning was issued in nicaragua and el salvador, but later withdrawn. <eos>\n","Real summary: <sos> el salvador struck by 7.0 magnitude earthquake <eos>\n","Pred summary: el salvador struck by 7 0 magnitude earthquake\n","\n","Text: <sos> kargil martyr's daughter gurmehar kaur's picture from an old campaign holding placard \" pakistan did not kill my dad, war killed him \" is being trolled by twitter users. while one user tweeted dawood ibrahim's picture that read, \" i didn't kill people in 1993, bombs killed them \" , another posted rahul gandhi's photo stating, \" i did not destroy congress, my speeches did \" . <eos>\n","Real summary: <sos> twitter trolls kargil martyr's daughter for old campaign pic <eos>\n","Pred summary: tweets twitter trolls kargil daughter for old campaign pic for his old win in pak turns old pic\n"]}],"source":["def replace_tokens(s):\n","    return s.replace(\"<SOS>\", \"\").replace(\"<EOS>\", \"\").strip()\n","\n","for i in range(10):\n","    text = replace_tokens(df['text'][i])\n","    real_summary = replace_tokens(df['summary'][i])\n","    pred_summary = summarize(text)\n","\n","    print(f\"\\nText: {text}\"\n","          f\"\\nReal summary: {real_summary}\"\n","          f\"\\nPred summary: {pred_summary}\")"]},{"cell_type":"code","execution_count":16,"id":"fdf81129","metadata":{"execution":{"iopub.execute_input":"2025-12-24T15:58:28.447916Z","iopub.status.busy":"2025-12-24T15:58:28.447711Z","iopub.status.idle":"2025-12-24T15:59:02.858601Z","shell.execute_reply":"2025-12-24T15:59:02.85782Z"},"papermill":{"duration":34.428243,"end_time":"2025-12-24T15:59:02.867434","exception":false,"start_time":"2025-12-24T15:58:28.439191","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Text : Apple Inc. announced a new campus in Austin, Texas, expected to create over 5,000 jobs by 2026, focusing on hardware R&D. CEO Tim Cook emphasized innovation and community commitment. \n","Summary (transformer) : apple to create 5 000 jobs globally by mid way\n","\n","Text : The Prime Minister held a press conference on Tuesday, unveiling new economic reforms aimed at boosting small businesses and creating job opportunities across the country. \n","Summary (transformer) : pm modi announces new reforms across india\n","\n","Text : In yesterday’s football match, Barcelona defeated Real Madrid 3-1, with Messi scoring a hat-trick. The victory gives Barcelona an edge in the league standings. \n","Summary (transformer) : barcelona thrash messi 3 1 in their hat trick\n","\n","Text : A new study revealed that regular exercise and a balanced diet can significantly reduce the risk of heart disease and improve overall life expectancy. \n","Summary (transformer) : human exercise can reduce risk to treat life risk study\n","\n","Text : The latest Marvel movie broke box office records over the weekend, earning over $200 million globally in its opening weekend. \n","Summary (transformer) : marvel box office breaks 200 mn over 200 mn since opening weekend\n","\n","Text : Scientists discovered a new species of dolphin in the Pacific Ocean, highlighting the importance of marine conservation and biodiversity protection. \n","Summary (transformer) : mice discovered from ocean for treat rich in ocean\n","\n","Text : The stock market saw a sharp increase today, with tech stocks leading the gains as investors responded positively to quarterly earnings reports. \n","Summary (transformer) : what breaks 2016's biggest tech stock market on sept 25\n"]}],"source":["texts = [\n","    \"Apple Inc. announced a new campus in Austin, Texas, expected to create over 5,000 jobs by 2026, focusing on hardware R&D. CEO Tim Cook emphasized innovation and community commitment.\",\n","    \n","    \"The Prime Minister held a press conference on Tuesday, unveiling new economic reforms aimed at boosting small businesses and creating job opportunities across the country.\",\n","\n","    \"In yesterday’s football match, Barcelona defeated Real Madrid 3-1, with Messi scoring a hat-trick. The victory gives Barcelona an edge in the league standings.\",\n","    \n","    \"A new study revealed that regular exercise and a balanced diet can significantly reduce the risk of heart disease and improve overall life expectancy.\",\n","    \n","    \"The latest Marvel movie broke box office records over the weekend, earning over $200 million globally in its opening weekend.\",\n","    \n","    \"Scientists discovered a new species of dolphin in the Pacific Ocean, highlighting the importance of marine conservation and biodiversity protection.\",\n","    \n","    \"The stock market saw a sharp increase today, with tech stocks leading the gains as investors responded positively to quarterly earnings reports.\"\n","]\n","\n","for text in texts:\n","    print(f\"\\nText : {text} \\nSummary (transformer) : {summarize(text)}\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":33526,"isSourceIdPinned":false,"sourceId":44284,"sourceType":"datasetVersion"}],"dockerImageVersionId":31089,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":7629.267619,"end_time":"2025-12-24T15:59:06.014694","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-24T13:51:56.747075","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}